<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Statistics | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Statistics for Machine LearningOne. 两大派系 频率派：统计机器学习（设计模型，找到loss function，设计算法。本质为优化问题） 贝叶斯派：概率图模型（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型） Data X: data, X={x_1, x_2, …, x_n}^T  Dimension">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistics">
<meta property="og:url" content="http://yoursite.com/2020/09/11/Statistics/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Statistics for Machine LearningOne. 两大派系 频率派：统计机器学习（设计模型，找到loss function，设计算法。本质为优化问题） 贝叶斯派：概率图模型（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型） Data X: data, X={x_1, x_2, …, x_n}^T  Dimension">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT3.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT4.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT5.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/DT6.png">
<meta property="og:updated_time" content="2020-10-02T04:01:18.828Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistics">
<meta name="twitter:description" content="Statistics for Machine LearningOne. 两大派系 频率派：统计机器学习（设计模型，找到loss function，设计算法。本质为优化问题） 贝叶斯派：概率图模型（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型） Data X: data, X={x_1, x_2, …, x_n}^T  Dimension">
<meta name="twitter:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Statistics
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Statistics-for-Machine-Learning"><a href="#Statistics-for-Machine-Learning" class="headerlink" title="Statistics for Machine Learning"></a>Statistics for Machine Learning</h4><h5 id="One-两大派系"><a href="#One-两大派系" class="headerlink" title="One. 两大派系"></a>One. 两大派系</h5><ul>
<li><strong>频率派：统计机器学习</strong>（设计模型，找到loss function，设计算法。本质为优化问题）</li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型）</li>
<li>Data<ul>
<li>X: data, X={x_1, x_2, …, x_n}^T  Dimension(N, P)</li>
<li>θ: parameter,   X~p(X|θ)</li>
</ul>
</li>
<li><strong>频率派</strong><ul>
<li>θ为未知常数；X为随机变量</li>
<li>loss = P(X|θ) = Π P(x_i|θ)<ul>
<li>x_1, x_2, …, x_n之间独立同分布</li>
</ul>
</li>
<li>Maximum Likelihood Estimation 极大似然估计<ul>
<li>θ_MLE = argmax_θ log P(X|θ)</li>
</ul>
</li>
</ul>
</li>
<li><strong>贝叶斯派</strong><ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率<ul>
<li>P(θ|X) = P(X, θ)/P(X) = P(X|θ)P(θ)/P(X) = likelihood*prior / ∫_θ P(X|θ) dθ</li>
</ul>
</li>
<li>Maximum A Posterior Probability 最大后验概率<ul>
<li>找到θ在分布中最大值点</li>
<li>θ_MLE = argmax_θ P(X|θ) = argmax_θ P(X|θ)P(θ)</li>
<li>P(θ|X)其分母是不变的，则θ_MLE与分子成正比，只需要算分子</li>
</ul>
</li>
<li>贝叶斯估计<ul>
<li>P(θ|X) = P(X|θ)P(θ) / ∫_θ P(X|θ) dθ</li>
<li>必须完整计算整个分子式</li>
</ul>
</li>
<li>贝叶斯预测<ul>
<li>X (train), X~ (test),  X -&gt; θ -&gt; X~</li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li>P(X~|X) = ∫_θ P(X~, θ|X) dθ = ∫_θ P(X~|X)P(θ|X) dθ</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Two-Linear-Regression（频率派）"><a href="#Two-Linear-Regression（频率派）" class="headerlink" title="Two. Linear Regression（频率派）"></a>Two. Linear Regression（频率派）</h5><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线f(w) = w^T x + b（偏置b可先忽略）</p>
<ul>
<li><p>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）</p>
<ul>
<li>线性（属性线性、全局线性、系数线性）<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul>
</li>
<li>全局性<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul>
</li>
<li>数据未加工<ul>
<li>预处理：PCA，流行</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>矩阵表达</strong></p>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）<ul>
<li><strong>L(w) = Σ||w^T x_i - y_i||^2</strong> = Σ(w^T x_i - y_i)^2 = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y</li>
</ul>
</li>
<li><strong>w~ = argmin L(w)</strong></li>
<li>求导 dL/dw =  2 X^T X w - 2 X^T Y = 0  —-&gt; X^T X w = X^T Y<ul>
<li><strong>w~ = (X^T X)^-1 X^T Y</strong> (伪逆：(X^T X)^-1 X^T)</li>
</ul>
</li>
<li>x_3的误差为(w^T x_3 - y_3)，即所有误差分成一小段一小段</li>
</ul>
</li>
<li><p><strong>几何意义</strong></p>
<ul>
<li>f(w) = w^T x &lt;=&gt; f(β) = x^T β</li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li><p>目标：在p维空间中找到一条直线f(β)离Y最近，即Y在p维空间的投影</p>
<ul>
<li><p>若向量a与向量b垂直，则 a^T b = 0</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p>
</li>
<li><p>虚线为(Y - Xβ) 与X的p维空间垂直，X^T (Y-Xβ) = 0 —-&gt; β = (X^T X)^-1 X^T Y</p>
</li>
<li>误差分散在p个维度上</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>概率角度</strong></p>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ^2)</li>
<li>y = f(w) + ε = w^T x + ε<ul>
<li>y|x,w ~ N(w^T x, σ^2)  &lt;=&gt; <strong>P(y|x,w)</strong> =1/(sqrt(2*pi)*σ) exp(-(y - w^T x)^2/(2*σ^2))</li>
</ul>
</li>
<li>定义log-likelihood： <ul>
<li>L_MLE (w) = log P(y|x,w) = log Π P(y_i|x_i,w) = Σ log P(y_i|x_i,w) = Σ[log(1/(sqrt(2*pi)*σ)) + log(exp(-(y_i - w^T x_i)^2/(2*σ^2)))] = Σ[log(1/(sqrt(2*pi)*σ)) -(y_i - w^T x_i)^2/(2*σ^2)]</li>
<li>样本之间独立同分布 </li>
<li>w~ = argmax L_MLE (w) = argmax -(y_i - w^T x_i)^2/(2*σ^2) = argmin (y_i - w^T x_i)^2</li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Regularization 正则化</strong> </p>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong><ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：argmin [L(w)+λP(w)]</li>
</ul>
</li>
<li>L1 -&gt; Lasso<ul>
<li>P(w) = ||w||_1</li>
</ul>
</li>
<li>L2 -&gt; Ridge 岭回归<ul>
<li>P(w) = ||w||_2 = w^T w</li>
<li>权值衰减</li>
<li>J(w) = Σ||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y<ul>
<li>w~ = argmin J(w)</li>
<li>dJ/dw = 2 (X^T X + λ I)w - 2 X^T Y = 0, w~ = (X^T X + λ I)^-1 X^T Y</li>
<li>X^T X 是半正定矩阵+对角矩阵=(X^T X + λ I)正定矩阵，必然<strong>可逆</strong></li>
</ul>
</li>
</ul>
</li>
<li>贝叶斯的角度<ul>
<li>参数w服从分布，w~N(0,σ_0^2) —&gt; <strong>P(w)</strong> = 1/(sqrt(2*pi)*σ_0) exp(||w||^2/(2*σ_0^2))</li>
<li>P(w|y) = P(y|w)P(w) / P(y)</li>
<li>MAP: w~ = argmax P(w|y) = argmax P(y|w)P(w) = argmax log(P(y|w)P(w)) = argmax log[1/(2*pi*σ_0*σ) exp(-(y_i - w^T x_i)^2/(2*σ^2) -||w||^2/(2*σ_0^2))] = argmin [(y_i - w^T x_i)^2 + σ^2/σ_0^2||w||^2] = argmin [L(w)+λP(w)]</li>
<li>λ = σ^2/σ_0^2</li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Three-Linear-Classification"><a href="#Three-Linear-Classification" class="headerlink" title="Three. Linear Classification"></a>Three. Linear Classification</h5><ul>
<li>线性回归——&gt;激活函数，降维——-&gt;线性分类</li>
<li>硬分类：0/1<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul>
</li>
<li>软分类：[0,1]区间内概率<ul>
<li>生成式：Gaussian Discriminant Analysis（转换用贝叶斯求解）</li>
<li>判别式：Logisitic Regression（直接学习P(Y|X)）</li>
</ul>
</li>
<li><p><strong>Perceptron 感知机</strong> (1957年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>样本：{(x_i, y_i)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：f(x) = sign(w^T x + b) （w^T x大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）<ul>
<li>L(w) = Σ I{y_i * (w^T x_i) &lt; 0} （非连续函数，不可导）</li>
<li>L(w) = Σ -y_i w^T x_i, dL = -y_i x_i</li>
</ul>
</li>
<li>若是非线性可分，可是使用pocket algorithm</li>
</ul>
</li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数N_1，均值X_c1，方差S_c1，负样本个数N_2，均值X_c2，方差S_c2</li>
<li>思想：类内小，类间大<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 w^T x（因为垂直，则Z平面即w向量）<ul>
<li>（前提设置||w||=1）</li>
<li>则样本点投影到Z平面为：|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i</li>
</ul>
</li>
</ul>
</li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong> <ul>
<li>N_1 = 1/N_1 Σ w^T x_i</li>
<li>S_1 = 1/N_1 Σ (w^T x_i - Z_1) (w^T x_i - Z_1)^T</li>
<li>类间：(Z_1-Z_2)^2</li>
<li>类内：S_1+S_2</li>
</ul>
</li>
<li>策略：L(w) = (Z_1-Z_2)^2 / (S_1+S_2) = [w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w] / [ w^T (S_c1+ S_c2) w ]<ul>
<li>分子 = [w^T (1/N_1 Σ x_i - 1/N_2 Σ x_i)]^2 = [w^T (X_c1 - X_c2)]^2 = w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w</li>
<li>分母 =  w^T S_c1 w +  w^T S_c2 w =  w^T (S_c1+ S_c2) w<ul>
<li>S_1 =  1/N_1 Σ (w^T x_i - 1/N_1 Σ w^T x_j)(w^T x_i - 1/N_1 Σ w^T x_j)^T=w^T [1/N_1 Σ (x_i - x_c1)(x_i - x_c1)^T] w = w^T S_c1 w</li>
</ul>
</li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li>L(w) = w^T S_b w / w^T S_w w</li>
<li>w~ = argmax L(w)<ul>
<li>dL/dw = 2*S_b w (w^T S_w w)^-1 + (w^T S_b w) * (-1) (w^T S_w w)^-2 * 2 * S_w w = 0</li>
<li>S_b w (w^T S_w w) = (w^T S_b w) S_w w （(w^T S_w w) 最终计算得一个实数，一维，没有方向）（求解w～关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li>w = (w^T S_w w)/(w^T S_b w) * S_w^-1 * S_b w，正比于(S_w^-1 * S_b w) ，正比于(S_w^-1 *(X_c1 - X_c2))</li>
<li>（S_b w = (X_c1 - X_c2)(X_c1 - X_c2)^T w，(X_c1 - X_c2)^T w为实数）</li>
<li>（若S_w是对角矩阵，各向同性，S_w正比于单位矩阵，则w正比于(X_c1 - tX_c2)</li>
</ul>
</li>
</ul>
</li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul>
</li>
<li><strong>Logistic Regression</strong><ul>
<li></li>
</ul>
</li>
</ul>
<h5 id="Four-Naive-Bayes"><a href="#Four-Naive-Bayes" class="headerlink" title="Four. Naive Bayes"></a>Four. Naive Bayes</h5><ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong><ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
</ul>
</li>
<li>Data<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c_1, c_2, …,c_k}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul>
</li>
<li><strong>prior probability</strong> <ul>
<li>P(Y=c_k)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul>
</li>
<li><strong>conditional probability</strong><ul>
<li>P(X=x|Y=c_k) = P(X^(1)=x^(1), …, X^(d)=x^(d)|Y=c_k) = ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>特征条件独立</li>
<li>^(1) 上标表示第1维度</li>
</ul>
</li>
<li><strong>joint probability distributions</strong><ul>
<li>联合概率分布</li>
<li>P(X, Y) = P(X|Y)P(Y)</li>
</ul>
</li>
<li><strong>posterior probability</strong><ul>
<li>P(Y=c_k|X=x) = P(X=x|Y=c_k)P(Y=c_k)/ΣP(X=x|Y=c_k)P(Y=c_k) = P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)/Σ P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>则分类器为<ul>
<li>y=f(x) = argmax P(Y=c_k|X=x) = argmax P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>分母不变，则仅于分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>loss function</strong><ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li>L(Y, f(x)) = 1 if Y!=f(x) or 0 if Y==f(x)<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul>
</li>
<li>期望风险函数：R_exp(f) = E[L(Y, f(x))]<ul>
<li>根据联合概率分布：R_exp(f)  = E_x Σ[L(c_k|f(x))]P(c_k|X)</li>
</ul>
</li>
<li>f(x) = argmin Σ[L(c_k|f(x))]P(c_k|X) <ul>
<li>根据L(Y, f(x))函数展开，消去Y==f(x)项</li>
<li>f(x) = <strong>argmin ΣP(y!=c_k|X=x)</strong> = argmin (1-P(y=c_k|X=x)) = <strong>argmax P(y=c_k|X=x)</strong></li>
</ul>
</li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c_k类别的最大概率为多少）</li>
</ul>
</li>
<li>详情案例见统计学习方法(第二版)63页</li>
</ul>
<h5 id="Five-Decision-Tree"><a href="#Five-Decision-Tree" class="headerlink" title="Five. Decision Tree"></a>Five. Decision Tree</h5><ul>
<li><p>基于数据特征构造决策树</p>
<ul>
<li>有向边</li>
<li><p>结点</p>
<ul>
<li>内部结点(internal node)-&gt;表示特征</li>
<li><p>叶子结点(leaf node)-&gt;表示类别</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p>
</li>
<li><p>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</p>
</li>
</ul>
</li>
<li>决策树表示给定特征条件下类的条件概率分布。<ul>
<li>一个条概率分布定义特征空间的一个划分上</li>
<li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li>
</ul>
</li>
<li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果<br><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li>
</ul>
</li>
<li><p>决策树学习过程：特征选择、决策树生成、剪枝</p>
<ul>
<li><p><strong>ID3算法</strong>（分类、多叉树）</p>
<ul>
<li><p>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）</p>
<ul>
<li>Information Gain 信息增益。信息增益越大对应的特征越重要</li>
<li>Entropy 熵，表示随机变量不确定的度量</li>
<li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"></p>
</li>
</ul>
</li>
<li><p>生成</p>
<ul>
<li>选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</p>
<ul>
<li><p><strong>C4.5算法</strong>（分类、多叉树）</p>
<ul>
<li>特征选择</li>
<li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p>
</li>
<li><p>生成</p>
<ul>
<li>与ID3算法类似</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>基于信息论的熵模型的，这里面会涉及大量的对数运算</li>
</ul>
</li>
<li>二叉树模型会比多叉树运算效率高</li>
<li>无剪枝</li>
</ul>
</li>
<li><p><strong>CART</strong> classification and regression tree（分类、回归、二叉树）</p>
<ul>
<li>分类<ul>
<li>特征选择<ul>
<li>Gini基尼指数</li>
</ul>
</li>
</ul>
</li>
<li><p>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</p>
<ul>
<li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"></p>
</li>
<li><p>生成</p>
<ul>
<li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li>
<li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li>
</ul>
</li>
</ul>
</li>
<li><p>损失函数</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p>
<ul>
<li><p>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p>
</li>
<li><p>回归</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Four-KNN"><a href="#Four-KNN" class="headerlink" title="Four. KNN"></a>Four. KNN</h5><h5 id="Six-Support-Vector-Machines"><a href="#Six-Support-Vector-Machines" class="headerlink" title="Six. Support Vector Machines"></a>Six. Support Vector Machines</h5>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckfcjxxin000gb4eg21xc95ij" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/08/12/Bayes-MRF/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Bayes_MRF</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/11/Statistics/">Statistics</a>
          </li>
        
          <li>
            <a href="/2020/08/12/Bayes-MRF/">Bayes_MRF</a>
          </li>
        
          <li>
            <a href="/2020/08/09/HMM-CRF/">HMM_CRF</a>
          </li>
        
          <li>
            <a href="/2020/07/25/Spectral-Cluster/">Spectral_Cluster</a>
          </li>
        
          <li>
            <a href="/2020/05/06/machine-learning/">machine_learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>