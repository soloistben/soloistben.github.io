<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Statistics | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (\(f(x)=w^T x+b\)), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistics">
<meta property="og:url" content="http://yoursite.com/2020/09/11/Statistics/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (\(f(x)=w^T x+b\)), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png">
<meta property="og:updated_time" content="2020-10-07T08:46:46.732Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistics">
<meta name="twitter:description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (\(f(x)=w^T x+b\)), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化">
<meta name="twitter:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Statistics
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="statistics-for-machine-learning">Statistics for Machine Learning</h4>
<h5 id="one.-两大派系">One. 两大派系</h5>
<ul>
<li><strong>频率派：统计机器学习</strong>（model (<span class="math inline">\(f(x)=w^T x+b\)</span>), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）
<ul>
<li>正则化（L1,L2）</li>
<li>核化（Kernel SVM）</li>
<li>集成化（AdaBoost，RandForest）</li>
<li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li>
</ul></li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））
<ul>
<li>有向图：Bayesian Network (Deep Directed Network)
<ul>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder (VAE)</li>
<li>GAN</li>
</ul></li>
<li>无向图：Markov Network (Deep Boltzmann Network )</li>
<li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li>
<li>统称 Deep Generative Model（但深层很难计算）</li>
<li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li>
</ul></li>
<li>Data
<ul>
<li>X: data, X={x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>}<sup>T</sup> Dimension(N, P)</li>
<li>θ: parameter, X~p(X|θ)</li>
</ul></li>
<li><strong>频率派</strong>
<ul>
<li>θ为未知常数；X为随机变量</li>
<li><span class="math inline">\(loss = P(X|θ) = \prod_{i=1}^n P(x_i|θ)\)</span>
<ul>
<li>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>之间独立同分布</li>
</ul></li>
<li>Maximum Likelihood Estimation 极大似然估计
<ul>
<li><span class="math inline">\(θ_{MLE}= argmax_θ \ log^{P(X|θ)}\)</span></li>
</ul></li>
</ul></li>
<li><strong>贝叶斯派</strong>
<ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率
<ul>
<li><span class="math inline">\(P(θ|X) = \frac{P(X, θ)}{P(X)} = \frac{P(X|θ)P(θ)}{P(X)} = \frac{likelihood*prior}{\int_θ P(X|θ) d_θ}\)</span></li>
</ul></li>
<li>Maximum A Posterior Probability 最大后验概率
<ul>
<li>找到θ在分布中最大值点</li>
<li><span class="math inline">\(θ_{MLE} = argmax_θ \ P(X|θ) = argmax_θ \ P(X|θ)P(θ)\)</span></li>
<li>P(θ|X)其分母是不变的，则θ<sub>MLE</sub>与分子成正比，只需要算分子</li>
</ul></li>
<li>贝叶斯估计
<ul>
<li><span class="math inline">\(P(θ|X) = \frac{P(X|θ)P(θ)}{\int_θ P(X|θ) d_θ}\)</span></li>
<li>必须完整计算整个分子式</li>
</ul></li>
<li>贝叶斯预测
<ul>
<li><span class="math inline">\(X (train), \hat{X} (test), X → θ → \hat{X}\)</span></li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li><span class="math inline">\(P(\hat{X}|X) = \int_θ P(\hat{X}, θ|X) d_θ = \int_θ P(\hat{X}|X)P(θ|X) d_θ\)</span></li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-linear-regression">Two. Linear Regression</h5>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线<span class="math inline">\(f(w) = w^T x + b\)</span>（偏置b可先忽略）</p>
<ul>
<li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）
<ul>
<li>线性（属性线性、全局线性、系数线性）
<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul></li>
<li>全局性
<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul></li>
<li>数据未加工
<ul>
<li>预处理：PCA，流行</li>
</ul></li>
</ul></li>
<li><strong>矩阵表达</strong>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）
<ul>
<li><span class="math inline">\(\mathbf{L(w) = \sum ||w^T x_i - y_i||^2} = \sum (w^T x_i - y_i)^2 \\ = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathbf{\hat{w} = argmin \ L(w)}\)</span></li>
<li>求导$  = 2 X^T X w - 2 X^T Y=0 → X^T X w = X^T Y$
<ul>
<li><span class="math inline">\(\hat{w} = (X^T X)^{-1} X^T Y\)</span> (伪逆：<span class="math inline">\((X^T X)^{-1} X^T\)</span>)</li>
</ul></li>
<li><span class="math inline">\(x_3\)</span>的误差为<span class="math inline">\(w^T x_3 - y_3\)</span>，即所有误差分成一小段一小段</li>
</ul></li>
<li><p><strong>几何意义</strong></p>
<ul>
<li><span class="math inline">\(f(w) = w^T x \Leftrightarrow f(β) = x^T β\)</span></li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li>目标：在p维空间中找到一条直线<span class="math inline">\(f(β)\)</span>离Y最近，即Y在p维空间的投影
<ul>
<li><p>若向量a与向量b垂直，则 <span class="math inline">\(a^T b = 0\)</span></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li>
<li>虚线为<span class="math inline">\(Y - Xβ\)</span> 与<span class="math inline">\(X\)</span>的p维空间垂直，<span class="math inline">\(X^T (Y-Xβ) = 0 → β = (X^T X)^{-1} X^T Y\)</span></li>
<li>误差分散在p个维度上</li>
</ul></li>
</ul></li>
<li><p><strong>概率角度</strong></p>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ<sup>2</sup>)</li>
<li><span class="math inline">\(y = f(w) + ε = w^T x + ε\)</span>
<ul>
<li><span class="math inline">\(y|x,w\)</span> ~<span class="math inline">\(N(w^T x, σ^2) \Leftrightarrow P(y|x,w) =\frac{1}{\sqrt{2\pi}σ} e^{-\frac{(y - w^T x)^2}{2*σ^2}}\)</span></li>
</ul></li>
<li>定义log-likelihood：
<ul>
<li><span class="math inline">\(L_{MLE} (w) = log^{P(y|x,w)} = log^{\prod P(y_i|x_i,w)} = \sum log^{P(y_i|x_i,w)} \\ = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} + log^{e^{-\frac{(y - w^T x)^2}{2σ^2}}}] = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} -\frac{(y - w^T x)^2}{2σ^2}]\)</span></li>
<li>样本之间独立同分布</li>
<li><span class="math inline">\(\hat{w} = argmax \ L_{MLE} (w) = argmax \ \frac{(y - w^T x)^2}{2σ^2} = argmin \ (y_i - w^T x_i)^2\)</span></li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul></li>
</ul></li>
<li><p><strong>Regularization 正则化</strong></p>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong>
<ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：<span class="math inline">\(argmin [L(w)+λP(w)]\)</span></li>
</ul></li>
<li>L1 -&gt; Lasso
<ul>
<li><span class="math inline">\(P(w) = ||w||_1\)</span></li>
</ul></li>
<li>L2 -&gt; Ridge 岭回归
<ul>
<li><span class="math inline">\(P(w) = ||w||_2 = w^T w\)</span></li>
<li>权值衰减</li>
<li><span class="math inline">\(J(w) = \sum||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w \\ = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y\)</span>
<ul>
<li><span class="math inline">\(\hat{w} = argmin \ J(w)\)</span></li>
<li><span class="math inline">\(\frac{dJ}{dw} = 2 (X^T X + λ I)w - 2 X^T Y = 0, \hat{w} = (X^T X + λ I)^{-1} X^T Y\)</span></li>
<li><span class="math inline">\(X^T X\)</span> 是半正定矩阵+对角矩阵=<span class="math inline">\((X^T X + λI)\)</span>正定矩阵，必然<strong>可逆</strong></li>
</ul></li>
</ul></li>
<li>贝叶斯的角度
<ul>
<li>参数w服从分布，<span class="math inline">\(w\)</span>~<span class="math inline">\(N(0,σ\_0^2) → P(w) = \frac{1}{\sqrt{2\pi}σ_0} e^{-\frac{||w||^2}{2σ_0^2}}\)</span></li>
<li><span class="math inline">\(P(w|y) = \frac{P(y|w)P(w)}{P(y)}\)</span></li>
<li>MAP: <span class="math inline">\(\hat{w} = argmax \ P(w|y) = argmax \ P(y|w)P(w) = argmax \ log^{P(y|w)P(w)} \\ = argmax \ log^{\frac{1}{2\piσ_0σ} e^{(-\frac{(y_i - w^T x_i)^2}{2σ^2} -\frac{||w||^2}{2σ_0^2})}} = argmin [(y_i - w^T x_i)^2 + \frac{σ^2}{σ_0^2}||w||^2] = argmin [L(w)+λP(w)]\)</span></li>
<li><span class="math inline">\(λ = \frac{σ^2}{σ_0^2}\)</span></li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-linear-classification">Three. Linear Classification</h5>
<ul>
<li>线性回归------&gt;激活函数，降维-------&gt;线性分类</li>
<li>硬分类：0/1
<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul></li>
<li>软分类：[0,1]区间内概率
<ul>
<li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li>
<li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li>
</ul></li>
<li><p><strong>Perceptron 感知机</strong> (1958年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>判别模型</li>
<li>样本：{(x<sub>i</sub>, y<sub>i</sub>)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：<span class="math inline">\(f(x) = sign(w^T x + b)\)</span> （<span class="math inline">\(w^T x\)</span>大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）
<ul>
<li><span class="math inline">\(L(w) = \sum I\{y_i * (w^T x_i) &lt; 0\}\)</span> （非连续函数，不可导）</li>
<li><span class="math inline">\(L(w) = \sum-y_i w^T x_i,\frac{dL}{dw} = -y_i x_i\)</span></li>
</ul></li>
<li>若是非线性可分，可是使用pocket algorithm</li>
<li>从感知机到深度学习（发展历程）
<ul>
<li>1958年提出PLA</li>
<li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li>
<li>1981年提出MLP（多层感知机），FeedForward Neural Network</li>
<li>1986年BP+MLP，RNN</li>
<li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li>
<li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li>
<li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li>
<li>2009年GPU发展，2011年speech，2012年ImageNet</li>
<li>2013年Variational Autoencoder (VAE)</li>
<li>2014年GAN</li>
<li>2016年Alpha Go</li>
<li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li>
<li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li>
</ul></li>
</ul></li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数<span class="math inline">\(N1\)</span>，均值<span class="math inline">\(X_{c1}\)</span>，方差<span class="math inline">\(S_{c1}\)</span>，负样本个数<span class="math inline">\(N_{2}\)</span>，均值<span class="math inline">\(X_{c2}\)</span>，方差<span class="math inline">\(S_{c2}\)</span>，（<span class="math inline">\(X_{c1} = \frac{1}{N_1} \sum x_i,S_{c1} = \frac{1}{N_1} \sum (x_i - X_{c1})(x_i - X_{c1})^T\)</span>）</li>
<li>思想：类内小，类间大
<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 <span class="math inline">\(w^T x\)</span>（因为垂直，则Z平面即w向量）
<ul>
<li>（前提设置<span class="math inline">\(||w||=1\)</span>）</li>
<li>则样本点投影到Z平面为：<span class="math inline">\(|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i\)</span></li>
</ul></li>
</ul></li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong>
<ul>
<li><span class="math inline">\(N_1 = \frac{1}{N_1} \sum w^T x_i\)</span></li>
<li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - Z_1) (w^T x_i - Z_1)^T\)</span></li>
<li>类间：<span class="math inline">\((Z_1-Z_2)^2\)</span>，类内：<span class="math inline">\(S_1+S_2\)</span></li>
</ul></li>
<li>策略：<span class="math inline">\(L(w) = \frac{(Z_1-Z_2)^2}{(S_1+S_2)}= \frac{w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w}{w^T(S_{c1}+ S_{c2}) w }\)</span>
<ul>
<li>分子 =$ [w^T ( x_i -  x_i)]^2= [w^T (X_{c1} - X_{c2})]^2 = w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w$</li>
<li>分母 = <span class="math inline">\(w^T S_{c1} w + w^T S_{c2} w = w^T (S_{c1}+ S_{c2}) w\)</span>
<ul>
<li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - \frac{1}{N_1} \sum w^T x_j)(w^T x_i - \frac{1}{N_1} \sum w^T x_j)^T \\ = w^T [\frac{1}{N_1}\sum(x_i - X_{c1})(x_i - X_{c1})^T] w = w^T S_{c1} w\)</span></li>
</ul></li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li><span class="math inline">\(L(w) = \frac{w^T S_b w}{w^T S_w w}, \hat{w} = argmax \ L(w)\)</span>
<ul>
<li><span class="math inline">\(\frac{dL}{dw} = 2S_b w (w^T S_w w)^{-1} + (w^T S_b w) * -(w^T S_w w)^{-2} * 2 * S\_w w = 0\)</span></li>
<li><span class="math inline">\(S_b w (w^T S_w w) = (w^T S_b w) S_w w\)</span> （<span class="math inline">\((w^T S_w w)\)</span> 最终计算得一个实数，一维，没有方向）（求解<span class="math inline">\(\hat{w}\)</span>关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li><span class="math inline">\(w = (w^T S_w w)/(w^T S_b w)S_w^{-1}S_b w\)</span>，正比于<span class="math inline">\((S_w^{-1}S_b w)\)</span> ，正比于<span class="math inline">\((S_w^{-1}(X_{c1} - X_{c2}))\)</span></li>
<li>（<span class="math inline">\(S_b w = (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w，(X_{c1} - X_{c2})^T w\)</span>为实数）</li>
<li>（若<span class="math inline">\(S_w\)</span>是对角矩阵，各向同性，<span class="math inline">\(S_w\)</span>正比于单位矩阵，则<span class="math inline">\(w\)</span>正比于<span class="math inline">\((X_{c1} - X_{c2})\)</span></li>
</ul></li>
</ul></li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul></li>
<li><p><strong>Logistic Regression</strong></p>
<ul>
<li>线性回归------&gt;sigmoid-------&gt;线性分类</li>
<li>判别模型</li>
<li>model
<ul>
<li><span class="math inline">\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</span>，将<span class="math inline">\(w^T x\)</span>映射到处于[0,1]区间的概率值p</li>
<li><span class="math inline">\(p_1 = P(y=1|x) = sigmoid(w^T x) = \frac{1}{1+e^{w^T x}}\)</span></li>
<li><span class="math inline">\(p_0 = P(y=0|x) = sigmoid(w^T x) = \frac{e^{w^T x}}{1+e^{w^T x}}\)</span></li>
<li>综合表达：<span class="math inline">\(P(y|x) = p_1^y * p_0^{1-y}\)</span></li>
</ul></li>
<li><span class="math inline">\(\hat{w} = argmax \ P(Y|X) = argmax \ log^{\prod P(y_i|x_i)} = argmax \ \sum log^{P(y_i|x_i)} \\= argmax \ \sum[y_i*log^{p_1} + (1-y_i)*log^{p_0}] \Leftrightarrow (-cross entropy)\)</span>
<ul>
<li>MLE &lt;=&gt; loss function (min cross entropy)</li>
</ul></li>
</ul></li>
<li><strong>Gaussian Discriminant Analysis</strong>
<ul>
<li>生成模型、连续
<ul>
<li><span class="math inline">\(\hat{y} = argmax \ P(y|x) = argmax \ P(x|y)P(y)\)</span></li>
<li>分类：最终比较<span class="math inline">\(P(y=0|x)，P(y=1|x)\)</span>大小</li>
<li><span class="math inline">\(P(y|x)\)</span>正比于<span class="math inline">\(P(x|y)P(y)\)</span>，即联合概率<span class="math inline">\(P(x, y)\)</span></li>
</ul></li>
<li>Data：N个d维样本，二分类(0,1)，正样本个数<span class="math inline">\(N_1\)</span>，方差<span class="math inline">\(S_1\)</span>，负样本个数<span class="math inline">\(N_2\)</span>，方差<span class="math inline">\(S_2\)</span></li>
<li><strong>prior probability</strong>
<ul>
<li>先验概率服从伯努利分布</li>
<li>y ~ Bernoulli，<span class="math inline">\(P(y=1) = p，P(y=0) = 1-p\)</span></li>
<li><span class="math inline">\(P(y) = p^y(1-p)^{1-y}\)</span></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li>
<li>x|y=1 ~ N(u<sub>1</sub>, σ<sup>2</sup>)</li>
<li>x|y=0 ~ N(u<sub>2</sub>, σ<sup>2</sup>)</li>
<li>方差一样（权值共享），均值不一样</li>
<li><span class="math inline">\(P(x|y) = N(u_1, σ^2)^y * N(u_2, σ^2)^{1-y}\)</span></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li><span class="math inline">\(log^{MLE} → L(θ) = log^{\prod P(x_i, y_i)} = \sum log^{P(x_i|y_i)P(y_i)} = \sum [log^{P(x_i|y_i)} + log^{P(y_i)}] \\ = \sum [log^{N(u_1, σ^2)^{y_i} * N(u_2, σ^2)^{1-y_i}} + log^{p^{y_i}*(1-p)^{1-y_i}}] \\ = \sum[y_ilog^{N(u_1, σ^2)} + (1-y_i)log^{N(u_2, σ^2)} + y_ilog^p + (1-y_i)log^{1-p}]\)</span></li>
<li><span class="math inline">\(θ = (u_1, u_2, σ, p)\)</span></li>
<li><span class="math inline">\(\hat{θ} = argmax \ L(θ)\)</span></li>
<li>求解4个参数
<ul>
<li><span class="math inline">\(p\)</span>
<ul>
<li>相关部分 <span class="math inline">\(L = \sum [y_ilog^p + (1-y_i)log^{1-p}]\)</span></li>
<li><span class="math inline">\(\frac{dL}{dp} = \sum [\frac{y_i}{p} - \frac{1-y_i}{1-p}] = 0 → \sum [y_i(1-p)- (1-y_i)p] = \sum(y_i - p) = 0\)</span></li>
<li><span class="math inline">\(\hat{p} = \frac{1}{N} \sum y_i = \frac{N_1}{N}\)</span>（二分类（0,1），<span class="math inline">\(\sum y_i = N_1\)</span>）</li>
</ul></li>
<li><span class="math inline">\(u_1\)</span> （同理 <span class="math inline">\(u_2\)</span>）
<ul>
<li>相关部分 <span class="math inline">\(L = \sum y_ilog^{N(u_1, σ^2)} = \sum y_ilog^{\frac{1}{(2\pi)^{\frac{d}{2}}σ^{\frac{1}{2}}} e^{\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ}}}\)</span></li>
<li><span class="math inline">\(u_1 = argmax L ∝ argmax \sum y_i\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ} = argmax \frac{-1}{2} \sum y_i[(x_i-u_1)^T(x_i-u_1)σ^{-1}] \\ = argmax \frac{-1}{2} \sum y_i[x_i^Tσ^{-1}x_i-2u_1^Tσ^{-1}x_i+u_1^Tσ^{-1}u_1]\)</span></li>
<li><span class="math inline">\(\frac{dL}{du_1} = \frac{-1}{2}\sum y_i[-2σ^{-1} x_i + 2σ^{-1} u_1] = 0 → \sum y_i(u_1 - x_i) = 0\)</span></li>
<li><span class="math inline">\(u_1 = \frac{\sum y_i x_i}{\sum y_i} = \frac{\sum y_i x_i}{N_1}\)</span></li>
</ul></li>
<li>σ
<ul>
<li>相关部分<span class="math inline">\(L = \sum [y_ilog^{N(u_1, σ^2)}+(1-y_i)log^{N(u_2, σ^2)}] = \sum [log^{N(u_1, σ^2)}+log^{N(u_2, σ^2)}]\)</span>
<ul>
<li>（二分类，非0即1，可以拆分算，可以省去<span class="math inline">\(y_i\)</span>）</li>
</ul></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li>
<li><span class="math inline">\(σ = \frac{1}{N} (N_1S_1 + N_2S_2)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Naive Bayes</strong></p>
<ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong>
<ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
<li>最简单概率图模型</li>
</ul></li>
<li>生成模型、离散</li>
<li>Data
<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c<sub>1</sub>, c<sub>2</sub>, ...,c<sub>k</sub>}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul></li>
<li><strong>prior probability</strong>
<ul>
<li>P(Y=c<sub>k</sub>)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li><span class="math inline">\(P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(d)}=x^{(d)}|Y=c_k) = \prod_{j}^{d} P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li>
<li>特征条件独立（上标表示第j-th维度）</li>
</ul></li>
<li><strong>joint probability distributions</strong>
<ul>
<li>联合概率分布</li>
<li><span class="math inline">\(P(X, Y) = P(X|Y)P(Y)\)</span></li>
</ul></li>
<li><strong>posterior probability</strong>
<ul>
<li><span class="math inline">\(P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum P(X=x|Y=c_k)P(Y=c_k)} = \frac{P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}\)</span></li>
<li>则分类器为
<ul>
<li><span class="math inline">\(y=f(x) = argmax \ P(Y=c_k|X=x) ∝ argmax \ P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li>
<li>分母不变，则仅与分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
<li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li>
</ul></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li><span class="math inline">\(L(Y, f(x)) = \left\{\begin{matrix}  1 \ if \ Y!= f(x)\\  0 \ if \ Y=f(x) \end{matrix}\right.\)</span>
<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul></li>
</ul></li>
<li>期望风险函数：<span class="math inline">\(R_{exp(f)} = E[L(Y, f(x))]\)</span>
<ul>
<li>根据联合概率分布：<span class="math inline">\(R_{exp(f)} = E_x \sum [L(c_k|f(x))]P(c_k|X)\)</span></li>
</ul></li>
<li><span class="math inline">\(f(x) = argmin \ Σ[L(c_k|f(x))]P(c_k|X)\)</span>
<ul>
<li>根据L(Y, f(x))函数展开，消去Y=f(x)项</li>
<li><span class="math inline">\(f(x) = argmin \ \sum P(y \neq c_k|X=x) = argmin \ (1-P(y=c_k|X=x)) = argmax \ P(y=c_k|X=x)\)</span></li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c<sub>k</sub>类别的最大概率为多少）</li>
</ul></li>
<li>详情案例见统计学习方法(第二版)63页</li>
<li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckfyznogw0010f5eghmxahpgd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/10/02/Decision-Tree/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Decision_Tree
        
      </div>
    </a>
  
  
    <a href="/2020/08/12/Bayes-MRF/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Bayes_MRF</div>
    </a>
  
</nav>

  
</article>



    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });
      </script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
          <li>
            <a href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
          </li>
        
          <li>
            <a href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
          </li>
        
          <li>
            <a href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>