<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Statistics | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (f(x)=w^T x+b), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化（Neu">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistics">
<meta property="og:url" content="http://yoursite.com/2020/09/11/Statistics/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (f(x)=w^T x+b), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化（Neu">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png">
<meta property="og:updated_time" content="2020-10-06T12:05:08.277Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistics">
<meta name="twitter:description" content="Statistics for Machine Learning One. 两大派系  频率派：统计机器学习（model (f(x)=w^T x+b), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为优化问题）  正则化（L1,L2） 核化（Kernel SVM） 集成化（AdaBoost，RandForest） 层次化（Neu">
<meta name="twitter:image" content="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Statistics
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="statistics-for-machine-learning">Statistics for Machine Learning</h4>
<h5 id="one.-两大派系">One. 两大派系</h5>
<ul>
<li><strong>频率派：统计机器学习</strong>（model (f(x)=w^T x+b), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）
<ul>
<li>正则化（L1,L2）</li>
<li>核化（Kernel SVM）</li>
<li>集成化（AdaBoost，RandForest）</li>
<li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li>
</ul></li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））
<ul>
<li>有向图：Bayesian Network (Deep Directed Network)
<ul>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder (VAE)</li>
<li>GAN</li>
</ul></li>
<li>无向图：Markov Network (Deep Boltzmann Network )</li>
<li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li>
<li>统称 Deep Generative Model（但深层很难计算）</li>
<li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li>
</ul></li>
<li>Data
<ul>
<li>X: data, X={x_1, x_2, ..., x_n}^T Dimension(N, P)</li>
<li>θ: parameter, X~p(X|θ)</li>
</ul></li>
<li><strong>频率派</strong>
<ul>
<li>θ为未知常数；X为随机变量</li>
<li>loss = P(X|θ) = Π P(x_i|θ)
<ul>
<li>x_1, x_2, ..., x_n之间独立同分布</li>
</ul></li>
<li>Maximum Likelihood Estimation 极大似然估计
<ul>
<li>θ_MLE = argmax_θ log P(X|θ)</li>
</ul></li>
</ul></li>
<li><strong>贝叶斯派</strong>
<ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率
<ul>
<li>P(θ|X) = P(X, θ)/P(X) = P(X|θ)P(θ)/P(X) = likelihood*prior / ∫_θ P(X|θ) dθ</li>
</ul></li>
<li>Maximum A Posterior Probability 最大后验概率
<ul>
<li>找到θ在分布中最大值点</li>
<li>θ_MLE = argmax_θ P(X|θ) = argmax_θ P(X|θ)P(θ)</li>
<li>P(θ|X)其分母是不变的，则θ_MLE与分子成正比，只需要算分子</li>
</ul></li>
<li>贝叶斯估计
<ul>
<li>P(θ|X) = P(X|θ)P(θ) / ∫_θ P(X|θ) dθ</li>
<li>必须完整计算整个分子式</li>
</ul></li>
<li>贝叶斯预测
<ul>
<li>X (train), X~ (test), X -&gt; θ -&gt; X~</li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li>P(X~|X) = ∫_θ P(X~, θ|X) dθ = ∫_θ P(X~|X)P(θ|X) dθ</li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-linear-regression">Two. Linear Regression</h5>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线f(w) = w^T x + b（偏置b可先忽略）</p>
<ul>
<li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）
<ul>
<li>线性（属性线性、全局线性、系数线性）
<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul></li>
<li>全局性
<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul></li>
<li>数据未加工
<ul>
<li>预处理：PCA，流行</li>
</ul></li>
</ul></li>
<li><strong>矩阵表达</strong>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）
<ul>
<li><strong>L(w) = Σ||w^T x_i - y_i||^2</strong> = Σ(w^T x_i - y_i)^2 = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y</li>
</ul></li>
<li><strong>w~ = argmin L(w)</strong></li>
<li>求导 dL/dw = 2 X^T X w - 2 X^T Y = 0 ----&gt; X^T X w = X^T Y
<ul>
<li><strong>w~ = (X^T X)^-1 X^T Y</strong> (伪逆：(X^T X)^-1 X^T)</li>
</ul></li>
<li>x_3的误差为(w^T x_3 - y_3)，即所有误差分成一小段一小段</li>
</ul></li>
<li><strong>几何意义</strong>
<ul>
<li>f(w) = w^T x &lt;=&gt; f(β) = x^T β</li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li>目标：在p维空间中找到一条直线f(β)离Y最近，即Y在p维空间的投影
<ul>
<li><p>若向量a与向量b垂直，则 a^T b = 0</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li>
<li>虚线为(Y - Xβ) 与X的p维空间垂直，X^T (Y-Xβ) = 0 ----&gt; β = (X^T X)^-1 X^T Y</li>
<li>误差分散在p个维度上</li>
</ul></li>
</ul></li>
<li><strong>概率角度</strong>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ^2)</li>
<li>y = f(w) + ε = w^T x + ε
<ul>
<li>y|x,w ~ N(w^T x, σ^2) &lt;=&gt; <strong>P(y|x,w)</strong> =1/(sqrt(2*pi)*σ) exp(-(y - w^T x)<sup>2/(2*σ</sup>2))</li>
</ul></li>
<li>定义log-likelihood：
<ul>
<li>L_MLE (w) = log P(y|x,w) = log Π P(y_i|x_i,w) = Σ log P(y_i|x_i,w) = Σ[log(1/(sqrt(2*pi)*σ)) + log(exp(-(y_i - w^T x_i)<sup>2/(2*σ</sup>2)))] = Σ[log(1/(sqrt(2*pi)*σ)) -(y_i - w^T x_i)<sup>2/(2*σ</sup>2)]</li>
<li>样本之间独立同分布</li>
<li>w~ = argmax L_MLE (w) = argmax -(y_i - w^T x_i)<sup>2/(2*σ</sup>2) = argmin (y_i - w^T x_i)^2</li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul></li>
</ul></li>
<li><strong>Regularization 正则化</strong>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong>
<ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：argmin [L(w)+λP(w)]</li>
</ul></li>
<li>L1 -&gt; Lasso
<ul>
<li>P(w) = ||w||_1</li>
</ul></li>
<li>L2 -&gt; Ridge 岭回归
<ul>
<li>P(w) = ||w||_2 = w^T w</li>
<li>权值衰减</li>
<li>J(w) = Σ||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w = w<sup>T(X</sup>T X + λ I) w - 2 w^T X^T Y + Y^T Y
<ul>
<li>w~ = argmin J(w)</li>
<li>dJ/dw = 2 (X^T X + λ I)w - 2 X^T Y = 0, w~ = (X^T X + λ I)^-1 X^T Y</li>
<li>X^T X 是半正定矩阵+对角矩阵=(X^T X + λ I)正定矩阵，必然<strong>可逆</strong></li>
</ul></li>
</ul></li>
<li>贝叶斯的角度
<ul>
<li>参数w服从分布，w~N(0,σ_0^2) ---&gt; <strong>P(w)</strong> = 1/(sqrt(2*pi)*σ_0) exp(||w||<sup>2/(2*σ_0</sup>2))</li>
<li>P(w|y) = P(y|w)P(w) / P(y)</li>
<li>MAP: w~ = argmax P(w|y) = argmax P(y|w)P(w) = argmax log(P(y|w)P(w)) = argmax log[1/(2*pi*σ_0*σ) exp(-(y_i - w^T x_i)<sup>2/(2*σ</sup>2) -||w||<sup>2/(2*σ_0</sup>2))] = argmin [(y_i - w^T x_i)^2 + σ<sup>2/σ_0</sup>2||w||^2] = argmin [L(w)+λP(w)]</li>
<li>λ = σ<sup>2/σ_0</sup>2</li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-linear-classification">Three. Linear Classification</h5>
<ul>
<li>线性回归------&gt;激活函数，降维-------&gt;线性分类</li>
<li>硬分类：0/1
<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul></li>
<li>软分类：[0,1]区间内概率
<ul>
<li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li>
<li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li>
</ul></li>
<li><p><strong>Perceptron 感知机</strong> (1958年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>判别模型</li>
<li>样本：{(x_i, y_i)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：f(x) = sign(w^T x + b) （w^T x大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）
<ul>
<li>L(w) = Σ I{y_i * (w^T x_i) &lt; 0} （非连续函数，不可导）</li>
<li>L(w) = Σ -y_i w^T x_i, dL = -y_i x_i</li>
</ul></li>
<li>若是非线性可分，可是使用pocket algorithm</li>
<li>从感知机到深度学习（发展历程）
<ul>
<li>1958年提出PLA</li>
<li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li>
<li>1981年提出MLP（多层感知机），FeedForward Neural Network</li>
<li>1986年BP+MLP，RNN</li>
<li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li>
<li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li>
<li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li>
<li>2009年GPU发展，2011年speech，2012年ImageNet</li>
<li>2013年Variational Autoencoder (VAE)</li>
<li>2014年GAN</li>
<li>2016年Alpha Go</li>
<li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li>
<li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li>
</ul></li>
</ul></li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数N_1，均值X_c1，方差S_c1，负样本个数N_2，均值X_c2，方差S_c2，（S_c1 = 1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T）</li>
<li>思想：类内小，类间大
<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 w^T x（因为垂直，则Z平面即w向量）
<ul>
<li>（前提设置||w||=1）</li>
<li>则样本点投影到Z平面为：|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i</li>
</ul></li>
</ul></li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong>
<ul>
<li>N_1 = 1/N_1 Σ w^T x_i</li>
<li>S_1 = 1/N_1 Σ (w^T x_i - Z_1) (w^T x_i - Z_1)^T</li>
<li>类间：(Z_1-Z_2)^2</li>
<li>类内：S_1+S_2</li>
</ul></li>
<li>策略：L(w) = (Z_1-Z_2)^2 / (S_1+S_2) = [w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w] / [ w^T (S_c1+ S_c2) w ]
<ul>
<li>分子 = [w^T (1/N_1 Σ x_i - 1/N_2 Σ x_i)]^2 = [w^T (X_c1 - X_c2)]^2 = w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w</li>
<li>分母 = w^T S_c1 w + w^T S_c2 w = w^T (S_c1+ S_c2) w
<ul>
<li>S_1 = 1/N_1 Σ (w^T x_i - 1/N_1 Σ w^T x_j)(w^T x_i - 1/N_1 Σ w^T x_j)<sup>T=w</sup>T [1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T] w = w^T S_c1 w</li>
</ul></li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li>L(w) = w^T S_b w / w^T S_w w</li>
<li>w~ = argmax L(w)
<ul>
<li>dL/dw = 2*S_b w (w^T S_w w)^-1 + (w^T S_b w) * (-1) (w^T S_w w)^-2 * 2 * S_w w = 0</li>
<li>S_b w (w^T S_w w) = (w^T S_b w) S_w w （(w^T S_w w) 最终计算得一个实数，一维，没有方向）（求解w～关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li>w = (w^T S_w w)/(w^T S_b w) * S_w^-1 * S_b w，正比于(S_w^-1 * S_b w) ，正比于(S_w^-1 *(X_c1 - X_c2))</li>
<li>（S_b w = (X_c1 - X_c2)(X_c1 - X_c2)^T w，(X_c1 - X_c2)^T w为实数）</li>
<li>（若S_w是对角矩阵，各向同性，S_w正比于单位矩阵，则w正比于(X_c1 - tX_c2)</li>
</ul></li>
</ul></li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul></li>
<li><strong>Logistic Regression</strong>
<ul>
<li>线性回归------&gt;sigmoid-------&gt;线性分类</li>
<li>判别模型</li>
<li>model
<ul>
<li>sigmoid(x) = 1/(1+e<sup>-x)，将w</sup>T x映射到处于[0,1]区间的概率值p</li>
<li>p_1 = P(y=1|x) = sigmoid(w^T x) = 1/(1+e<sup>(w</sup>T x))</li>
<li>p_0 = P(y=0|x) = sigmoid(w^T x) = e<sup>(w</sup>T x)/(1+e<sup>(w</sup>T x))</li>
<li>综合表达：P(y|x) = p_1^y * p_0^(1-y)</li>
</ul></li>
<li>w~ = argmax P(Y|X) = argmax log Π P(y_i|x_i) = argmax Σ log P(y_i|x_i) = argmax Σ [y_i*log p_1 + (1-y_i)*log p_0] （-cross entropy）
<ul>
<li>MLE &lt;=&gt; loss function (min cross entropy)</li>
</ul></li>
</ul></li>
<li><strong>Gaussian Discriminant Analysis</strong>
<ul>
<li>生成模型、连续
<ul>
<li>y~ = argmax P(y|x) = argmax P(x|y)P(y)</li>
<li>分类：最终比较P(y=0|x)，P(y=1|x)大小</li>
<li>P(y|x)正比于P(x|y)P(y)，即联合概率P(x, y)</li>
</ul></li>
<li>Data：N个d维样本，二分类(0,1)，正样本个数N_1，方差S_1，负样本个数N_2，方差S_2</li>
<li><strong>prior probability</strong>
<ul>
<li>先验概率服从伯努利分布</li>
<li>y ~ Bernoulli，P(y=1) = p，P(y=0) = 1-p</li>
<li>P(y) = p<sup>y*(1-p)</sup>(1-y)</li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li>
<li>x|y=1 ~ N(u_1, σ)</li>
<li>x|y=0 ~ N(u_2, σ)</li>
<li>方差一样（权值共享），均值不一样</li>
<li>P(x|y) = N(u_1, σ)^y * N(u_2, σ)^(1-y)</li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>log MLE =&gt; L(θ) = log Π P(x_i, y_i) = Σ log [P(x_i|y_i)P(y_i)] = Σ[log P(x_i|y_i) + log P(y_i)] = Σ[log N(u_1, σ)^y_i * N(u_2, σ)^(1-y_i) + log p<sup>y_i*(1-p)</sup>(1-y_i)] = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ) + y_i*log p + (1-y_i)*log (1-p)]</li>
<li>θ = (u_1, u_2, σ, p)</li>
<li>θ ~ = argmax L(θ)</li>
<li>求解4个参数
<ul>
<li>p
<ul>
<li>相关部分 L = Σ [log p^y_i + log (1-p)^(1-y_i)]</li>
<li>dL/dp = Σ [y_i/p - (1-y_i)/(1-p)] = 0 =&gt; Σ [y_i*(1-p)- (1-y_i)*p] = Σ (y_i - p) = 0</li>
<li>p~ = 1/N Σ y_i = N_1/N（二分类（0,1），Σ y_i = N_1）</li>
</ul></li>
<li>u_1 （同理 u_2）
<ul>
<li>相关部分 L = Σ y_i*log N(u_1, σ) = Σ y_i*log [1/((2*pi)<sup>(d/2)*σ</sup>(1/2)) exp((x_i-u_1)^T(x_i-u_1)/-2*σ)</li>
<li>u_1 = argmax L = argmax Σ y_i * [(x_i-u_1)^T(x_i-u_1)/-2*σ] = argmax -1/2 Σ y_i * [(x_i-u_1)<sup>T(x_i-u_1)σ</sup>-1] = argmax -1/2 Σ y_i * [x_i^T σ^-1 x_i - 2*u_1^T σ^-1 x_i + u_1^T σ^-1 u_1]</li>
<li>dL/du_1 = -1/2 Σ y_i * [-2*σ^-1 x_i + 2*σ^-1 u_1] = 0 =&gt; Σ y_i * (u_1 - x_i) = 0</li>
<li>u_1 = Σ y_i x_i / Σ y_i = Σ y_i x_i / N_1</li>
</ul></li>
<li>σ
<ul>
<li>相关部分 L = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ)] = Σlog N(u_1, σ) + Σlog N(u_2, σ)
<ul>
<li>（二分类，非0即1，可以拆分算，可以省去y_i）</li>
</ul></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li>
<li>σ = 1/N (N_1*S_1 + N_2*S_2)</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Naive Bayes</strong>
<ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong>
<ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
<li>最简单概率图模型</li>
</ul></li>
<li>生成模型、离散</li>
<li>Data
<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c_1, c_2, ...,c_k}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul></li>
<li><strong>prior probability</strong>
<ul>
<li>P(Y=c_k)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>P(X=x|Y=c_k) = P(X<sup>(1)=x</sup>(1), ..., X<sup>(d)=x</sup>(d)|Y=c_k) = ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>特征条件独立</li>
<li>^(1) 上标表示第1维度</li>
</ul></li>
<li><strong>joint probability distributions</strong>
<ul>
<li>联合概率分布</li>
<li>P(X, Y) = P(X|Y)P(Y)</li>
</ul></li>
<li><strong>posterior probability</strong>
<ul>
<li>P(Y=c_k|X=x) = P(X=x|Y=c_k)P(Y=c_k)/ΣP(X=x|Y=c_k)P(Y=c_k) = P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)/Σ P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>则分类器为
<ul>
<li>y=f(x) = argmax P(Y=c_k|X=x) = argmax P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>分母不变，则仅于分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
<li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li>
</ul></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li>L(Y, f(x)) = 1 if Y!=f(x) or 0 if Y==f(x)
<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul></li>
</ul></li>
<li>期望风险函数：R_exp(f) = E[L(Y, f(x))]
<ul>
<li>根据联合概率分布：R_exp(f) = E_x Σ[L(c_k|f(x))]P(c_k|X)</li>
</ul></li>
<li>f(x) = argmin Σ[L(c_k|f(x))]P(c_k|X)
<ul>
<li>根据L(Y, f(x))函数展开，消去Y==f(x)项</li>
<li>f(x) = <strong>argmin ΣP(y!=c_k|X=x)</strong> = argmin (1-P(y=c_k|X=x)) = <strong>argmax P(y=c_k|X=x)</strong></li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c_k类别的最大概率为多少）</li>
</ul></li>
<li>详情案例见统计学习方法(第二版)63页</li>
<li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckfyy7lca000xijeggj39y6tw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/10/02/Decision-Tree/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Decision_Tree
        
      </div>
    </a>
  
  
    <a href="/2020/08/12/Bayes-MRF/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Bayes_MRF</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
          <li>
            <a href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
          </li>
        
          <li>
            <a href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
          </li>
        
          <li>
            <a href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>