<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>SVM | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Data : N个p维样本 X（维度N×p），y_i = {-1,1} SVM 三宝：间隔，对偶，核技巧  提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即所有样本距离平面都足够大） hard-margin SVM（硬间隔）  最大间隔分类器 = max margin(w, b) s.t. y_i(w^T x_i+b) &amp;gt; 0 for i = 1">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM">
<meta property="og:url" content="http://yoursite.com/2020/10/02/SVM/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Data : N个p维样本 X（维度N×p），y_i = {-1,1} SVM 三宝：间隔，对偶，核技巧  提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即所有样本距离平面都足够大） hard-margin SVM（硬间隔）  最大间隔分类器 = max margin(w, b) s.t. y_i(w^T x_i+b) &amp;gt; 0 for i = 1">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png">
<meta property="og:updated_time" content="2020-10-05T05:12:03.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SVM">
<meta name="twitter:description" content="Data : N个p维样本 X（维度N×p），y_i = {-1,1} SVM 三宝：间隔，对偶，核技巧  提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即所有样本距离平面都足够大） hard-margin SVM（硬间隔）  最大间隔分类器 = max margin(w, b) s.t. y_i(w^T x_i+b) &amp;gt; 0 for i = 1">
<meta name="twitter:image" content="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-SVM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/SVM/" class="article-date">
  <time datetime="2020-10-02T08:50:04.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SVM
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Data : N个p维样本 X（维度N×p），y_i = {-1,1}</li>
<li><p>SVM 三宝：间隔，对偶，核技巧</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p></li>
<li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</li>
<li><strong>hard-margin SVM</strong>（硬间隔）
<ul>
<li>最大间隔分类器 = max margin(w, b) s.t. y_i(w^T x_i+b) &gt; 0 for i = 1,...,N</li>
<li>点到直线距离，垂直线最短</li>
<li>margin(w, b) = min distance(w, b, x_i) = <strong>min 1/||w|| |w^T x_i + b|</strong></li>
<li>→ max_w,b min_x 1/||w|| |w^T x_i + b| (s.t. y_i(w^T x_i+b) &gt; 0) = max_w,b 1/||w|| min_x y_i(w^T x_i + b) (存在r&gt;0，使y_i(w^T x_i + b) =r，可以设置r=1)</li>
<li>→ max 1/||w|| s.t. min y_i(w^T x_i + b) = 1 → <strong>min 1/2 w^T w s.t. y_i(w^T x_i + b) &gt;=1</strong>(convex optimization 二次凸优化问题)(primal problem原问题)</li>
<li>拉格朗日：L(w, b, λ) = 1/2 w^T w + Σ λ_i(1-y_i(w^T x_i + b)) (λ_i &gt;= 0, (1-y_i(w^T x_i + b)) &lt;= 0；若(1-y_i(w^T x_i + b))&gt;0，L为正无穷，无解；仅在 λ_i=0， (1-y_i(w^T x_i + b)) =0，达到最大L)</li>
<li><strong>primal problem 原问题</strong>&lt;=&gt; <strong>min_w,b max_λ L(w, b, λ) s.t. λ_i &gt;= 0</strong> （对w，b没有限制）→ min 1/2 w^T w</li>
<li><strong>dual problem 对偶问题</strong>：<strong>max_λ min_w,b L(w, b, λ) s.t. λ_i &gt;= 0</strong>
<ul>
<li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li>
<li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li>
<li>dL/db = d[Σ λ_i(1-y_i(w^T x_i + b))]/db = d[- Σ λ_i y_i b]/db = -<strong>Σ λ_i y_i =0</strong>，带入原式L = 1/2 w^T w + Σ λ_i - Σ λ_i y_i w^T x_i</li>
<li>dL/dw = w - Σ λ_i y_i x_i = 0 → <strong>w = Σ λ_i y_i x_i</strong>，带入原式L = 1/2 w^T w + Σ λ_i - w^T w = Σ λ_i - 1/2 w^T w</li>
<li>→ <strong>min 1/2 w^T w - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</strong> （此处不能求λ偏导）</li>
<li>→ min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li><strong>KKT条件</strong>：参数偏导为0（dL/db=0，dL/dw=0，dL/dλ=0），λ_i(1-y_i(w^T x_i + b))=0， λ_i&gt;=0，(1-y_i(w^T x_i + b)&lt;=0</li>
<li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li>
<li>w~ = Σ λ_i y_i x_i, （存在x_k, y_k，1-y_k(w^T x_k + b)=0）b~ = y_k - w^T x_k = y_k - (Σ λ_i y_i x_i^T) x_k</li>
</ul></li>
<li><p><strong>f(x) = sign(w~^T x + b~)</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p>
<ul>
<li>落在虚线的样本点就是x_k（y_k(w^T x_k + b)=1），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li>
</ul></li>
</ul></li>
<li><strong>soft-margin SVM</strong>（软间隔）
<ul>
<li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li>
<li>soft-margin SVM在hard-margin SVM基础上允许一点点错误，min 1/2 w^T w + loss
<ul>
<li>分错点的个数：loss = Σ I{y_i(w^T x_i + b)&lt;1} （关于w是不连续的，无法求导，因此不采取）</li>
<li><p>hinge 距离：hinge loss = max{0, 1-y_i(w^T x_i + b)}</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p></li>
<li>min 1/2 w^T w + C Σ max{0, 1-y_i(w^T x_i + b)} s.t. y_i(w^T x_i + b)&gt;=1 （超参数C）</li>
<li><p>→ 设定ξ_i = y_i(w^T x_i + b)，<strong>min 1/2 w^T w + C Σ ξ_i</strong> s.t. y_i(w^T x_i + b)&gt;=1-ξ_i, ξ_i&gt;=0（同样用对偶问题方式来求解）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p></li>
</ul></li>
</ul></li>
<li>约束优化问题
<ul>
<li>primal problem 原问题：<strong>min f(x) s.t. m_i(x)&lt;=0, n_j(x)=0 (i=1,...,M, j=1,...,N)</strong></li>
<li>原问题的无约束形式（关于x的函数）：拉格朗日：L(x, λ, η) = f(x) + Σ λ_i m_i(x) + Σ η_i n_i(x) → <strong>min_x max_λ,η L(x, λ, η) s.t. λ_i&gt;=0</strong></li>
<li>证明两者等价：如果违法约束m_i(x)&gt;0，max_λ L → ∞；反之，max_λ L 必有最大值（λ_i=0时）（即排除了m_i(x)&gt;0情况，过滤掉违反约束的情况）</li>
<li>dual problem 对偶问题（关于λ,η的函数）：<strong>max_λ,η min_x L(x, λ, η) s.t. λ_i&gt;=0</strong>
<ul>
<li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （max_λ,η min_x L(x, λ, η) &lt;= min_x max_λ,η L(x, λ, η)）</li>
<li>证明：min_x L &lt;= L &lt;= max_λ,η L → A(λ,η) &lt;= L &lt;= B(x) → A(λ,η) &lt;= B(x) → max A(λ,η) &lt;= min B(x)<br>
</li>
<li>→ max_λ,η min_x L(x, λ, η) &lt;= min_x max_λ,η L(x, λ, η)</li>
<li>（在min_x L已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li>
<li>强对偶性：对偶问题=原问题</li>
</ul></li>
<li>几何解释
<ul>
<li>primal problem: min f(x) s.t. m_1(x)&lt;=0 (D定义域，D=dom_f ∩ dom_m_1) <strong>原问题最优解：p* = min f(x)</strong></li>
<li>L(x, λ) = f(x) + λ m_1(x) s.t. λ&gt;=0 <strong>对偶最优解：d* = max_λ min_x L(x, λ)</strong></li>
<li>将问题投影入二维空间：引入集合(区域) G = {(m_1(x), f(x))|x∈D}
<ul>
<li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li>
<li>凸集是指集合内任意两点的连线都在集合内</li>
<li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li>
<li>目标函数f如果不是凸函数，则不是凸优化问题</li>
<li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li>
<li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li>
<li>结论：凸函数的局部最优解就是全局最优解</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p>
<ul>
<li><p><strong>p* = inf {f(x)|(m_1(x), f(x))∈G, m_1(x)&lt;=0}</strong>（集合中没有最小值概念，对应的是下确界）</p>
<ul>
<li>P* 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li>
</ul></li>
<li>d* = max_λ g(λ) , g(λ) = min_x f(x) + λ m_1(x) , <strong>g(λ) = inf {f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G}</strong>
<ul>
<li>一条过原点直线 f(x) + λ m_1(x) = 0 (斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）g(λ) &lt;= p*</li>
<li>当调整斜率λ*，得到一个 g(λ*) = f(x) + λ* m_1(x) 同时与G的俩角相切，此时，直线与纵轴的交点为d*（绿线）</li>
<li><p>d* &lt;= p* （<strong>凸优化+slater条件 → d* = p*</strong>）（SVM是二次规划问题，符合slater条件）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li>slater条件
<ul>
<li>Convex + Slater → Strong Duality （充分不必要条件）</li>
<li>定义：存在x~在relint，使m_i(x)&lt;0 (i=1,...,M)
<ul>
<li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li>
<li>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = Ax + b（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li>
</ul></li>
<li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li>
<li>放松的slater条件：在m_i(x)中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足m_i(x)&lt;0 （凸二次规划问题：目标函数f是凸的，不等式约束m_i是仿射函数，等式约束n_j也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li>
</ul></li>
<li>KKT条件
<ul>
<li>KKT &lt;=&gt; Strong Duality (d* = p*)（充要条件）</li>
<li>从p*得到最优x*，从d*得到λ*、η*</li>
<li><strong>可行域（可行条件）：m_i(x*)&lt;=0, n_j(x*)=0, λ*&gt;=0</strong></li>
<li>互补松弛
<ul>
<li>d* = max_λ,η g(λ,η) = g(λ*, η*) = min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*) = f(x*) + Σ λ_i* m_i(x*) + Σ η_i* n_i(x*) = f(x*) + Σ λ_i* m_i(x*) &lt;= f(x*) = p*</li>
<li>（λ_i&gt;=0，m_i&lt;=0，则 (λ_i m_i) &lt;= 0）</li>
<li><strong>互补松弛条件 ：Σ λ_i* m_i(x*) = 0 → λ_i* m_i(x*)</strong></li>
</ul></li>
<li>梯度为0
<ul>
<li>min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*)</li>
<li>x*是对应x最小值，则 <strong>dL/dx = 0</strong></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>kernel SVM</strong>
<ul>
<li>Kernel Method（思想角度）</li>
<li>Kernel Trick（计算角度）</li>
<li><p>Kernel function</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png" alt="SVM9"></p>
<ul>
<li><strong>非线性带来高维转换（从模型角度）</strong>
<ul>
<li>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</li>
<li>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p>
<ul>
<li>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</li>
<li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）<br>
</li>
<li><font color="red">非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</font></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p>
<ul>
<li>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</li>
<li>三种方法转高维：1、类似MLP直接转高维；2、Kernel方法转高维；3、深度学习运用与或非构建有向无环图（神经网络）（与或非（三种基础运算均可用PLA表示）解决异或问题（复合运算）），神经网络：复合表达式、复合函数、MLP（FeedForward Neural Network）
<ul>
<li><p>XOR：x_1⊕x_2 = (¬x_1∧x_2)∨(x_1∧¬x_2)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png" alt="FNN1" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>对偶表示带来内积（从优化角度）</strong>
<ul>
<li>从频率视角归化到优化问题</li>
<li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题
<ul>
<li>Doul Problem: min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li>内积：x_i^T x_j</li>
<li>非线性转换：Φ(x_i)^T Φ(x_j) （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此Φ(x_i)^T Φ(x_j) 很难i求解和计算量很大）</li>
<li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到Φ(x_i)^T Φ(x_j) 结果</strong>（不需要先求Φ(x)再求内积）</li>
</ul></li>
<li><strong>Kernel function : K(x, x') = Φ(x)^T Φ(x') = &lt;Φ(x), Φ(x')&gt;</strong>
<ul>
<li>存在x, x'∈X，使K(x, x') = Φ(x)^T Φ(x')，则K就是一个核函数（如K(x, x')=exp(-(x-x')<sup>2/(2σ</sup>2))）</li>
<li>蕴含了非线性转换+内积</li>
</ul></li>
</ul></li>
<li>一般核函数指<strong>正定核函数</strong> <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a>
<ul>
<li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x')为核函数</li>
<li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x')，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使K(x, x') = &lt;Φ(x), Φ(x')&gt;，则K(x, x')为正定核函数</li>
<li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x')，若满足两个条件（对称性、正定性）则为正定和函数
<ul>
<li>对称性：K(x, x') = K(x', x)</li>
<li>正定性：任取N个元素，x_1,x_2,...,x_N∈X，对应的Gram矩阵是半正定的（K=[K(x_i, x_j)]）（两个定义等价，即证明：<strong>K(x, x') = &lt;Φ(x), Φ(x')&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li>
<li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（&lt;f, g&gt; = &lt;g, f&gt;）、正定性（内积大于等于0，&lt;f, f&gt; &gt;= 0）、线性性（&lt;r_1 f_1 + r_2 f_2 , g&gt; = r_1 &lt;f_1, g&gt; + r_2 &lt;f_2, g&gt;））<br>
</li>
</ul></li>
<li>必要性证明
<ul>
<li>在Hilbert Space中的Φ(x)具有对称性性质，K(x, x') = &lt;Φ(x), Φ(x')&gt; = &lt;Φ(x'), Φ(x)&gt; = K(x', x)</li>
<li>K=[K(x_i, x_j)]（维度N×N）（半正定：任意a列向量，a^T K a &gt;=0）</li>
<li>a^T K a = Σ Σ a_i a_j K_ij = Σ Σ a_i a_j K(x_i, x_j) = Σ Σ a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; =线性性= Σ Σ a_i a_j Φ(x_i)^T Φ(x_j) = Σ a_i Φ(x_i)^T Σ a_j Φ(x_j) = [Σ a_i Φ(x_i)]^T Σ a_j Φ(x_j) = &lt;Σ a_i Φ(x_i), Σ a_j Φ(x_j)&gt; = ||Σ a_i Φ(x_i),||^2 &gt;= 0，半正定性</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/SVM/" data-id="ckfz5p7k9000qb9egw1e15w1k" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/10/02/Dimensionality-Reduction/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Dimensionality_Reduction
        
      </div>
    </a>
  
  
    <a href="/2020/10/02/Decision-Tree/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Decision_Tree</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
          <li>
            <a href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
          </li>
        
          <li>
            <a href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
          </li>
        
          <li>
            <a href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>