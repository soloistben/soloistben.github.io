<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>MR.C</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-11-26T12:50:12.719Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>(soloistben)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C_plus_note</title>
    <link href="http://yoursite.com/2020/11/17/C-plus-note/"/>
    <id>http://yoursite.com/2020/11/17/C-plus-note/</id>
    <published>2020-11-17T08:15:24.000Z</published>
    <updated>2020-11-26T12:50:12.719Z</updated>
    
    <content type="html"><![CDATA[<h4 id="One-Definition-of-Class-without-pointer"><a href="#One-Definition-of-Class-without-pointer" class="headerlink" title="One. Definition of Class (without pointer)"></a>One. Definition of Class (without pointer)</h4><ul><li><p>防止重复引用头文件（e.g. complex.h 对应 __COMPLEX__）（guard 防卫式声明）</p></li><li><p>public（定义提供外部调用函数），private （<font color="red"><strong>数据放入private</strong></font>，仅限类内用）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li><li><p>Constructor Function 构造函数</p><ul><li>构造函数与类名称一样</li><li>Overloading 重载（一个类可以重载多个构造函数，下面特例不允许）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="comment">// (default argument 默认实参，构造函数不需要写返回类型，默认为类)</span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)      <span class="comment">// initialization list 初始化列表（仅构造函数有）</span></span><br><span class="line">    &#123;...&#125;               <span class="comment">// r, i 也可以在函数体内是assignments赋值</span></span><br><span class="line">    <span class="comment">// （但是比初始化列表慢一点）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">complex</span>() <span class="comment">// 与上面构造函数冲突，不允许这么设置</span></span><br><span class="line">        : re(<span class="number">0</span>), im(<span class="number">0</span>)</span><br><span class="line">    &#123;...&#125; </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li>Destructor Function 析构函数（不带指针的类，一般不用写析构函数）</li><li>Initialization list 初始化列表（仅构造函数有），<font color="red"><strong>优先考虑使用初始化列表</strong></font></li><li><p>构造函数一般写在public，也可以写在private中，但仅限类内调用，外部可用singleton调用，但只能调用一个 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> A&amp; <span class="title">getInstance</span><span class="params">()</span></span>;<span class="comment">// 单例模式</span></span><br><span class="line">    setup() &#123;...&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    A();</span><br><span class="line">    A(<span class="keyword">const</span> A&amp; rhs);</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">A&amp; A::getInstance()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">static</span> A a;<span class="comment">// 只有当调用时，才会创建该对象</span></span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用</span></span><br><span class="line">A::getInstance().setup();</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Template 模板</p><ul><li><p>当未最终确定数据变量的类型时，或者需要多种数据类型，可以<strong>使用Template</strong></p></li><li><p>当设置多种类型，则当前就有上面类的定义，这是模板带来的代码膨胀（这不是缺点，是必须要两套代码）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(T r=<span class="number">0</span>, T i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">T <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function">T <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    T re, im;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="endif"><a href="#endif" class="headerlink" title="endif"></a>endif</h1><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">```c++</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;complex.h&quot;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    complex&lt;double&gt; c1(2.0, 1.0);   // &lt;double&gt; 确定变量类型</span><br><span class="line">complex&lt;int&gt; c1(2, 1);   </span><br><span class="line">    cout&lt;&lt;c1.real();</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></code></pre><ul><li><p>function template 函数模板</p><ul><li><p>当函数内容一样时，仅传入对象类型不一样，即可使用函数模板</p></li><li><p>编译器会对function template进行引数推导(argument deduction)（自动检测是什么类型） </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">T</span>&amp; <span class="title">min</span> (<span class="title">const</span> <span class="title">T</span>&amp; <span class="title">a</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">b</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">return</span> b&lt;a ? b:a;<span class="comment">// 即使是自定义的类，操作符 &lt; 重载就可以</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>namespace 命名空间（防止命名冲突）</p><ul><li>directive: “using namespace std;”    写demo时常用</li><li>declaration: “using std::cout;”  只声明了一个</li></ul></li><li><p>Inline Function 内联函数</p><ul><li>在类内定义的函数，且不是复杂函数，都可以编译为inline函数</li><li>在类外定义的函数，需要在<font color="red">成员函数前设定特有字符 <strong>inline</strong></font></li><li>优点是执行得快</li><li>是否变成inline，由编译器决定（创建权在程序员手上，决定权在编译器上）</li></ul></li><li><p>pass Value &amp; pass Reference 传递参数：传值与传引用</p><ul><li>直接pass value会因为值大小影响函数速度</li><li>引用本质也是指针，指针4个字节，速度快</li><li><font color="red">**提前考虑是否需要const**</font></li><li><p><font color="red"><strong>建议所有参数均传引用，return也尽量返回引用</strong></font>（在变量生命周期外（局部变量）返回引用是错误的，其余情况都可以返回引用）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;</span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>Friend Function 友元函数</p><ul><li><p>可以直接访问private数据，比函数读取private数据更快（用来做特例，破坏类的整体封装性）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// friend function</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">complex</span>&amp; __doapl (<span class="keyword">complex</span>*, <span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;<span class="comment">// 直接读取private变量</span></span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// this可以隐藏，不可以写</span></span><br><span class="line">    <span class="comment">// this是默认存在，不是临时变量</span></span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>相同class的各个object互为friend</strong>，可以直接获取private数据</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="comment">// constructor function </span></span><br><span class="line">      <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">          : re(r), im(i)</span><br><span class="line">      &#123;...&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// inline function</span></span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">func</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; param)</span></span></span><br><span class="line"><span class="function">      </span>&#123; </span><br><span class="line">          <span class="keyword">return</span> param.re+param.im; <span class="comment">// 直接读取private变量</span></span><br><span class="line">      &#125;</span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> re, im;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>operator overloading 操作符重载</p><ul><li>其实跟定义函数一样，只是操作符更直观</li><li>编译器会去寻找相关的operator的被重写的函数（不管用什么方法，只能写一个，<strong>编译器只选择其中一个使用，没有优先级</strong>）</li><li><p>两种方法：</p><ul><li><p>成员函数（类内），默认有this</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// 全局函数</span></span><br><span class="line">  <span class="comment">// 参数1会被改动，参数2不会被改动</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r) <span class="comment">// const complex&amp; r （可以用complex，传值，但会慢）</span></span><br><span class="line">  &#123;</span><br><span class="line">      ths-&gt;re += r.re;</span><br><span class="line">      ths-&gt;im += r.im;</span><br><span class="line">      <span class="keyword">return</span> *ths;    <span class="comment">// 传递者(*ths)无需知道接收者(complex&amp;)是以reference形式接收</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 类的成员函数</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)<span class="comment">// 默认有this，但不能写出来</span></span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);    <span class="comment">// 返回类型必须是引用complex&amp;（防止c3+=c2+=c1; 连续赋值情况）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>非成员函数（类外），无this；写在类外，例如加法有多个情况，全部写在类内是有局限性</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将加法所有情况，都写出来</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// temp object 临时变量（函数执行完就销毁）</span></span><br><span class="line">    <span class="comment">// 不能当成reference返回</span></span><br><span class="line">    <span class="comment">// 要返回value，才能保存生成的临时变量（局部变量）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+real(y), imag(x)+imag(y));   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+y, imag(x));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(x+real(y), imag(y));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> - (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(-real(x), -imag(x));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==real(y) &amp;&amp; imag(x)==imag(y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==y &amp;&amp; imag(x)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(y)==x &amp;&amp; imag(y)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="title">conj</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x), -imag(x)); <span class="comment">// 共轭复数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// '&lt;&lt;' 该操作符只能写成全局函数（非成员函数）</span></span><br><span class="line"><span class="comment">// os 用于显示，一直在其内容变化，不能写成const </span></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; <span class="string">'('</span> &lt;&lt; real(x) &lt;&lt; <span class="string">','</span> &lt;&lt; imag(x) &lt;&lt; <span class="string">')'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="Two-Definition-of-Class-with-pointer"><a href="#Two-Definition-of-Class-with-pointer" class="headerlink" title="Two. Definition of Class (with pointer)"></a>Two. Definition of Class (with pointer)</h4><ul><li><p>当定义类中含有指针，则必须有<font color="red">“拷贝构造”、“拷贝赋值”、“析构函数”</font></p><ul><li>拷贝构造、拷贝赋值属于深拷贝（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</li><li>若不实现深拷贝，系统默认是浅拷贝，即两个指针指向同一内存</li><li><p>析构函数则在对象离开作用域（对象定义的花括号内）后，在析构函数释放数据内存，再销毁对象</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __MY_STRING__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __MY_STRING__</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;ostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造函数</span></span><br><span class="line">    String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr=<span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*带指针的类，必须要有 拷贝构造、拷贝赋值、析构函数*/</span></span><br><span class="line">    <span class="comment">// 拷贝构造（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</span></span><br><span class="line">    <span class="comment">//（如果不重写，系统默认是浅拷贝，即两个指针指向同一内存）</span></span><br><span class="line">    String(<span class="keyword">const</span> String&amp; str);  </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拷贝赋值（深拷贝）</span></span><br><span class="line">    String&amp; <span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 析构函数（带有指针的类，需要在对象离开作用域（对象定义的花括号）后，在系够函数释放数据内存）</span></span><br><span class="line">    ~String();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">char</span>* <span class="title">get_data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> m_data;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 指针 动态分配</span></span><br><span class="line">    <span class="comment">// 数组 固定分配</span></span><br><span class="line">    <span class="keyword">char</span>* m_data;<span class="comment">// 32位，指针4Byte</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (cstr)   <span class="comment">// 传入字符串，不是0</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 最后一位有标识符号</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(cstr)+<span class="number">1</span>]; </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, cstr);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;  <span class="comment">//没有传入字符串</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];</span><br><span class="line">        *m_data = <span class="string">'\0'</span>;     <span class="comment">// 默认只有最后的标识符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 深拷贝</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String&amp; String::<span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 拷贝赋值 </span></span><br><span class="line">    <span class="comment">// 判断 两者是否指向同一内存（必须操作）</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> == &amp;str)</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 清除原指针指向</span></span><br><span class="line">    <span class="keyword">delete</span>[] m_data;</span><br><span class="line">    <span class="comment">// 重新分配内存</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::~String()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">delete</span>[] m_data;<span class="comment">// 数组的分类内存，则需要delete[]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> String&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; x.get_data();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">-------------------------------</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"my_string.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">s1</span><span class="params">(<span class="string">"c++"</span>)</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">s2</span><span class="params">(<span class="string">"hello"</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">s3</span><span class="params">(s1)</span></span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s1&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s2&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    s3 = s2;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Three-Stack-amp-Heap"><a href="#Three-Stack-amp-Heap" class="headerlink" title="Three. Stack &amp; Heap"></a>Three. Stack &amp; Heap</h4><ul><li>stack 栈：是存在于作用域中的一块内存空间。(local object / auto object)<ul><li>当在作用域内调用函数时，函数会形成一个stack来防治</li><li>当在作用域内创建新对象（局部变量），则会开辟新内来存储它，当作用域结束，则会被释放掉（调用析构函数）</li></ul></li><li><p>heap 堆：由操作系统提供的全局空间，动态分配。（每次分配内存，需要手动销毁）（heap object）</p><ul><li>若没有手动delete，会出现内存泄漏（memory leak）</li><li>对象离开作用域，指针p结束了，但指向p的对象仍然存在，没有机会去delete它了</li><li>new 一个对象，先是分配内存，再调用构造函数 -&gt; （c 语言）malloc(n) + 数据转型 +调用构造函数<ul><li>指针p为对象在内存中的起始位置，同时是类中的this</li></ul></li><li>delete：先调用析构函数，再释放内存 -&gt; （c 语言）调用析构函数 + free(p)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Complex</span>&#123;</span>...&#125;;<span class="comment">// 定义类</span></span><br><span class="line">...</span><br><span class="line"><span class="function">Complex <span class="title">c3</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">//global object</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="function">Complex <span class="title">c1</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">// c1所占用空间来自stack，离开作用域则会自动销毁</span></span><br><span class="line">    Complex* p = <span class="keyword">new</span> Complex(<span class="number">3</span>);<span class="comment">// 由系统动态分配全局内存，离开作用域不会自动销毁，则需手动销毁</span></span><br><span class="line">    <span class="keyword">delete</span> p；</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> Complex <span class="title">c2</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">// static object</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>static object 静态对象：离开作用域仍然存在，直至整个程序结束</p><ul><li>静态变量或者函数只有一份内存</li><li>非静态函数或变量，多次被调用，则会产生多份内存</li><li>静态类内变量，则是所有对象共有这个静态数据（必须在给它定义）</li><li><p>静态的成员函数没有 this</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span>// 银行账户类</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span> m_rate;<span class="comment">// 利率多少与用户无关，所以应该是共同一样的，因此为静态变量</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">set_rate</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span>&amp; x)</span></span>&#123;m_rate=x;&#125; <span class="comment">// 静态变量只能由静态函数处理</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> Account::m_rate = <span class="number">8.0</span>;<span class="comment">// 定义静态变量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Account::set_rate(<span class="number">5.0</span>);<span class="comment">// 可以直接调用</span></span><br><span class="line">    </span><br><span class="line">    Account a;</span><br><span class="line">    a.set_rate(<span class="number">5.0</span>);<span class="comment">// 对象调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li>global object 全局对象：作用域为整个程序，直至整个程序结束</li><li><p><strong>new 动态分配内存块</strong>（VC编译器）</p><ul><li><p>Cookie</p><ul><li>当在debug模式调用程序时，除了类的大小，不仅有上下两个cookie（最终大小必须是类的倍数，不足则加pad）而且会分配更多空间（方便系统回收内存）</li><li><p>在正常执行模式调用程序时，只有类的大小和cookie（最终大小必须是类的倍数）</p><p><img src="C-plus-note.assets/动态分配内存.png" alt="动态分配内存" style="zoom: 50%;"></p></li><li><p>程序员只看到绿色那块（类的大小），不会看到完整cookie的大小</p></li><li><strong>Cookie (16): 00000041 表示 大小为4×16=64，1表示操作系统已经分配出去</strong>（因为16进制在二进制后面四个比特位均为0），0表示已经归还给操作系统，记录了整体长度，方便程序知道回收大小</li></ul></li><li><p>动态分配数组<br><img src="C-plus-note.assets/动态分配数组内存.png" alt="动态分配数组内存" style="zoom:67%;"></p><ul><li>3 表示数组大小</li><li>数组的分类内存，则需要delete[]，（系统才知道删除的是个数组），否则会出错<br><img src="C-plus-note.assets/删除数组.png" alt="删除数组" style="zoom:75%;"></li></ul></li></ul></li></ul><h4 id="Four-Object-Oriented-Programming-OOP-Object-Oriented-Design-OOD"><a href="#Four-Object-Oriented-Programming-OOP-Object-Oriented-Design-OOD" class="headerlink" title="Four. Object Oriented Programming (OOP), Object Oriented Design (OOD)"></a>Four. Object Oriented Programming (OOP), Object Oriented Design (OOD)</h4><h4 id="Five"><a href="#Five" class="headerlink" title="Five."></a>Five.</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;One-Definition-of-Class-without-pointer&quot;&gt;&lt;a href=&quot;#One-Definition-of-Class-without-pointer&quot; class=&quot;headerlink&quot; title=&quot;One. Definitio
      
    
    </summary>
    
    
    
      <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>EM</title>
    <link href="http://yoursite.com/2020/10/06/EM/"/>
    <id>http://yoursite.com/2020/10/06/EM/</id>
    <published>2020-10-06T13:07:10.000Z</published>
    <updated>2020-10-06T13:07:10.975Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    <summary type="html">
    
      
      
        

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hidden_Markov_Model</title>
    <link href="http://yoursite.com/2020/10/06/Hidden-Markov-Model/"/>
    <id>http://yoursite.com/2020/10/06/Hidden-Markov-Model/</id>
    <published>2020-10-06T12:55:04.000Z</published>
    <updated>2020-10-06T15:17:47.984Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Hidden Markov Model = Markov Random Field + Time</strong><ul><li>动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）</li><li><p>GMM高斯混合模型（样本之间独立同分布）；但是动态模型，样本之间不是独立同分布</p><p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM1.png" alt="HMM1" style="zoom: 67%;"></p></li><li>图中动态模型包括：横向的时间关系（Time），纵向的混合关系（Mixture，不同变量混合）</li><li><p><strong>若hidden variable是离散的，则动态模型为HMM</strong>；线性连续的，则是Kalmm Filter 卡尔曼滤波器；非线性连续的，则是Partide Filter</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM2.png" alt="HMM2" style="zoom:75%;"></p></li><li>HMM: param λ = (π, A, B)（π是初始概率分布、A转移矩阵、发射矩阵）<ul><li><strong>observed variable O</strong>: o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>t</sub>,... → V = {v<sub>1</sub>, ...v<sub>M</sub>} （观察值，观测变量o的值域）</li><li><strong>hidden variable I</strong>: i<sub>1</sub>, i<sub>2</sub>, ..., i<sub>t</sub>,... → Q = {q<sub>1</sub>, ...q<sub>N</sub>} （隐变量i的值域）</li><li><span class="math inline">\(A = [a_{ij}], a_{ij} = P(i_{t+1}=q_{j}|i_{t}=q_{i})\)</span></li><li><span class="math inline">\(B = [b_{j(k)}], b_{j(k)} = P(o_{t}=v_{k}|i_{t}=q_{j})\)</span></li></ul></li><li>两个假设<ul><li>齐次马尔可夫假设<ul><li><span class="math inline">\(P(i_{t+1}|i_{t}, i_{t-1}, ..., i_{1}, o_{t}, o_{t-1}, ..., o_{1}) = P(i_{t+1}|i_{t})\)</span></li><li>隐状态i<sub>t+1</sub>只与隐状态i<sub>t</sub>有关</li></ul></li><li>观测独立假设<ul><li><span class="math inline">\(P(o_{t}|i_{t}, i_{t-1}, ..., t_{1}, o_{t-1}, ..., o_{1}) = P(o_{t}|i_{t})\)</span></li><li>观测状态o<sub>t</sub>只与观测状态i<sub>t</sub>有关</li></ul></li></ul></li><li>HMM解决三个问题（已知参数 λ = (π, A, B)）<ul><li>Evalution → P(O|λ) （已知参数下，求解O现象的概率）→ 前向后向算法</li><li>Learning → 求解参数，λ = argmax P(O|λ) → EM算法（以前是Baum Welch算法）</li><li>Decoding → 找到一个状态序列，使 P(I|O)达到最大（I～ = argmax P(I|O)）<ul><li>预测问题：<span class="math inline">\(P(i_{t+1}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，预测下一时刻t+1的隐状态</li><li><strong>滤波问题</strong>：<span class="math inline">\(P(i_{t}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，求解当前时刻t的隐状态</li></ul></li></ul><p><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=82" target="_blank" rel="noopener">白板系列</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hidden Markov Model = Markov Random Field + Time&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GMM高斯混合模型（样本之
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>PGM_Inference</title>
    <link href="http://yoursite.com/2020/10/06/PGM-Inference/"/>
    <id>http://yoursite.com/2020/10/06/PGM-Inference/</id>
    <published>2020-10-06T07:09:49.000Z</published>
    <updated>2020-10-06T15:52:39.989Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Inference 推断 → 求概率<ul><li>P(X) = P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>)</li><li>边缘概率：$P(x_{i}) = Σ_{1}...Σ_{i-1} Σ_{i+1}...Σ_{p} P(X) $（除了i的都积分）</li><li>条件概率：<span class="math inline">\(P(x_{a}|x_{b}),(x=x_{a}\bigcup x_{b})\)</span></li><li>最大后验 MAP Inference：<span class="math inline">\(\hat{Z} = argmaxP(Z|X) \propto argmax P(Z,X)\)</span>（贝叶斯定理）（不一定直接求后验，只要得到最大即<span class="math inline">\(\hat{Z}\)</span>可）</li></ul></li><li>方法<ul><li>精确推断<ul><li>Variable Elimination (VE) 变量消除法</li><li><strong>Belief Propagation</strong> (BP) 信念传播（与反向传播不一样）（弥补VE缺点）<ul><li>另外一个名称：Sum-Product Algorithm</li><li>针对树结构</li></ul></li><li>Junction Tree Algorithm<ul><li>基于BP，由树结构扩展到普通图结构</li></ul></li></ul></li><li>近似推断<ul><li>Loop Belief Propagation<ul><li>基于BP，针对有环图</li></ul></li><li>Mente Carlo Inference<ul><li>Importance Sampling</li><li>MCMC (Markov Chain Mente Carlo)</li><li>基于采样</li></ul></li><li>Variational Inference<ul><li>确定性近似</li></ul></li></ul></li></ul></li></ul><h5 id="variable-elimination">Variable Elimination</h5><p><img src="https://github.com/soloistben/images/raw/master/PGM_Inference/VE1.png" alt="VE1" style="zoom:75%;"></p><ul><li>假设a,b,c,d均为二值的随机变量{0,1}</li><li><span class="math inline">\(P(d) = Σ_{a,b,c} P(a,b,c,d) = Σ_{a,b,c} P(a)P(b|a)P(c|b)P(d|c)\)</span> （将各个条件概率看成因子）<ul><li>→ P(a=0)P(b=0|a=0)P(c=0|b=0)P(d|c=0) + P(a=1)P(b=0|a=1)P(c=0|b=0)P(d|c=0) +...+ P(a=1)P(b=1|a=1)P(c=1|b=1)P(d|c=1) = 8*因子积</li><li>由于马尔可夫的性质，结点只与相邻的相关，与其他无关，可以优先将有关的先合并。</li><li>→ <span class="math inline">\(Σ_{b,c} P(c|b)P(d|c) Σ_{a} P(a)P(b|a)\)</span>（<span class="math inline">\(Σ_{a} P(a)P(b|a) = Σ_{a} P(a,b) = P(b)\)</span>， <strong>将P(a)看成一个函数Φ(a)，P(b|a)看成Φ(a,b)，则Σ<sub>a</sub> P(a)P(b|a)看成Φ<sub>a</sub>(b)</strong>）</li><li>→ <span class="math inline">\(Σ_{c} P(d|c) Σ_{b}P(c|b)Φ_{a}(b) = Σ_{c} P(d|c) Φ_{b}(c) = Φ_{c}(d)\)</span></li></ul></li><li>若是无向图，则为 <span class="math inline">\(P(a,b,c,d) = \frac{1}{Z}\prod Φ(x)\)</span></li><li>主要思想：乘法分配律（ab+ac=a(a+c)）</li><li>缺点：1、重复计算（求另外一个点时，则需要重新求（则没有存储中间结果，若是链很长，则计算量很大））；2、消去次序（一般相关最少的先消去，但在无向图找到最优消去次序是NP-Hard问题）</li></ul><h5 id="belief-propagation">Belief Propagation</h5><ul><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Inference 推断 → 求概率
&lt;ul&gt;
&lt;li&gt;P(X) = P(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, ..., x&lt;sub&gt;p&lt;/sub&gt;)&lt;/li&gt;
&lt;li&gt;边缘概率：$P(x_{i}) = Σ_{1}...Σ_{i-1} Σ_
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Exponential_Family_Distribution</title>
    <link href="http://yoursite.com/2020/10/05/Exponential-Family-Distribution/"/>
    <id>http://yoursite.com/2020/10/05/Exponential-Family-Distribution/</id>
    <published>2020-10-05T08:43:34.000Z</published>
    <updated>2020-10-07T13:13:35.336Z</updated>
    
    <content type="html"><![CDATA[<h4 id="指数族分布">指数族分布</h4><ul><li>Gaussian Distribution, Bernoulli Distribution (Categorical Distribution), Binomial Distribution (Multinomial Distribution), Poisson Distribution, Beta Distribution, Dirichlet Distribution, Gamma Distribution</li><li><font color="red"><span class="math inline">\(P(x|η) = h(x) e^{η^{T} Φ(x) - A(η)}\)</span></font> （分为三部分）<ul><li>η属于参数、p维向量；A(η): log partition function（配分函数）；h(x)与η无关，往往设置为1</li><li>partition function（源自于统计物理学）：<span class="math inline">\(P(x|θ) = \frac {1}{Z} \hat{P}(x|θ)\)</span> （Z为归一化因子，<span class="math inline">\(Z=\int \hat{P}(x|θ) d_{x}\)</span>）</li><li><span class="math inline">\(P(x|η) = \frac {1}{e^{A(η)}} h(x) e^{η^{T} Φ(x)} → e^{A(η)} = Z → A(η) = log Z\)</span>，所以A(η)是log partition function</li></ul></li><li>特点<ul><li><strong>充分统计量 sufficient statistics</strong>：Φ(x)<ul><li>统计量：对样本的加工，关于样本的一个函数，均值、方差等</li><li>充分：统计量就可以完整表达样本特征信息了</li><li>Online Learning （可以仅存储样本统计量信息，就不需要存储大量样本，起到压缩数据的效果）</li></ul></li><li><strong>共轭</strong><ul><li><span class="math inline">\(P(Y|X) = \frac{P(X|Y)P(Y)}{\int P(X|Y)P(Y) d_{Y}}\)</span>（后验概率是求不出来，或者积分难问题，太复杂，难以求解）</li><li>计算f(Y)后验分布的期望：近似推断（变分推断、MCMC）</li><li>若似然P(X|Y)与先验P(Y)共轭（如二项分布和Beta分布），则后验与先验同分布，则仅需算后验分布的参数即可，就可以不用计算积分</li></ul></li><li><strong>最大熵</strong>（无信息先验）<ul><li>没有先验，则认为是所有样本等概率，但无法定量分析，则引入最大熵</li><li>赋予先验：共轭（为了计算方便）；最大熵（无信息先验）</li></ul></li></ul></li><li>模型和推断<ul><li><strong>广义线性模型</strong><ul><li>目标：解决分类、回归问题</li><li>线性组合（w<sup>T</sup>x）</li><li>link function（激活函数的反函数）</li><li>指数族分布（y|x ~ 指数族分布）（如线性回归：y|x ~ N(μ,σ<sup>2</sup>)；线性分类：y|x ~ 0/1分布（Bernoulli））</li></ul></li><li><strong>概率图模型</strong><ul><li>无向图：RBM 波尔兹曼机</li></ul></li><li><strong>变分推断</strong><ul><li>指数族分布可以简化变分推断</li></ul></li></ul></li><li>Gaussian Distribution<ul><li><span class="math inline">\(P(x|θ) = \frac{1}{\sqrt{2\pi}σ} e^{\frac{(x-μ)^2}{-2σ^2}},θ=(μ,σ^2)\)</span> 一维高斯分布<ul><li>η = η(θ)，A(η) = A(η(θ))，将θ映射成η​</li><li><span class="math inline">\(P(x|\theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2\mu x+\mu ^2)} = e^{-\frac{1}{2}log^{2\pi \sigma^2}} e^{-\frac{1}{2\sigma^2} {\begin{bmatrix} -2\mu \ 1 \end{bmatrix}}{\begin{bmatrix}x \\ x^2 \end{bmatrix}} -\frac{\mu^2}{2\sigma^2}} \\ = exp({\begin{bmatrix} \frac{\mu}{\sigma^2} \ \frac{-1}{2\sigma^2} \end{bmatrix}}{\begin{bmatrix}x \\ x^2 \end{bmatrix}} - (\frac{\mu^2}{2\sigma^2} + \frac{1}{2} log^{2\pi \sigma^2}))\)</span><ul><li><span class="math inline">\(η^T = {\begin{bmatrix}\frac{μ}{σ^2}\ \frac{-1}{2σ^2} \end{bmatrix}}, Φ(x) = {\begin{bmatrix}x \\ x^2 \end{bmatrix}}, A(η)=\frac{μ^2}{2σ^2}+ \frac{1}{2} log^{2\piσ^2}\)</span></li><li>设定<span class="math inline">\(η={\begin{bmatrix}η \\ η^2 \end{bmatrix}}\)</span>，则<span class="math inline">\(A(η) = -\frac{η_1^2}{4η_2}+\frac{1}{2}log^{-\frac{\pi}{η_2}}\)</span></li></ul></li></ul></li></ul></li><li><strong>充分统计量Φ(x) 与 对数配分函数A(η)</strong><ul><li>P(x|η) 概率积分为1</li><li><span class="math inline">\(P(x|η) = \frac{1}{e^{A(η)}}h(x)e^{η^T Φ(x)}→e^{A(η)}=\int h(x)e^{η^TΦ(x)}d_x\)</span></li><li>两边对η求导：<span class="math inline">\(e^{A(η)}*A&#39;(η) = \frac{d(\int h(x)e^{η^TΦ(x)}d_{x})}{dη}=\int h(x) Φ(x)e^{η^TΦ(x)}d_{x}\)</span></li><li><span class="math inline">\(A&#39;(η) =\int h(x) Φ(x)e^{η^T Φ(x)-A(η)}d_{x}=\int Φ(x)P(x|η)d_{x} = E_{P(x|η)}[Φ(x)]\)</span></li><li><span class="math inline">\(A&#39;&#39;(η)=Var_{P(x|η)} [Φ(x)]\)</span>，则A(η)是一个凸函数</li></ul></li><li><strong>充分统计量Φ(x) 与 极大似然估计</strong><ul><li>Data: D = {x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>} N个样本</li><li><span class="math inline">\(η_{MLE}=argmax \ log^{P(D|η)}=argmax \ log^{\prod P(x_i|η)}=argmax \sum log^{P(x_i|η)} \\ = argmax \sum log^{h(x_i)e^{η^T Φ(x_i)-A(η)}} = argmax \sum [log^{h(x_i)}+η^T Φ(x_i)-A(η)]\)</span><ul><li><span class="math inline">\(→ η_{MLE} ∝argmax \ \sum [η^T Φ(x_i)-A(η)]\)</span></li><li><span class="math inline">\(\frac{d(Σ [η^T Φ(x_i)-A(η)])}{dη} =\sum \frac{d(η^T Φ(x_i)-A(η))}{dη}=\sum[Φ(x_i)-A&#39;(η)] =\sum Φ(x_i)- N*A&#39;(η)=0 \\ → A&#39;(η_{MLE})=\frac{1}{N}\sum Φ(x_i)\)</span></li><li><span class="math inline">\(η_{MLE} = A&#39;^{(-1)}(η_{MLE})\)</span>（反函数） → 则η<sub>MLE</sub>是可求解的，只需要记录即<span class="math inline">\(\frac{1}{N}\sum Φ(x_i)\)</span>可，不需要记录所有样本</li></ul></li></ul></li><li><strong>最大熵角度</strong><ul><li>若一个事件发生概率为p，其信息量为-log p （若p=1,信息量就为0；一个确定的事件没有信息量）</li><li>熵：<span class="math inline">\(E[-log^p]=\int -p(x)log^{p(x)} d_{x}=-\sum p(x)log^{p(x)}\)</span> （对信息的衡量，对信息可能性的衡量）（熵只与x的分布有关，与取值无关）</li><li>最大熵&lt;=&gt;等可能（利用最大熵对等可能定量分析）（最大熵，对未知的分布进行猜测，因为不知道，所以认为都是等可能的）（<strong>要想熵最大，未知的分布必须是等可能</strong>）<ul><li><span class="math inline">\(H[p]=-\sum p(x)log^{p(x)}\)</span></li><li>假设x是离散的，P(x=1) = p<sub>1</sub>，P(x=2) = p<sub>2</sub>，...，P(x=k) = p<sub>k</sub>，Σp<sub>i</sub> = 1</li><li>则 <span class="math inline">\(max \ H[P] = max[-\sum p_i log^{p_i}],s.t.\sum p_i=1\)</span></li><li><span class="math inline">\(\hat{p_i} = argmax \ H[P] = argmin \ \sum p_i log^{p_i}\)</span> （优化问题）</li><li>拉格朗日：<span class="math inline">\(L(p, λ) = \sum p_i log^{p_i} - λ(1- \sum p_i)，\frac{dL}{dp_i} = log^{p_i} + 1 - λ = 0 → p_i = e^{λ-1}\)</span> (常数)</li><li>则 p<sub>1</sub> = p<sub>2</sub> = ... p<sub>k</sub> = 1/k，p(x)是了离散型均匀分布</li></ul></li><li>最大熵原理：<strong>在满足已知事实（约束条件）（已知数据）下，什么分布是具有最大熵的分布</strong><ul><li>Data = {x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>} 通过经验分布（概率分布<span class="math inline">\(\hat{P}(X=x) = \hat{p}(x) = \frac{count(x)}{N}\)</span>）定量描述数据</li><li>需要P分布的<span class="math inline">\(E_{\hat{p}}[x], Var_{\hat{p}}[x]\)</span>，设定f(x)是任意关于x的向量函数（<span class="math inline">\(f=[f_1,f_2,...,f_Q]^T\)</span>），则<span class="math inline">\(E_{\hat{p}}[f(x)]=Δ\)</span>（即是已知事实）</li><li><span class="math inline">\(P(X)→p(x),H[p]=-\sum p(x)log^{p(x)}\)</span> $= argmin ∑ p(x)log^{p(x)}, s.t.∑p(x)=1,E_p[f(x)]=∑p(x)f(x)=E_{}[f(x)]=Δ $</li><li>拉格朗日：<span class="math inline">\(L(p, λ_0, λ) = \sum p(x)log^{p(x)} - λ_0(1- \sum p(x)) - λ^T(Δ - \sum p(x)f(x))\)</span></li><li>求导：<span class="math inline">\(\frac{dL}{d_{p(x)}} = \sum [log^{p(x)} + 1] - \sum λ_0 - \sum λ^T f(x) = Σ[log^{p(x)}+1-λ_0-λ^T f(x)] = 0 \\ \rightarrow log^{p(x)}+1-λ_0-λ^T f(x)=0 →p(x) = e^{λ^T f(x) - (1-λ_0)}\)</span></li><li>令η=λ<sup>T</sup>，Φ(x)=f(x)，A(η)=(1-λ<sub>0</sub>)</li><li>则用最大熵原理推出，p(x)是指数分布时熵最大</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;指数族分布&quot;&gt;指数族分布&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Gaussian Distribution, Bernoulli Distribution (Categorical Distribution), Binomial Distribution (Multinom
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Dimensionality_Reduction</title>
    <link href="http://yoursite.com/2020/10/02/Dimensionality-Reduction/"/>
    <id>http://yoursite.com/2020/10/02/Dimensionality-Reduction/</id>
    <published>2020-10-02T13:10:36.000Z</published>
    <updated>2020-10-07T06:08:16.415Z</updated>
    
    <content type="html"><![CDATA[<ul><li>过拟合<ul><li>解决方法：增加数据、正则化、降维</li><li>原因：<strong>维度灾难</strong><ul><li>在没有很多数据集时，只能降维</li><li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li><li><p>从几何层面看：</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p><ul><li>2维正方形面积：1，圆形：<span class="math inline">\(\pi*(0.5)^2\)</span></li><li>3维正方体体积：1，球体体积：<span class="math inline">\(\frac{4}{3}*\pi*(0.5)^3 = K*(0.5)^3\)</span></li><li>D维超立方体体积：1，超球体体积： <span class="math inline">\(K*(0.5)^D\)</span></li><li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p></li><li>D维外超球体体积：<span class="math inline">\(K*1^D = K\)</span>，环形体积：外超球体体积 - 内超球体体积 = <span class="math inline">\(K - K(1-e)^D\)</span><ul><li>V外/V内 <span class="math inline">\(=1-(1-e)^D =1\)</span>（<span class="math inline">\(0&lt;e&lt;1\)</span>，D趋向无穷大之后，<span class="math inline">\((1-e)^D\)</span>趋向于0）</li><li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li></ul></li></ul></li></ul></li></ul></li><li>Data<ul><li>N个p维样本 X（维度N×p）（设<span class="math inline">\(I\)</span>为N维全1列向量）</li><li>样本均值 <span class="math inline">\(\hat{X}=\frac{1}{N} \sum x_i = \frac{1}{N} X^T I\)</span>（维度p×1）</li><li>方差 <span class="math inline">\(S = \frac{1}{N} \sum (x_i - \hat{X})(x_i - \hat{X})^T = \frac{1}{N} X^T (I - \frac{1}{N} I I^T) (1-\frac{1}{N} I I^T)^T X = \frac{1}{N} X^T H H^T X = \frac{1}{N} X^T H X\)</span><ul><li>（维度p×p）</li><li><span class="math inline">\(H = I-\frac{1}{N} I I^T\)</span> centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li><li><span class="math inline">\(H^T = H, H^2 = H H^T = (I-\frac{1}{N}I I^T) (I-\frac{1}{N} I I^T)^T = I-\frac{1}{N} I I^T = H\)</span></li><li><span class="math inline">\(H^n = H\)</span></li></ul></li></ul></li><li>降维方法<ul><li>直接降维 （特征选择：人工选取重要特征 ）</li><li>线性降维<ul><li><strong>Principal Components Analysis PCA 主成分分析</strong><ul><li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p></li><li>最大投影方差<ul><li>找到一个u<sub>1</sub>平面，使投影间距达到最大（投影到u<sub>2</sub>平面，距离太小，没有意义）</li><li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li><li>第1步：中心化：先将所有数据平移，利于计算，即 <span class="math inline">\(x_i - \hat{X}\)</span></li><li>第2步：投影到u<sub>1</sub>平面：<span class="math inline">\((x_i - \hat{X})^T u_1\)</span>（设定 <span class="math inline">\(|u_1|=1\)</span>，即<span class="math inline">\(u_1^T u_1=1\)</span>）</li><li>第3步：投影方差$ J =  ^2 =  = u_1^T S u_1$<ul><li><span class="math inline">\(\hat{u_1} = argmax \ u_1^T S u_1,s.t. u_1^T u_1=1\)</span><ul><li>使用拉格朗日求解</li><li><span class="math inline">\(L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)\)</span></li><li><span class="math inline">\(\frac{dL}{du_1} = 2 S u_1 - 2λ u_1 = 0 → S u_1 = λ u_1\)</span></li><li><strong>u<sub>1</sub>为eigen-vector特征向量，λ为eigen-value特征值</strong></li><li>解法1：对方差矩阵特征分解，即可求解PCA</li><li>解法2：直接对原始数据进行操作：中心化后的 <span class="math inline">\(HX = UΣV^T\)</span> 进行奇异值分解（U和V均为正交矩阵），<span class="math inline">\(S = X^T H X = X^T H^T H X = (VΣU^T)(UΣV^T) = V Σ^2 V^T\)</span>（可先忽略1/N常数）（维度p×p），因此直接求解HX奇异值分解，也就是求解了S的特征分解</li><li>假设 <span class="math inline">\(B = H X X^T H = (UΣV^T)(VΣU^T) = U Σ^2 U^T\)</span>（维度N×N），则B与S有一样的eigenvalue，（S特征分解得到方向V（主成分）然后通过<span class="math inline">\(HX V\)</span>得到新坐标）（B特征分解直接得到坐标U，称为主坐标分析 <strong>principal coordinate analysis PCoA</strong>）</li><li><span class="math inline">\(HX V = UΣV^T V = UΣ\)</span>（UΣ是坐标矩阵），<span class="math inline">\(BUΣ = U Σ^2 U^T UΣ = UΣ Σ^2\)</span>（UΣ是特征向量组成的矩阵）</li></ul></li></ul></li></ul></li><li>最小重构代价<ul><li>投影在u<sub>1</sub>平面的点恢复到原来样子的代价</li><li>设从原p维降到q维（下面u代表特征）<ul><li><span class="math inline">\(x_i = \sum_k^p (x_i^T u_k) u_k, \hat{x_i} = \sum_k^q (x_i^T u_k) u_k\)</span> (用特征<span class="math inline">\(u_k\)</span>描述样本<span class="math inline">\(x_i\)</span>，<span class="math inline">\(x_i^T u_k\)</span>为距离大小，<span class="math inline">\(u_k\)</span>为单位大小，则第k维描述为<span class="math inline">\((x_i^T u_k) u_k\)</span>)</li><li>代价函数 <span class="math inline">\(L =\frac{1}{N} \sum||x_i - \hat{x_i}||^2 = \sum_{k=q+1}^p u_k^T S u_k = \sum_{k=q+1}^p λ_k, s.t. u_k^T u_k=1\)</span></li><li><span class="math inline">\(u_k = argmin \ L\)</span></li></ul></li></ul></li><li>概率角度<ul><li>P-PCA<ul><li>设定observed data X为p维（特征为连续型数据），latent variable Z为q维（q&lt;p）</li><li>设定<span class="math inline">\(Z\)</span>~<span class="math inline">\(N(0, I)\)</span>（服从高斯分布，q维）</li><li><span class="math inline">\(X = WZ + u + ε\)</span>​（X是Z的一个线性变换加噪声）</li><li><span class="math inline">\(X|Z\)</span>~<span class="math inline">\(N(WZ + u, σ^2 I)，X~N(u, WW^T+σ^2 I)\)</span></li><li>噪声<span class="math inline">\(ε\)</span>~<span class="math inline">\(N(0, σ^2 I)\)</span>（p维）</li><li>这也是一种Linear Gaussian Model（称该模型各向同性，对各方向影响是一样的）</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>求P(Z), P(X|Z), P(X), 最后求后验P(Z|X)、用EM求参数W, u, σ</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR4.png" alt="DR4" style="zoom:75%;"></p><pre><code>  + 从服从高斯分布的Z中，投影点在方向W，进行线性变换得到X，也得到服从高斯分布的X|Z，方向W上有很多的高斯分布（各向同性）；X的分布不在方向W上，且中间很宽（因为高斯分布中间高两边低）[详情](https://www.bilibili.com/video/BV1aE411o7qd?p=27)</code></pre><ul><li>MDS</li><li>非线性降维<ul><li>流型</li><li>Isomap</li><li>LLE</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;过拟合
&lt;ul&gt;
&lt;li&gt;解决方法：增加数据、正则化、降维&lt;/li&gt;
&lt;li&gt;原因：&lt;strong&gt;维度灾难&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;在没有很多数据集时，只能降维&lt;/li&gt;
&lt;li&gt;每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>SVM</title>
    <link href="http://yoursite.com/2020/10/02/SVM/"/>
    <id>http://yoursite.com/2020/10/02/SVM/</id>
    <published>2020-10-02T08:50:04.000Z</published>
    <updated>2020-10-09T10:27:18.933Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Data : N个p维样本 X（维度N×p），y = {-1,1}</li><li><p>SVM 三宝：间隔，对偶，核技巧</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p></li><li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</li><li><p><strong>hard-margin SVM</strong>（硬间隔）</p><ul><li>最大间隔分类器 <span class="math inline">\(= max \ margin(w, b),s.t. y_i(w^T x_i+b) &gt; 0,for i = 1,...,N\)</span></li><li>点到直线距离，垂直线最短</li><li><span class="math inline">\(margin(w, b) = min \ distance(w, b, x_i) = min \ \frac{1}{||w||} |w^T x_i + b|\)</span></li><li><span class="math inline">\(→ max_{w,b} \ min_x \frac{1}{||w||} |w^T x_i + b|,(s.t. y_i(w^T x_i+b) &gt; 0)=max_{w,b} \frac{1}{||w||} min_x y_i(w^T x_i + b)\)</span> (存在<span class="math inline">\(r&gt;0\)</span>，使<span class="math inline">\(y_i(w^T x_i + b) =r\)</span>，可以设置<span class="math inline">\(r=1\)</span>)</li><li><span class="math inline">\(→ max \ \frac{1}{||w||} ,s.t. min \ y_i(w^T x_i + b) = 1 → min \ \frac{1}{2} w^T w , s.t. y_i(w^T x_i + b)\geqslant 1\)</span>(convex optimization 二次凸优化问题)(primal problem原问题)</li><li>拉格朗日：<span class="math inline">\(L(w, b, λ) = \frac{1}{2} w^T w + \sum λ_i(1-y_i(w^T x_i + b)) (λ_i \geqslant 0, (1-y_i(w^T x_i + b)) \leqslant 0\)</span>；若<span class="math inline">\((1-y_i(w^T x_i + b))&gt;0\)</span>，L为正无穷，无解；仅在 <span class="math inline">\(λ_i=0， (1-y_i(w^T x_i + b)) =0\)</span>，达到最大L)</li><li><strong>primal problem 原问题</strong>&lt;=&gt; <span class="math inline">\(min_{w,b} \ max_λ \ L(w, b, λ),s.t. λ_i \geqslant 0\)</span> （对w，b没有限制）<span class="math inline">\(→ min \ \frac{1}{2} w^T w\)</span></li><li><p><strong>dual problem 对偶问题</strong>：<span class="math inline">\(max_λ \ min_{w,b} \ L(w, b, λ),s.t. λ_i \geqslant 0\)</span></p><ul><li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li><li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li><li><span class="math inline">\(\frac{dL}{db} = \frac{d[\sum λ_i(1-y_i(w^T x_i + b))]}{db} = \frac{d[- Σ λ_i y_i b]}{db} = -\sum λ_i y_i =0\)</span>，带入原式<span class="math inline">\(L = \frac{1}{2} w^T w + \sum λ_i - \sum λ_i y_i w^T x_i\)</span></li><li><span class="math inline">\(\frac{dL}{dw} = w - \sum λ_i y_i x_i = 0 → w = \sum λ_i y_i x_i\)</span>，带入原式<span class="math inline">\(L=\frac{1}{2} w^T w + \sum λ_i - w^T w = \sum λ_i - \frac{1}{2}w^T w\)</span></li><li><font color="red"><span class="math inline">\(→ min \ \frac{1}{2} w^T w - \sum λ_i,s.t. λ_i\geqslant0, \sum λ_i y_i =0\)</span></font> （此处不能求λ偏导）</li><li><span class="math inline">\(→ min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i,s.t. λ_i\geqslant 0, \sum λ_i y_i =0\)</span></li><li><strong>KKT条件</strong>：参数偏导为0（<span class="math inline">\(\frac{dL}{db}=0，\frac{dL}{dw}=0，\frac{dL}{dλ}=0\)</span>），<span class="math inline">\(λ_i(1-y_i(w^T x_i + b))=0， λ_i\geqslant0，(1-y_i(w^T x_i + b)\leqslant0\)</span></li><li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li><li><span class="math inline">\(\hat{w} = \sum λ_i y_i x_i\)</span>, （存在<span class="math inline">\(x_k,y_k,1-y_k(w^T x_k + b)=0\)</span>）<span class="math inline">\(\hat{b} = y_k - w^T x_k = y_k - (\sum λ_i y_i x_i^T) x_k\)</span></li></ul></li><li><p><span class="math inline">\(f(x) = sign(\hat{w}^T x + \hat{b})\)</span></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p><ul><li>落在虚线的样本点就是<span class="math inline">\(x_k（y_k(w^T x_k + b)=1\)</span>），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li></ul></li></ul></li><li><strong>soft-margin SVM</strong>（软间隔）<ul><li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li><li>soft-margin SVM在hard-margin SVM基础上允许一点点错误，$min   w^T w + loss $<ul><li>分错点的个数：<span class="math inline">\(loss = \sum I\{y_i(w^T x_i + b)&lt;1\}\)</span> （关于w是不连续的，无法求导，因此不采取）</li><li><p>hinge 距离：<span class="math inline">\(hinge \ loss = max \{0, 1-y_i(w^T x_i + b)\}\)</span></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p></li><li><span class="math inline">\(min \ \frac{1}{2} w^T w + C \sum max \{0, 1-y_i(w^T x_i + b)\},s.t. y_i(w^T x_i + b)\geqslant 1\)</span> （超参数C）</li><li><p>→ 设定<span class="math inline">\(ξ_i = y_i(w^T x_i + b)，min \ \frac{1}{2} w^T w + C \sum ξ_i,s.t. y_i(w^T x_i + b)\geqslant (1-ξ_i), ξ_i\geqslant 0\)</span>（同样用对偶问题方式来求解）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p></li></ul></li></ul></li><li>约束优化问题<ul><li>primal problem 原问题：<span class="math inline">\(min \ f(x),s.t. m_i(x) \leqslant 0, n_j(x)=0 (i=1,...,M, j=1,...,N)\)</span></li><li>原问题的无约束形式（关于x的函数）：拉格朗日：<font color="red"><span class="math inline">\(L(x, λ, η) = f(x) + \sum λ_i m_i(x) + \sum η_i n_i(x) → min_x \ max_{λ,η} \ L(x, λ, η),s.t. λ_i\geqslant 0\)</span></font></li><li>证明两者等价：如果违法约束<span class="math inline">\(m_i(x)&gt;0，max_λ \ L → ∞\)</span>；反之，<span class="math inline">\(max_λ \ L\)</span> 必有最大值（<span class="math inline">\(λ_i=0\)</span>时）（即排除了<span class="math inline">\(m_i(x)&gt;0\)</span>情况，过滤掉违反约束的情况）</li><li>dual problem 对偶问题（关于λ,η的函数）：<font color="red"><span class="math inline">\(max_{λ,η} \ min_x \ L(x, λ, η),s.t. λ_i\geqslant 0\)</span></font><ul><li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （<span class="math inline">\(max_{λ,η} \ min_x \ L(x, λ, η) \leqslant min_x \ max_{λ,η} \ L(x, λ, η)\)</span>）</li><li>证明：<span class="math inline">\(min_x \ L \leqslant L \leqslant max_{λ,η} \ L → A(λ,η) \leqslant L \leqslant B(x) → A(λ,η) \leqslant B(x) → max \ A(λ,η) \leqslant min \ B(x)\)</span></li><li><span class="math inline">\(→ max_{λ,η} \ min_x \ L(x, λ, η) \leqslant min_x \ max_{λ,η} \ L(x, λ, η)\)</span></li><li>（在<span class="math inline">\(min_x \ L\)</span>已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li><li>强对偶性：对偶问题=原问题</li></ul></li><li>几何解释<ul><li>primal problem: <span class="math inline">\(min \ f(x),s.t. m_1(x)\leqslant 0\)</span> (D定义域，D=dom<sub>f</sub> ∩ dom<sub>m1</sub>)， <font color="red">原问题最优解：<span class="math inline">\(p^* = min f(x)\)</span></font></li><li><span class="math inline">\(L(x, λ) = f(x) + λ m_1(x),s.t. λ\geqslant 0\)</span> <font color="red">对偶最优解：<span class="math inline">\(d^* = max_λ \ min_x \ L(x, λ)\)</span></font></li><li>将问题投影入二维空间：引入集合(区域) <span class="math inline">\(G = \{(m_1(x), f(x))|x∈D\}\)</span><ul><li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li><li>凸集是指集合内任意两点的连线都在集合内</li><li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li><li>目标函数f如果不是凸函数，则不是凸优化问题</li><li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li><li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li><li>结论：凸函数的局部最优解就是全局最优解</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p><ul><li><p><span class="math inline">\(p^* = inf \{f(x)|(m_1(x), f(x))∈G, m_1(x) \leqslant 0\}\)</span>（集合中没有最小值概念，对应的是下确界）</p><ul><li><span class="math inline">\(P*\)</span> 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li></ul></li><li><span class="math inline">\(d^* = max_λ \ g(λ) , g(λ) = min_x \  f(x) + λ m_1(x) , g(λ) = inf \{f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G\}\)</span><ul><li>一条过原点直线 <span class="math inline">\(f(x)+λm_1(x)= 0\)</span>(斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）<span class="math inline">\(g(λ)\leqslant p^*\)</span></li><li>当调整斜率<span class="math inline">\(λ^*\)</span>，得到一个 <span class="math inline">\(g(λ^*) = f(x) + λ^* m_1(x)\)</span> 同时与G的俩角相切，此时，直线与纵轴的交点为<span class="math inline">\(d^*\)</span>（绿线）</li><li><p><span class="math inline">\(d^* \leqslant p^*\)</span> （凸优化+slater条件 → <span class="math inline">\(d^* = p^*\)</span>）（SVM是二次规划问题，符合slater条件）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p></li></ul></li></ul></li></ul></li><li>slater条件<ul><li>Convex + Slater → Strong Duality （充分不必要条件）</li><li>定义：存在<span class="math inline">\(\hat{x}\)</span>在relint，使<span class="math inline">\(m_i(x)&lt;0 (i=1,...,M)\)</span><ul><li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li><li>仿射函数即由由1阶多项式构成的函数，一般形式为 <span class="math inline">\(f (x) = Ax + b\)</span>（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li></ul></li><li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li><li>放松的slater条件：在<span class="math inline">\(m_i(x)\)</span>中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足<span class="math inline">\(m_i(x)&lt;0\)</span> （凸二次规划问题：目标函数f是凸的，不等式约束<span class="math inline">\(m_i\)</span>是仿射函数，等式约束<span class="math inline">\(n_j\)</span>也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li></ul></li><li>KKT条件<ul><li>KKT &lt;=&gt; Strong Duality (<span class="math inline">\(d^* = p^*\)</span>)（充要条件）</li><li>从<span class="math inline">\(p^*\)</span>得到最优<span class="math inline">\(x^*\)</span>，从<span class="math inline">\(d^*\)</span>得到<span class="math inline">\(λ^*\)</span>、<span class="math inline">\(η^*\)</span></li><li>可行域（可行条件）：<span class="math inline">\(m_i(x^*) \leqslant 0, n_j(x^*)=0, λ^*\geqslant 0\)</span></li><li>互补松弛<ul><li><span class="math inline">\(d^* = max_{λ,η} \ g(λ,η) = g(λ^*, η^*) = min_x \ L(x, λ^*, η^*) \leqslant L(x^*, λ^*, η^*) \\ = f(x^*) + \sum λ_im_i(x^*) + \sum η_in_i(x^*) = f(x^*) + \sum λ_im_i(x^*) \leqslant f(x^*) = p^*\)</span></li><li>（<span class="math inline">\(λ_i\geqslant 0，m_i \leqslant 0，则 λ_i m_i \leqslant 0\)</span>）</li><li><font color="red">互补松弛条件 ：$λ_i m_i(x^<em>) = 0 → λ_i<sup>* m_i(x</sup></em>) $</font></li></ul></li><li>梯度为0<ul><li><span class="math inline">\(min_x \ L(x, λ^*, η^*) &lt;= L(x^*, λ^*, η^*)\)</span></li><li><span class="math inline">\(x^*\)</span>是对应x最小值，则 <span class="math inline">\(\frac{dL}{dx} = 0\)</span></li></ul></li></ul></li></ul></li><li><strong>kernel SVM</strong><ul><li>Kernel Method（思想角度）</li><li>Kernel Trick（计算角度）</li><li><p>Kernel function</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png" alt="SVM9"></p><ul><li><strong>非线性带来高维转换（从模型角度）</strong><ul><li>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</li><li>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p><ul><li>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</li><li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）<br></li><li><font color="red">非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</font></li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p><ul><li>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</li><li>三种方法转高维：1、类似MLP直接转高维；2、Kernel方法转高维；3、深度学习运用与或非构建有向无环图（神经网络）（与或非（三种基础运算均可用PLA表示）解决异或问题（复合运算）），神经网络：复合表达式、复合函数、MLP（FeedForward Neural Network）<ul><li><p>XOR：x<sub>1</sub>⊕x<sub>2</sub> = (¬x<sub>1</sub>∧x<sub>2</sub>)∨(x<sub>1</sub>∧¬x<sub>2</sub>)</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png" alt="FNN1" style="zoom: 67%;"></p></li></ul></li></ul></li></ul></li><li><strong>对偶表示带来内积（从优化角度）</strong><ul><li>从频率视角归化到优化问题</li><li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题<ul><li>Doul Problem: <span class="math inline">\(min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i s.t. λ_i \geqslant 0, \sum_i λ_i y_i =0\)</span></li><li>内积：<span class="math inline">\(x_i^T x_j\)</span></li><li>非线性转换：<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span> （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此很<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span>难i求解和计算量很大）</li><li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span> 结果</strong>（不需要先求Φ(x)再求内积）</li></ul></li><li><strong>Kernel function : <span class="math inline">\(K(x, x&#39;) = Φ(x)^T Φ(x&#39;) = \ &lt;Φ(x), Φ(x&#39;)&gt;\)</span></strong><ul><li>存在<span class="math inline">\(x, x&#39;∈X\)</span>，使<span class="math inline">\(K(x, x&#39;) = Φ(x)^T Φ(x&#39;)\)</span>，则K就是一个核函数（如<span class="math inline">\(K(x, x&#39;)=e^{\frac{-(x-x&#39;)^2}{2σ^2}}\)</span>）</li><li>蕴含了：非线性转换+内积</li></ul></li></ul></li><li>一般核函数指<strong>正定核函数</strong> <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a><ul><li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x')为核函数</li><li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x')，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使<span class="math inline">\(K(x, x&#39;) = \ &lt;Φ(x), Φ(x&#39;)&gt;\)</span>，则K(x, x')为正定核函数</li><li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x')，若满足两个条件（对称性、正定性）则为正定和函数<ul><li>对称性：<span class="math inline">\(K(x, x&#39;) = K(x&#39;, x)\)</span></li><li>正定性：任取N个元素，x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>∈X，对应的Gram矩阵是半正定的（K=[K(x<sub>i</sub>, x<sub>j</sub>)]）（两个定义等价，即证明：<strong>K(x, x') = &lt;Φ(x), Φ(x')&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li><li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（&lt;f, g&gt; = &lt;g, f&gt;）、正定性（内积大于等于0，&lt;f, f&gt; &gt;= 0）、线性性（&lt;r<sub>1</sub> f<sub>1</sub> + r<sub>2</sub> f<sub>2</sub> , g&gt; = r<sub>1</sub> &lt;f<sub>1</sub>, g&gt; + r<sub>2</sub> &lt;f<sub>2</sub>, g&gt;））<br></li></ul></li><li>必要性证明<ul><li>在Hilbert Space中的Φ(x)具有对称性性质，<span class="math inline">\(K(x, x&#39;) = &lt;Φ(x), Φ(x&#39;)&gt; = &lt;Φ(x&#39;), Φ(x)&gt; = K(x&#39;, x)\)</span></li><li>K=[K(x<sub>i</sub>, x<sub>j</sub>)]（维度N×N）（半正定：任意a列向量，<span class="math inline">\(a^T K a \geqslant 0\)</span>）</li><li><span class="math inline">\(a^T K a = \sum_i \sum_j a_i a_j K_{ij} = \sum_i \sum_j a_i a_j K(x_i, x_j) = \sum_i \sum_j a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; → 线性性\\ → \sum_i \sum_j a_i a_j Φ(x_i)^T Φ(x_j) = \sum_i a_i Φ(x_i)^T \sum_j a_j Φ(x_j) = [\sum_i a_i Φ(x_i)]^T \sum_j a_j Φ(x_j) \\ = \ &lt;\sum_i a_i Φ(x_i), \sum_j a_j Φ(x_j)&gt; \ = ||\sum_i a_i Φ(x_i)||^2 &gt;= 0，半正定性\)</span></li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Data : N个p维样本 X（维度N×p），y = {-1,1}&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SVM 三宝：间隔，对偶，核技巧&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/soloistben/images/raw/master/sta
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Decision_Tree</title>
    <link href="http://yoursite.com/2020/10/02/Decision-Tree/"/>
    <id>http://yoursite.com/2020/10/02/Decision-Tree/</id>
    <published>2020-10-02T08:49:49.000Z</published>
    <updated>2020-10-07T06:14:10.687Z</updated>
    
    <content type="html"><![CDATA[<ul><li>基于数据特征构造决策树<ul><li>有向边</li><li>结点<ul><li>内部结点(internal node)-&gt;表示特征</li><li><p>叶子结点(leaf node)-&gt;表示类别</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p></li><li>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</li></ul></li><li>决策树表示给定特征条件下类的条件概率分布。<ul><li>一个条概率分布定义特征空间的一个划分上</li><li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li></ul></li><li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li></ul></li><li>决策树学习过程：特征选择、决策树生成、剪枝<ul><li><strong>ID3算法</strong>（分类、多叉树）<ul><li>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）<ul><li>Information Gain 信息增益。信息增益越大对应的特征越重要</li><li>Entropy 熵，表示随机变量不确定的度量</li><li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p><figure><img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"><figcaption>DT2</figcaption></figure></li></ul></li><li>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</li><li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li></ul></li><li><strong>C4.5算法</strong>（分类、多叉树）<ul><li>特征选择<ul><li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p></li></ul></li><li>生成：与ID3算法类似</li><li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li><li>二叉树模型会比多叉树运算效率高</li><li>无剪枝</li></ul></li><li><strong>CART</strong> classification and regression tree（分类、回归、二叉树）<ul><li>分类<ul><li>特征选择<ul><li>Gini基尼指数</li><li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li><li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li></ul><figure><img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"><figcaption>DT4</figcaption></figure></li><li>生成<ul><li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li><li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li></ul></li><li><p>损失函数</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p><ul><li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p></li></ul></li><li>回归</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;基于数据特征构造决策树
&lt;ul&gt;
&lt;li&gt;有向边&lt;/li&gt;
&lt;li&gt;结点
&lt;ul&gt;
&lt;li&gt;内部结点(internal node)-&amp;gt;表示特征&lt;/li&gt;
&lt;li&gt;&lt;p&gt;叶子结点(leaf node)-&amp;gt;表示类别&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;ht
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Statistics</title>
    <link href="http://yoursite.com/2020/09/11/Statistics/"/>
    <id>http://yoursite.com/2020/09/11/Statistics/</id>
    <published>2020-09-11T08:34:56.000Z</published>
    <updated>2020-10-07T08:54:38.465Z</updated>
    
    <content type="html"><![CDATA[<h4 id="statistics-for-machine-learning">Statistics for Machine Learning</h4><h5 id="one.-两大派系">One. 两大派系</h5><ul><li><strong>频率派：统计机器学习</strong>（model (<span class="math inline">\(f(x)=w^T x+b\)</span>), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）<ul><li>正则化（L1,L2）</li><li>核化（Kernel SVM）</li><li>集成化（AdaBoost，RandForest）</li><li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li></ul></li><li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））<ul><li>有向图：Bayesian Network (Deep Directed Network)<ul><li>Sigmoid Belief Network</li><li>Variational Autoencoder (VAE)</li><li>GAN</li></ul></li><li>无向图：Markov Network (Deep Boltzmann Network )</li><li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li><li>统称 Deep Generative Model（但深层很难计算）</li><li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li></ul></li><li>Data<ul><li>X: data, X={x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>}<sup>T</sup> Dimension(N, P)</li><li>θ: parameter, X~p(X|θ)</li></ul></li><li><strong>频率派</strong><ul><li>θ为未知常数；X为随机变量</li><li><span class="math inline">\(loss = P(X|θ) = \prod_{i=1}^n P(x_i|θ)\)</span><ul><li>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>之间独立同分布</li></ul></li><li>Maximum Likelihood Estimation 极大似然估计<ul><li><span class="math inline">\(θ_{MLE}= argmax_θ \ log^{P(X|θ)}\)</span></li></ul></li></ul></li><li><strong>贝叶斯派</strong><ul><li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li><li>posterior probability 后验概率<ul><li><span class="math inline">\(P(θ|X) = \frac{P(X, θ)}{P(X)} = \frac{P(X|θ)P(θ)}{P(X)} = \frac{likelihood*prior}{\int_θ P(X|θ) d_θ}\)</span></li></ul></li><li>Maximum A Posterior Probability 最大后验概率<ul><li>找到θ在分布中最大值点</li><li><span class="math inline">\(θ_{MLE} = argmax_θ \ P(X|θ) = argmax_θ \ P(X|θ)P(θ)\)</span></li><li>P(θ|X)其分母是不变的，则θ<sub>MLE</sub>与分子成正比，只需要算分子</li></ul></li><li>贝叶斯估计<ul><li><span class="math inline">\(P(θ|X) = \frac{P(X|θ)P(θ)}{\int_θ P(X|θ) d_θ}\)</span></li><li>必须完整计算整个分子式</li></ul></li><li>贝叶斯预测<ul><li><span class="math inline">\(X (train), \hat{X} (test), X → θ → \hat{X}\)</span></li><li>训练数据通过学习参数θ，与测试数据关联</li><li><span class="math inline">\(P(\hat{X}|X) = \int_θ P(\hat{X}, θ|X) d_θ = \int_θ P(\hat{X}|X)P(θ|X) d_θ\)</span></li></ul></li></ul></li></ul><h5 id="two.-linear-regression">Two. Linear Regression</h5><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p><p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线<span class="math inline">\(f(w) = w^T x + b\)</span>（偏置b可先忽略）</p><ul><li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）<ul><li>线性（属性线性、全局线性、系数线性）<ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li><li>系数非线性：神经网络（感知机）</li></ul></li><li>全局性<ul><li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li></ul></li><li>数据未加工<ul><li>预处理：PCA，流行</li></ul></li></ul></li><li><strong>矩阵表达</strong><ul><li>Least Squares 最小二乘估计法（最小平方法）<ul><li><span class="math inline">\(\mathbf{L(w) = \sum ||w^T x_i - y_i||^2} = \sum (w^T x_i - y_i)^2 \\ = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y\)</span></li></ul></li><li><span class="math inline">\(\mathbf{\hat{w} = argmin \ L(w)}\)</span></li><li>求导$  = 2 X^T X w - 2 X^T Y=0 → X^T X w = X^T Y$<ul><li><span class="math inline">\(\hat{w} = (X^T X)^{-1} X^T Y\)</span> (伪逆：<span class="math inline">\((X^T X)^{-1} X^T\)</span>)</li></ul></li><li><span class="math inline">\(x_3\)</span>的误差为<span class="math inline">\(w^T x_3 - y_3\)</span>，即所有误差分成一小段一小段</li></ul></li><li><p><strong>几何意义</strong></p><ul><li><span class="math inline">\(f(w) = w^T x \Leftrightarrow f(β) = x^T β\)</span></li><li>可以将数据X看成p维的空间，Y是不在该p维空间内</li><li>目标：在p维空间中找到一条直线<span class="math inline">\(f(β)\)</span>离Y最近，即Y在p维空间的投影<ul><li><p>若向量a与向量b垂直，则 <span class="math inline">\(a^T b = 0\)</span></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li><li>虚线为<span class="math inline">\(Y - Xβ\)</span> 与<span class="math inline">\(X\)</span>的p维空间垂直，<span class="math inline">\(X^T (Y-Xβ) = 0 → β = (X^T X)^{-1} X^T Y\)</span></li><li>误差分散在p个维度上</li></ul></li></ul></li><li><p><strong>概率角度</strong></p><ul><li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li><li>数据本身会带有噪声 ε~N(0, σ<sup>2</sup>)</li><li><span class="math inline">\(y = f(w) + ε = w^T x + ε\)</span><ul><li><span class="math inline">\(y|x,w\)</span> ~<span class="math inline">\(N(w^T x, σ^2) \Leftrightarrow P(y|x,w) =\frac{1}{\sqrt{2\pi}σ} e^{-\frac{(y - w^T x)^2}{2*σ^2}}\)</span></li></ul></li><li>定义log-likelihood：<ul><li><span class="math inline">\(L_{MLE} (w) = log^{P(y|x,w)} = log^{\prod P(y_i|x_i,w)} = \sum log^{P(y_i|x_i,w)} \\ = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} + log^{e^{-\frac{(y - w^T x)^2}{2σ^2}}}] = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} -\frac{(y - w^T x)^2}{2σ^2}]\)</span></li><li>样本之间独立同分布</li><li><span class="math inline">\(\hat{w} = argmax \ L_{MLE} (w) = argmax \ \frac{(y - w^T x)^2}{2σ^2} = argmin \ (y_i - w^T x_i)^2\)</span></li><li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li></ul></li></ul></li><li><p><strong>Regularization 正则化</strong></p><ul><li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong><ul><li>最直接是加样本数据</li><li>降维or特征选择or特征提取 (PCA)</li><li>正则化（损失函数加个约束）：<span class="math inline">\(argmin [L(w)+λP(w)]\)</span></li></ul></li><li>L1 -&gt; Lasso<ul><li><span class="math inline">\(P(w) = ||w||_1\)</span></li></ul></li><li>L2 -&gt; Ridge 岭回归<ul><li><span class="math inline">\(P(w) = ||w||_2 = w^T w\)</span></li><li>权值衰减</li><li><span class="math inline">\(J(w) = \sum||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w \\ = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y\)</span><ul><li><span class="math inline">\(\hat{w} = argmin \ J(w)\)</span></li><li><span class="math inline">\(\frac{dJ}{dw} = 2 (X^T X + λ I)w - 2 X^T Y = 0, \hat{w} = (X^T X + λ I)^{-1} X^T Y\)</span></li><li><span class="math inline">\(X^T X\)</span> 是半正定矩阵+对角矩阵=<span class="math inline">\((X^T X + λI)\)</span>正定矩阵，必然<strong>可逆</strong></li></ul></li></ul></li><li>贝叶斯的角度<ul><li>参数w服从分布，<span class="math inline">\(w\)</span>~<span class="math inline">\(N(0,σ\_0^2) → P(w) = \frac{1}{\sqrt{2\pi}σ_0} e^{-\frac{||w||^2}{2σ_0^2}}\)</span></li><li><span class="math inline">\(P(w|y) = \frac{P(y|w)P(w)}{P(y)}\)</span></li><li>MAP: <span class="math inline">\(\hat{w} = argmax \ P(w|y) = argmax \ P(y|w)P(w) = argmax \ log^{P(y|w)P(w)} \\ = argmax \ log^{\frac{1}{2\piσ_0σ} e^{(-\frac{(y_i - w^T x_i)^2}{2σ^2} -\frac{||w||^2}{2σ_0^2})}} = argmin [(y_i - w^T x_i)^2 + \frac{σ^2}{σ_0^2}||w||^2] = argmin [L(w)+λP(w)]\)</span></li><li><span class="math inline">\(λ = \frac{σ^2}{σ_0^2}\)</span></li><li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li></ul></li></ul></li></ul><h5 id="three.-linear-classification">Three. Linear Classification</h5><ul><li><font color="red">线性回归------&gt;激活函数，降维-------&gt;线性分类</font></li><li>硬分类：0/1<ul><li>线性判别分析 (Fisher)</li><li>感知机</li></ul></li><li>软分类：[0,1]区间内概率<ul><li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li><li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li></ul></li><li><p><strong>Perceptron 感知机</strong> (1958年)</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p><ul><li>判别模型</li><li>样本：{(x<sub>i</sub>, y<sub>i</sub>)}, N个</li><li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li><li>模型：<span class="math inline">\(f(x) = sign(w^T x + b)\)</span> （<span class="math inline">\(w^T x\)</span>大于等于0表示为1（分类正确），反之为-1（分类错误））</li><li>策略：loss function（被错误分类的样本个数）<ul><li><span class="math inline">\(L(w) = \sum I\{y_i * (w^T x_i) &lt; 0\}\)</span> （非连续函数，不可导）</li><li><span class="math inline">\(L(w) = \sum-y_i w^T x_i,\frac{dL}{dw} = -y_i x_i\)</span></li></ul></li><li>若是非线性可分，可是使用pocket algorithm</li><li>从感知机到深度学习（发展历程）<ul><li>1958年提出PLA</li><li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li><li>1981年提出MLP（多层感知机），FeedForward Neural Network</li><li>1986年BP+MLP，RNN</li><li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li><li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li><li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li><li>2009年GPU发展，2011年speech，2012年ImageNet</li><li>2013年Variational Autoencoder (VAE)</li><li>2014年GAN</li><li>2016年Alpha Go</li><li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li><li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li></ul></li></ul></li><li><p><strong>线性判别分析</strong></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p><ul><li>样本：N个p维样本，二分类(+1,-1)，正样本个数<span class="math inline">\(N1\)</span>，均值<span class="math inline">\(X_{c1}\)</span>，方差<span class="math inline">\(S_{c1}\)</span>，负样本个数<span class="math inline">\(N_{2}\)</span>，均值<span class="math inline">\(X_{c2}\)</span>，方差<span class="math inline">\(S_{c2}\)</span>，（<span class="math inline">\(X_{c1} = \frac{1}{N_1} \sum x_i,S_{c1} = \frac{1}{N_1} \sum (x_i - X_{c1})(x_i - X_{c1})^T\)</span>）</li><li>思想：类内小，类间大<ul><li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li><li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li><li>Z平面的法向量为最后找到的分类函数 <span class="math inline">\(w^T x\)</span>（因为垂直，则Z平面即w向量）<ul><li>（前提设置<span class="math inline">\(||w||=1\)</span>）</li><li>则样本点投影到Z平面为：<span class="math inline">\(|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i\)</span></li></ul></li></ul></li><li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong><ul><li><span class="math inline">\(N_1 = \frac{1}{N_1} \sum w^T x_i\)</span></li><li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - Z_1) (w^T x_i - Z_1)^T\)</span></li><li>类间：<span class="math inline">\((Z_1-Z_2)^2\)</span>，类内：<span class="math inline">\(S_1+S_2\)</span></li></ul></li><li>策略：<span class="math inline">\(L(w) = \frac{(Z_1-Z_2)^2}{(S_1+S_2)}= \frac{w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w}{w^T(S_{c1}+ S_{c2}) w }\)</span><ul><li>分子 =$ [w^T ( x_i -  x_i)]^2= [w^T (X_{c1} - X_{c2})]^2 = w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w$</li><li>分母 = <span class="math inline">\(w^T S_{c1} w + w^T S_{c2} w = w^T (S_{c1}+ S_{c2}) w\)</span><ul><li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - \frac{1}{N_1} \sum w^T x_j)(w^T x_i - \frac{1}{N_1} \sum w^T x_j)^T \\ = w^T [\frac{1}{N_1}\sum(x_i - X_{c1})(x_i - X_{c1})^T] w = w^T S_{c1} w\)</span></li></ul></li><li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li><li><span class="math inline">\(L(w) = \frac{w^T S_b w}{w^T S_w w}, \hat{w} = argmax \ L(w)\)</span><ul><li><span class="math inline">\(\frac{dL}{dw} = 2S_b w (w^T S_w w)^{-1} + (w^T S_b w) * -(w^T S_w w)^{-2} * 2 * S\_w w = 0\)</span></li><li><span class="math inline">\(S_b w (w^T S_w w) = (w^T S_b w) S_w w\)</span> （<span class="math inline">\((w^T S_w w)\)</span> 最终计算得一个实数，一维，没有方向）（求解<span class="math inline">\(\hat{w}\)</span>关心的是方向，因为平面的大小可以缩放，所以意义不大）</li><li><span class="math inline">\(w = (w^T S_w w)/(w^T S_b w)S_w^{-1}S_b w\)</span>，正比于<span class="math inline">\((S_w^{-1}S_b w)\)</span> ，正比于<span class="math inline">\((S_w^{-1}(X_{c1} - X_{c2}))\)</span></li><li>（<span class="math inline">\(S_b w = (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w，(X_{c1} - X_{c2})^T w\)</span>为实数）</li><li>（若<span class="math inline">\(S_w\)</span>是对角矩阵，各向同性，<span class="math inline">\(S_w\)</span>正比于单位矩阵，则<span class="math inline">\(w\)</span>正比于<span class="math inline">\((X_{c1} - X_{c2})\)</span></li></ul></li></ul></li><li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li></ul></li><li><p><strong>Logistic Regression</strong></p><ul><li><font color="red">线性回归------&gt;sigmoid-------&gt;线性分类</font></li><li>判别模型</li><li>model<ul><li><span class="math inline">\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</span>，将<span class="math inline">\(w^T x\)</span>映射到处于[0,1]区间的概率值p</li><li><span class="math inline">\(p_1 = P(y=1|x) = sigmoid(w^T x) = \frac{1}{1+e^{w^T x}}\)</span></li><li><span class="math inline">\(p_0 = P(y=0|x) = sigmoid(w^T x) = \frac{e^{w^T x}}{1+e^{w^T x}}\)</span></li><li>综合表达：<span class="math inline">\(P(y|x) = p_1^y * p_0^{1-y}\)</span></li></ul></li><li><span class="math inline">\(\hat{w} = argmax \ P(Y|X) = argmax \ log^{\prod P(y_i|x_i)} = argmax \ \sum log^{P(y_i|x_i)} \\= argmax \ \sum[y_i*log^{p_1} + (1-y_i)*log^{p_0}] \Leftrightarrow (-cross entropy)\)</span><ul><li>MLE &lt;=&gt; loss function (min cross entropy)</li></ul></li></ul></li><li><strong>Gaussian Discriminant Analysis</strong><ul><li>生成模型、连续<ul><li><span class="math inline">\(\hat{y} = argmax \ P(y|x) = argmax \ P(x|y)P(y)\)</span></li><li>分类：最终比较<span class="math inline">\(P(y=0|x)，P(y=1|x)\)</span>大小</li><li><span class="math inline">\(P(y|x)\)</span>正比于<span class="math inline">\(P(x|y)P(y)\)</span>，即联合概率<span class="math inline">\(P(x, y)\)</span></li></ul></li><li>Data：N个d维样本，二分类(0,1)，正样本个数<span class="math inline">\(N_1\)</span>，方差<span class="math inline">\(S_1\)</span>，负样本个数<span class="math inline">\(N_2\)</span>，方差<span class="math inline">\(S_2\)</span></li><li><strong>prior probability</strong><ul><li>先验概率服从伯努利分布</li><li>y ~ Bernoulli，<span class="math inline">\(P(y=1) = p，P(y=0) = 1-p\)</span></li><li><span class="math inline">\(P(y) = p^y(1-p)^{1-y}\)</span></li></ul></li><li><strong>conditional probability</strong><ul><li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li><li>x|y=1 ~ N(u<sub>1</sub>, σ<sup>2</sup>)</li><li>x|y=0 ~ N(u<sub>2</sub>, σ<sup>2</sup>)</li><li>方差一样（权值共享），均值不一样</li><li><span class="math inline">\(P(x|y) = N(u_1, σ^2)^y * N(u_2, σ^2)^{1-y}\)</span></li></ul></li><li><strong>loss function</strong><ul><li><span class="math inline">\(log^{MLE} → L(θ) = log^{\prod P(x_i, y_i)} = \sum log^{P(x_i|y_i)P(y_i)} = \sum [log^{P(x_i|y_i)} + log^{P(y_i)}] \\ = \sum [log^{N(u_1, σ^2)^{y_i} * N(u_2, σ^2)^{1-y_i}} + log^{p^{y_i}*(1-p)^{1-y_i}}] \\ = \sum[y_ilog^{N(u_1, σ^2)} + (1-y_i)log^{N(u_2, σ^2)} + y_ilog^p + (1-y_i)log^{1-p}]\)</span></li><li><span class="math inline">\(θ = (u_1, u_2, σ, p)\)</span></li><li><span class="math inline">\(\hat{θ} = argmax \ L(θ)\)</span></li><li>求解4个参数<ul><li><span class="math inline">\(p\)</span><ul><li>相关部分 <span class="math inline">\(L = \sum [y_ilog^p + (1-y_i)log^{1-p}]\)</span></li><li><span class="math inline">\(\frac{dL}{dp} = \sum [\frac{y_i}{p} - \frac{1-y_i}{1-p}] = 0 → \sum [y_i(1-p)- (1-y_i)p] = \sum(y_i - p) = 0\)</span></li><li><span class="math inline">\(\hat{p} = \frac{1}{N} \sum y_i = \frac{N_1}{N}\)</span>（二分类（0,1），<span class="math inline">\(\sum y_i = N_1\)</span>）</li></ul></li><li><span class="math inline">\(u_1\)</span> （同理 <span class="math inline">\(u_2\)</span>）<ul><li>相关部分 <span class="math inline">\(L = \sum y_ilog^{N(u_1, σ^2)} = \sum y_ilog^{\frac{1}{(2\pi)^{\frac{d}{2}}σ^{\frac{1}{2}}} e^{\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ}}}\)</span></li><li><span class="math inline">\(u_1 = argmax L ∝ argmax \sum y_i\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ} = argmax \frac{-1}{2} \sum y_i[(x_i-u_1)^T(x_i-u_1)σ^{-1}] \\ = argmax \frac{-1}{2} \sum y_i[x_i^Tσ^{-1}x_i-2u_1^Tσ^{-1}x_i+u_1^Tσ^{-1}u_1]\)</span></li><li><span class="math inline">\(\frac{dL}{du_1} = \frac{-1}{2}\sum y_i[-2σ^{-1} x_i + 2σ^{-1} u_1] = 0 → \sum y_i(u_1 - x_i) = 0\)</span></li><li><span class="math inline">\(u_1 = \frac{\sum y_i x_i}{\sum y_i} = \frac{\sum y_i x_i}{N_1}\)</span></li></ul></li><li>σ<ul><li>相关部分<span class="math inline">\(L = \sum [y_ilog^{N(u_1, σ^2)}+(1-y_i)log^{N(u_2, σ^2)}] = \sum [log^{N(u_1, σ^2)}+log^{N(u_2, σ^2)}]\)</span><ul><li>（二分类，非0即1，可以拆分算，可以省去<span class="math inline">\(y_i\)</span>）</li></ul></li><li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li><li><span class="math inline">\(σ = \frac{1}{N} (N_1S_1 + N_2S_2)\)</span></li></ul></li></ul></li></ul></li></ul></li><li><p><strong>Naive Bayes</strong></p><ul><li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong><ul><li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li><li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li><li>最简单概率图模型</li></ul></li><li>生成模型、离散</li><li>Data<ul><li>X: data, (n, d), n个数据样本，每个d维向量</li><li>Y: class, Y={c<sub>1</sub>, c<sub>2</sub>, ...,c<sub>k</sub>}, k个类别</li><li>y: label, (1, n), n个标签</li></ul></li><li><strong>prior probability</strong><ul><li>P(Y=c<sub>k</sub>)</li><li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li><li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li><li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li><li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li><li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li></ul></li><li><strong>conditional probability</strong><ul><li><span class="math inline">\(P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(d)}=x^{(d)}|Y=c_k) = \prod_{j}^{d} P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li><li>特征条件独立（上标表示第j-th维度）</li></ul></li><li><strong>joint probability distributions</strong><ul><li>联合概率分布</li><li><span class="math inline">\(P(X, Y) = P(X|Y)P(Y)\)</span></li></ul></li><li><strong>posterior probability</strong><ul><li><span class="math inline">\(P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum P(X=x|Y=c_k)P(Y=c_k)} = \frac{P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}\)</span></li><li>则分类器为<ul><li><span class="math inline">\(y=f(x) = argmax \ P(Y=c_k|X=x) ∝ argmax \ P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li><li>分母不变，则仅与分子成正比</li><li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li><li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li></ul></li></ul></li><li><strong>loss function</strong><ul><li>最大后验概率转-&gt;期望风险最小化</li><li><span class="math inline">\(L(Y, f(x)) = \left\{\begin{matrix}  1 \ if \ Y!= f(x)\\  0 \ if \ Y=f(x) \end{matrix}\right.\)</span><ul><li>Y: train label, y=f(x) : predict label</li></ul></li></ul></li><li>期望风险函数：<span class="math inline">\(R_{exp(f)} = E[L(Y, f(x))]\)</span><ul><li>根据联合概率分布：<span class="math inline">\(R_{exp(f)} = E_x \sum [L(c_k|f(x))]P(c_k|X)\)</span></li></ul></li><li><span class="math inline">\(f(x) = argmin \ Σ[L(c_k|f(x))]P(c_k|X)\)</span><ul><li>根据L(Y, f(x))函数展开，消去Y=f(x)项</li><li><span class="math inline">\(f(x) = argmin \ \sum P(y \neq c_k|X=x) = argmin \ (1-P(y=c_k|X=x)) = argmax \ P(y=c_k|X=x)\)</span></li><li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c<sub>k</sub>类别的最大概率为多少）</li></ul></li><li>详情案例见统计学习方法(第二版)63页</li><li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;statistics-for-machine-learning&quot;&gt;Statistics for Machine Learning&lt;/h4&gt;
&lt;h5 id=&quot;one.-两大派系&quot;&gt;One. 两大派系&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;频率派：统计机器学习&lt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Bayes_MRF</title>
    <link href="http://yoursite.com/2020/08/12/Bayes-MRF/"/>
    <id>http://yoursite.com/2020/08/12/Bayes-MRF/</id>
    <published>2020-08-12T12:10:46.000Z</published>
    <updated>2020-10-07T02:51:13.878Z</updated>
    
    <content type="html"><![CDATA[<h4 id="bayes-network贝叶斯网络-markov-random-fields-马尔可夫随机场">Bayes Network贝叶斯网络 &amp; Markov Random Fields 马尔可夫随机场</h4><h5 id="one.-前提">One. 前提</h5><ul><li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）<ul><li>Representation 表示<ul><li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li><li>无向图 Markov Network (Markov Random Fields) (无向，则结点的概率仅取决于1阶邻居)</li><li>高斯图 （连续）<ul><li>Gassian Bayes Network</li><li>Gassian Markov Network</li></ul></li></ul></li><li>Inference 推断<ul><li>精确推断<ul><li>Variable Elimination, Belief Propagation, Junction Tree Algorithm</li></ul></li><li>近似推断<ul><li>确定性近似（变分推断）</li><li>随机性近似（蒙特卡洛，MCMC）</li></ul></li></ul></li><li>Learning 学习<ul><li>参数学习<ul><li>完备数据（非隐变量）（有向，无向）</li><li>隐变量（EM）</li></ul></li><li>结构学习 (学习更好的图结构，参数)</li></ul></li></ul></li><li><strong>高维随机变量</strong> P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大<ul><li>边缘概率 P(x<sub>i</sub>)</li><li>条件概率 P(x<sub>j</sub>|x<sub>i</sub>)</li></ul></li><li><strong>运算原则</strong><ul><li>sum rule: <span class="math inline">\(P(x_{1}) = \int_{x_{2}} P(x_{1}, x_{2}) d_{x_{2}}\)</span> (边缘概率)</li><li>poduct rule: <span class="math inline">\(P(x_{1}, x_{2}) = P(x_{1})*P(x_{2}|x_{1}) = P(x_{2})*P(x_{1}|x_{2})\)</span></li><li>chain rule: <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li><li>bayesian rule: <span class="math inline">\(P(x_{2}|x_{1}) = \frac{P(x_{1}, x_{2})}{P(x_{1}) }= \frac{P(x_{2})*P(x_{1}|x_{2}) }{\int P(x_{1}, x_{2}) d{x_{2}}}\)</span></li></ul></li><li><strong>缺点</strong><ul><li><font color="red">高维复杂 P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大</font></li><li>简化<ul><li>每个<strong>维度之间相互独立</strong> (特性太强了)<ul><li><span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i})\)</span></li><li>Naive Bayes 朴素贝叶斯 <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y)\)</span></li></ul></li><li>Markov Property 马尔可夫特性<ul><li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li><li>x<sub>i+1</sub> 只与 x<sub>i</sub>相关，与其他 x<sub>i-1</sub>,...,x<sub>1</sub>无关</li></ul></li><li><strong>条件独立性</strong>（可降低计算复杂度）<ul><li>给定x<sub>B</sub>情况下，集合x<sub>A</sub>与集合x<sub>C</sub>无关 （x<sub>A</sub>，x<sub>B</sub>，x<sub>C</sub>无交集）</li><li>x<sub>B</sub> 只与x<sub>C</sub>相关 <a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a> <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li></ul></li></ul></li></ul></li></ul><h5 id="two.-bayes">Two. Bayes</h5><ul><li>链式法则 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li><li><p>因子分解 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{p(i)})\)</span>（x<sub>p(i)</sub>为x<sub>i</sub>父亲集合，即指向x<sub>i</sub>的结点）（条件独立性） <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p></li><li><strong>tail to tail</strong><ul><li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A)\)</span></li><li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span><ul><li>则 <span class="math inline">\(P(C|A) = P(C|A,B)\)</span>（A，B同时发生时，不影响C），则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，B,C相互独立，“倒V路径”</font>）</li></ul></li><li>条件独立性：<span class="math inline">\(P(B|A)P(C|A) = P(B|A)P(C|A,B) = P(B,C|A)\)</span></li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p><ul><li><strong>head to tail</strong><ul><li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|B)\)</span></li><li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span></li><li>发生B时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞，A,C相互独立</font>）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p></li><li><strong>head to head</strong><ul><li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A、B有关系，不独立则难以分解</font>）</li><li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B)P(C|A,B)\)</span> （父亲结点先于子结点）</li><li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span><ul><li><span class="math inline">\(P(B) = P(B|A)\)</span>，则默认情况下，C还没被观察，A,B相互独立</li></ul></li><li>这个模式是想判断 <span class="math inline">\(P(A|C) == P(A|C,B)\)</span>，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p><ul><li>（<font color="red">若D被观测，路径也是是连通，A、B有关系</font>）</li></ul></li><li>有向图是的条件独立性（证明发生x<sub>B</sub>, x<sub>A</sub>, x<sub>C</sub>相互独立）<ul><li>D-separation<ul><li><p>x<sub>A</sub>, x<sub>B</sub>, x<sub>C</sub> 三个集合两两无交集 <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p></li><li>若A与C，存在B<sub>1</sub>，B<sub>2</sub>关系，且属于x<sub>B</sub>集合；存在B<sub>3</sub>，B<sub>4</sub>关系，且不属于x<sub>B</sub>集合</li><li>则符合：发生x<sub>B</sub>时，存在上述情况， x<sub>A</sub>, x<sub>C</sub>相互独立 （<strong>全局马尔可夫性</strong>）</li><li><span class="math inline">\(P(x_{i}|x_{-i}) = \frac{P(x_{i}, x_{-i})}{P(x_{-i})} = \frac{p(x)}{\int P(x_{i}) d{x_{i}}} = \frac{\prod_{j} P(x_{j}|x_{p(j)})}{\int \prod_{j} P(x_{j}|x_{p(j)})d_{x_{i}}}\)</span><ul><li>x<sub>-i</sub>表示集合{x<sub>1</sub>,...x<sub>p</sub>}中去除x<sub>i</sub>, <strong>x/x<sub>i</sub></strong></li><li><span class="math inline">\(\prod_{j} P(x_{j}|x_{p(j)})\)</span> 分为与x<sub>i</sub>有关和无关两部分</li><li><p>则 <span class="math inline">\(\frac{\prod_{j} P(x_{j}|x_{p(j)})}{\int \prod_{j} P(x_{j}|x_{p(j)})d_{x_{i}}}\)</span>无关部分则可以约去</p><figure><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"><figcaption>bayes_6</figcaption></figure></li><li><span class="math inline">\(P(x_{i}, x_{-i}) = \prod_{j} P(x_{j}|x_{p(j)})\)</span>，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket</li><li>一个人与全世界的关系=一个人与身边人的关系</li></ul></li></ul></li></ul></li><li>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）<ul><li>离散<ul><li>单一<ul><li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y=1)\)</span> (x 是p维， 当y被观测时，x各维度相互独立)</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p></li></ul></li><li>混合<ul><li><p><strong>GMM</strong> 高斯混合模型（多个高斯分布） -&gt; 做聚类</p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/GMM.png" alt="GMM" style="zoom:75%;"></li></ul></li><li>时间<ul><li><strong>Markov Chain</strong> 马尔可夫链</li><li><strong>Gaussian Process</strong> 无限维高斯分布</li></ul></li><li>动态模型 = 混合 + 时间<ul><li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li><li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li><li><strong>Partide Filter</strong> （非连续，非高斯）</li></ul></li></ul></li><li><p>连续</p><ul><li><strong>Gaussian Bayes Network</strong> 高斯图</li></ul></li></ul></li></ul><h5 id="three.-mrf">Three. MRF</h5><ul><li>无向图</li><li><p>条件独立性，发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关 （<strong>global markov</strong> 全局马尔可夫）</p><ul><li>存在集合x<sub>A</sub>, x<sub>C</sub>被x<sub>B</sub>分割（对应 bayes D-separation）， 那么发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关</li></ul></li><li><strong>local markov</strong> 局部马尔可夫<ul><li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p></li></ul></li><li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p><ul><li>在集合x<sub>-i,-j</sub>(集合没有i，j结点), 对于任意两个点x<sub>i</sub>, x<sub>j</sub>没有直接连接，则相互独立, (i ≠ j)</li></ul></li><li>条件独立性体现的三个方面，并且相互等价，可以互推 global markov &lt;=&gt; local markov &lt;=&gt; pair markov &lt;=&gt; 基于最大团的因子分解<ul><li>clique团，最大团<ul><li>集合间的结点相互联通</li><li>在一个团无法添加结点，则是最大团</li><li>(c<sub>1</sub>, c<sub>2</sub>,...表示团)</li></ul></li></ul></li><li>因子分解 <span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}),Z = Σ_{x_{1}}...Σ_{x_{p}}\prod_{i}^{p}Φ(x_{c_{i}})\)</span> （用因子分解证明条件独立性）<ul><li>Φ 势函数， 必须为正（大于0）<ul><li><span class="math inline">\(Φ (x_{c_{i}}) = e^{-E(x_{c_{i}})}\)</span> (E为能量函数，也叫势函数)</li><li>Φ (x<sub>ci</sub>) &lt;-&gt; P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</li><li><span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}) = \frac{1}{Z} \prod_{i}^{p} e^{-E(x_{c_{i}})} = \frac{1}{Z} e^{-\sum E(x_{c_{i}})}\)</span></li><li>Gibbs Distribution是统计物理的名称，与指数族分布形式一样（俩个等价）</li><li>最大熵原理：在满足已知事实，最终推出分布属于指数族分布</li><li>结论：Gibbs Distribution &lt;=&gt; Markov Random Field</li></ul></li><li>c<sub>i</sub>最大团，x<sub>ci</sub>最大团随机变量集合</li><li>Z 为联合概率分布的归一化因子</li><li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li><li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li><li>Pair-MRF 因子分解：<span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i} Φ(x_{i}) \prod_{j} Φ(x_{i}, x_{j})\)</span> (考虑边)</li><li>最大后验概率推理（图像分割问题）：max<sub>x</sub> P(x)<ul><li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p><figure><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"><figcaption>pair-markov</figcaption></figure></li><li>假设<span class="math inline">\(θ(x_{i}) = -logΦ(x_{i}),θ(x_{i}, x_{j}) = -logΦ(x_{i},x_{j})\)</span></li><li><p><span class="math inline">\(max_{x} P(x)\)</span> -&gt; 能量最小化 -&gt; <span class="math inline">\(min_{x} E(x) = \sum θ(x_{i}) + \sum θ(x_{i}, x_{j})\)</span></p><figure><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"><figcaption>pair-markov_2</figcaption></figure><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p></li><li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p><figure><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"><figcaption>pair-markov_4</figcaption></figure></li><li><p>结点的设置势函数，一个前景一个背景</p><figure><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"><figcaption>pair-markov_5</figcaption></figure></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;bayes-network贝叶斯网络-markov-random-fields-马尔可夫随机场&quot;&gt;Bayes Network贝叶斯网络 &amp;amp; Markov Random Fields 马尔可夫随机场&lt;/h4&gt;
&lt;h5 id=&quot;one.-前提&quot;&gt;One. 前提
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>HMM_CRF</title>
    <link href="http://yoursite.com/2020/08/09/HMM-CRF/"/>
    <id>http://yoursite.com/2020/08/09/HMM-CRF/</id>
    <published>2020-08-09T12:29:28.000Z</published>
    <updated>2020-10-06T13:11:42.336Z</updated>
    
    <content type="html"><![CDATA[<h4 id="hidden-markov-model-隐马尔可夫-conditional-random-field-条件随机场">Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场</h4><h5 id="one.-time-sequence-series">One. Time Sequence / Series</h5><ul><li>在时间序列中，起伏状态可以被观测到（股票走势就是一种时间序列，股票中的涨跌）</li><li>无法观测到的是时间序列的<strong>隐状态</strong>（股票中是否处于牛市状态，这类的就是隐状态）<ul><li>隐状态会有很多</li><li>当知道隐状态存在时，每个隐状态被观测时，都是相互独立（互不干扰）</li><li>隐状态之间是离散的</li></ul></li></ul><h5 id="two.-hmm">Two. HMM</h5><ul><li>在概率中，隐马尔可夫模型对应所有状态是相互独立的</li><li><strong>P(q_t|q_t-1)</strong> Transition Probability 转移概率<ul><li>P(q_t|q_t-1, q_t-2,..., q_1) = P(q_t|q_t-1)<ul><li>每个状态只取决于前面一个状态，而非前面所有状态</li><li>即当前隐状态到下一个隐状态的概率</li></ul></li><li>Discrete 离散的（用矩阵表示 A，维度(k, k)，k是开个隐状态）</li></ul></li><li><strong>P(y_t|q_t)</strong> Emmission/Measurement Probability 发射概率 / 观测概率<ul><li>P(y_t|q_1,..., q_t-1, q_t, y_1,..., y_t-1) = P(y_t|q_t)<ul><li>已知当前状态 q_t，得到事实变化 y_t 的概率</li></ul></li><li>Discrete or Continuous</li><li>若是离散时，可以用矩阵表示 B，维度(k, L)，L是y的取值范围（也是离散的）</li><li>若是连续的，无法使用矩阵表示，y可能是连续的一个分布</li></ul></li><li>两个概率决定了整个隐马尔可夫模型</li><li><p>在信号中用HMM也很多，时间轴每段范围的信号就相当与一个y_t <img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm.png" alt="hmm" style="zoom:67%;"></p></li><li>每个隐状态的概率和为1，则矩阵行和为1</li><li>P(X) = ∫_y P(X, Y) dy</li><li><strong>P(y_1, y_2, y_3)</strong> = Σq_1 Σq_2 Σq_3 P(y_1, y_2, y_3, q_1, q_2, q_3) = Σq_1 Σq_2 Σq_3 <strong>P(y_3|q_3)P(q_3|q_2)*P(y_2|q_2)P(q_2|q_1)*P(y_1|q_1)P(q_1)</strong><ul><li>P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|y_1, y_2, q_1, q_2, q_3)*P(y_1, y_2, q_1, q_2, q_3)</li><li>利用马尔可夫性质：P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|q_3)*P(q_3|y_1, y_2, q_1, q_2)*P(y_1, y_2, q_1, q_2) = P(y_3|q_3)P(q_3|q_2)*P(y_1, y_2, q_1, q_2)</li><li>P(y_3|q_3), P(q_3|q_2), P(y_2|q_2), P(q_2|q_1), P(y_1|q_1) 在马尔可夫参数A, B矩阵中可得</li><li><strong>P(q_1) 是初始状态</strong>，这需要问题给出</li><li>则马尔可夫模型需要三个参数，λ={A, B, P(q_1)} (假设y是离散的)</li></ul></li><li>马尔可夫模型有什么用？<ul><li>找50人讲10个单词（动物名字）</li><li><strong>λ_cat = argmax_λ log P(y_1, ..., y_50|λ)</strong><ul><li>当初始条件为λ，最大为说cat的情况</li><li>记录所有单词的λ</li><li>用高斯或者高斯混合模型计算λ</li></ul></li><li>当来了个新人，说其中一个单词，那么他说哪个单词概率最大？<ul><li><strong>P(y_new|λ_cat)</strong>, ... 概率最大为结果</li><li>若所有该概率计算结果很小时，说明新人说的不是原10个单词</li></ul></li></ul></li><li>公式范化（计算量较大） <img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm_function.png" alt="hmm_function" style="zoom: 67%;"><ul><li><p>一个定义 <img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB.png" alt="FB" style="zoom:67%;"></p></li><li>优化计算量<ul><li>alpha_i(t) = P(y_1, ..., y_t, q_t=i)</li><li>alpha_i(1) = P(y_1, q_1=i) = P(y_i|q_1=i)P(q1) = b_i(y_1) P(q_1)</li><li>alpha_j(2) = P(y_1, y_2, q_2=j) = Σq_1P(y_1, y_2, q_1=i, q_2=j) = Σq_1 P(y_2|q_2=j)P(q_2=j|q_1=i)*P(y_1, q_1=i) = P(y_2|q_2=j) Σq_1 P(q_2=j|q_1=i)*alpha_i(1) = b_j(y_1) Σq_1 a_ij*alpha_i(1)</li><li>开始递归 alpha_j(t) = b_j(y_t) Σq_1 a_ij*alpha_i(t-1) = P(y_1, ..., y_t, q_t=j)<ul><li><strong>P(y_1, ..., y_t) = Σj P(y_1, ..., y_t, q_t=j) = Σj alpha_j(t)</strong></li></ul></li><li>通过贝叶斯公式 <img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB_2.png" alt="FB_2" style="zoom:67%;"><ul><li>P(Y, q_t=i|λ) = P(Y, q_t=i)P(q_t=i) = P(y_1, ..., y_t|q_t=i)P(y_t+1, ..., y_T|q_t=i)P(q_t=i) = P(y_1, ..., y_t, q_t=i)P(y_t+1, ..., y_T|q_t=i) = alpha_i(t) beta_i(t)</li></ul></li></ul></li></ul></li><li>如何学习HMM的参数<ul><li>λ = argmax_λ log P(Y|λ) （极大似然估计）</li><li>用EM学习<ul><li>E-step: Σq_1... Σq_t log(P(Y, Q))*P(Q,Y|θ^g) = Σq_1... Σq_t log(P(q_1)*ΠP(q_t|q_t-1)*ΠP(y_t|q_t)) * P(Q,Y|θ^g) = Σq_1... Σq_t [log(P(q_1)+Σlog(a_q_t-1,q_t)+Σlog(b_q_t(y_t))] * P(Q,Y|θ^g)</li><li>part 5,6还没看完(需要EM基础)</li></ul></li></ul></li></ul><p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLFze15KrfxbGPEHyjxddbbxVvLa5kilFf" target="_blank" rel="noopener">徐亦达系列</a> <a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/dynamic_model.pdf" target="_blank" rel="noopener">pdf</a></p><h5 id="three.-crf">Three. CRF</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;hidden-markov-model-隐马尔可夫-conditional-random-field-条件随机场&quot;&gt;Hidden Markov Model 隐马尔可夫 &amp;amp; Conditional Random Field 条件随机场&lt;/h4&gt;
&lt;h5 id
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spectral_Cluster</title>
    <link href="http://yoursite.com/2020/07/25/Spectral-Cluster/"/>
    <id>http://yoursite.com/2020/07/25/Spectral-Cluster/</id>
    <published>2020-07-25T05:19:54.000Z</published>
    <updated>2020-07-27T09:32:54.190Z</updated>
    
    <content type="html"><![CDATA[<h4 id="spectral-cluster-谱聚类">Spectral Cluster 谱聚类</h4><h5 id="一分类与聚类">一、分类与聚类</h5><p>1、分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类别标号y中。</p><p>2、聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。目的是个簇内的元素之间越相似，簇间的相似度越小。</p><h5 id="二k-means-与-spectral-cluster">二、k-means 与 spectral cluster</h5><p>k-means 每次选择k个中心点，将每个数据点归类到离它最近的那个中心点所代表的簇中，迭代多次，一直到迭代了最大的步数或者前后 <strong>J</strong> 的值相差小于一个阈值为止。(u_k为中心点，r_nk 为数据点 x_n 被归类到 cluster k 的时候为 1 ，否则为 0 ) ​ <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/kmean.png" alt="kmean"></p><ul><li>结点与附近的结点相似度会更高，距离远的结点相似度更低；因此，对角线上颜色更亮，右上角左下角的区域更暗</li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/affinity_matrix.png" alt="Similarity Matrix" style="zoom: 25%;"></p><p>然而，k-means初始的中心点是随机选的，每次选择结果不同，大部分情况结果还是令人满意，偶尔也会陷入局部最优。传统 k-means的x_n输入是每个结点的所有信息（即完整的N维度信息）。</p><p>spectral cluster 谱聚类只需要结点之间的相似度矩阵即可，并不需要结点的完整信息，因此不必像k-means那样要求N维的欧氏空间的向量。即抓住了结点之间的主要信息，排除了冗余信息，计算复杂度也更小，所以比传统聚类方法会更加健壮一些。</p><h5 id="三谱的涵义">三、谱的涵义</h5><p>对于谱的概念，简而言之，可以把谱认定为对一个信号（视频，音频，图像，图）分解成为一些简单的元素线性组合（小波基，图基）。为了使得这种分解更加有意义，可以使得这些分解的元素之间是线性无关的（正交的），也就是说这些分解的简单元素可以看作是信号的基。面对研究的东西，往往都会细分的更细小层面才能挖掘更主要的信息，犹如研究生物，则要细分到细胞、基因层面；研究物理，则要细分到质子、夸克层面；目前深度学习研究图片也是细分到像素层级，音频则细分到音频、音位层级。因此针对图也是如此。</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/spectrum.png" alt="spectrum" style="zoom: 50%;"></p><p>在信号处理中，谱就是傅立叶变换，它提供了不同频率下的正弦和余弦波作为基，将信号在这些基进行分解。</p><p>​ <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/fourier.png" alt="fourier" style="zoom: 67%;"></p><p>在图中，“谱”则是指对图的拉普拉斯矩阵的特征分解，特征分解后正交化的特征向量就是对应的正交基，所有的特征值的全体统称为拉普拉斯矩阵的谱，最后则可以用特征向量和特征值来表示图的信息。</p><h5 id="四graph-与-laplacian">四、Graph 与 Laplacian</h5><ul><li><p>Graph G = (V, E, X)</p><p>结构信息：V 表示结点集，E 表示结点之间的边集，A 表示N维的邻接矩阵（若是无向图，则邻接矩阵内元素是0/1; 若是有向图，则邻接矩阵内元素的值是具体权重值w），D表示N维的度矩阵（对应结点的拥有邻居结点数，放置在矩阵对角线上）。</p></li></ul><p>​ 特征信息：X表示结点的N维度信息。</p><ul><li><p><strong>为什么特征分解最终选择拉普拉斯矩阵而不是相似矩阵？</strong></p><p>因为拉普拉斯矩阵的独有属性，它是半正定矩阵，则特征值都是大于或等于0, 可以有多个0特征值，每个0特征值对应特征向量上的值大于0对应的节点之间具有连通性，对应一个子图。拉普拉斯矩阵拥有相似矩阵（邻接矩阵）的特性，又拥有独有特性，则更好表达图的信息。</p></li><li><p><strong>为什么需要使用归一化后的拉普拉斯矩阵？</strong></p><p>聚类的目的为两点：第一点是最小化簇间的相似度，第二点是最大化簇内相似度。未归一化的拉普拉斯矩阵仅能达到第一点，归一化的拉普拉斯矩阵 能达到两点要求。(具体如何达到两点的推导，见文章<strong><code>A Tutorial on Spectral Clustering</code></strong>)</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/laplacian.png" alt="Laplacain Matrix" style="zoom: 25%;"></p></li><li><p>特征分解</p><p>求解得到特征值、特征向量，小到大排序特征值，对应特征向量也排序，则最终得到选择前k个最小特征值对应的特征向量。</p><ul><li>在Eigenvector Matrix，第0列是特征值为0对应的特征向量，是个全1列向量（颜色相同）（因为第二步构造的是全连接矩阵，则仅有一个特征值为0）</li><li>第1列对应较亮色块是值大于0的情况，暗色块值小于0，因此较亮色块中对应结点属于一个簇；第2列则是黄色块部分属于一个簇，等。依次类推即可。</li><li>中间图为Eigenvector Matrix 通过t-SNE降到2维的结果，能将不同簇的结点都明显区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接。</li><li>右间图为Eigenvector Matrix 通过t-SNE降到3维的结果，在3D空间领域，也能将不同簇的结点区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接，黄色与棕色有一两个点连接。</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector.png" alt="Eigenvector Matrix" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_2d.png" alt="Eigenvector Matrix 2D" style="zoom: 25%;"> <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_3d.png" alt="Eigenvector Matrix 3D" style="zoom: 50%;"></p><p>即spectral cluster可以看作为node feature高维matrix data (n,d) 经过一个<a href="http://blog.pluskid.org/?p=290" target="_blank" rel="noopener">laplace mapping</a>降维得到一个低维embedding (n, k)，embedding中融入更主要的信息，舍弃冗余信息。</p><h5 id="五spectral-clustering">五、Spectral Clustering</h5><p>​ 获得embedding即执行k-means</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/orgin.png" alt="原数据图" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/k_means.png" alt="直接 k-means 的结果" style="zoom:25%;"></p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/SpectralClustering.png" alt="谱聚类结果" style="zoom:25%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;spectral-cluster-谱聚类&quot;&gt;Spectral Cluster 谱聚类&lt;/h4&gt;
&lt;h5 id=&quot;一分类与聚类&quot;&gt;一、分类与聚类&lt;/h5&gt;
&lt;p&gt;1、分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类别标号y中。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>machine_learning</title>
    <link href="http://yoursite.com/2020/05/06/machine-learning/"/>
    <id>http://yoursite.com/2020/05/06/machine-learning/</id>
    <published>2020-05-06T12:20:41.000Z</published>
    <updated>2020-05-06T12:30:41.607Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习-machine-learning-from-tju">机器学习 machine learning from TJU</h3><h4 id="one.-绪论">One. 绪论</h4><ol type="1"><li><p>什么是智能？</p><ul><li><strong>Self-adaption 自适应</strong> （迁移学习，通用AI模型 ( Artificial General Intelligence, 即strong AI)）</li><li><strong>Self-consciousness 自我意识</strong> （模糊决策）</li><li>运算智能：快速计算，存储</li><li>感知智能：人类五官的能力（视觉、听觉、触觉等）【已解决】</li><li>认知智能：大脑的能力（逻辑推理、知识理解、决策思考）<strong>（语言处理）</strong>（概念、意识、观念）（理解、思考、决策）【正在解决】</li></ul></li><li><p><strong>Turning Test</strong> 图灵测试：正常人分别和正常人、AI聊天，是否能分清人与AI（是否有用是哲学问题，AI是否伪装，从而不通过Turning Test）</p><p><strong>Behaviorism 行为主义</strong>，仅看行为是否符合智能，不管内部部分（有漏洞）</p><p><strong>Connectionism联结主义</strong>，只看内部构造（用神经网络模拟），符合大脑构造，则认为有智能（婴儿无法通过Turning test，但他结构是符合的）（但无法知道大脑构造，如何产生意识？）</p><p>模拟鸟的飞行，制造飞机（虽然达不到鸟内部的全部飞行系统，但能模拟飞行）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/turning test.png" alt="turning test" style="zoom: 33%;"></p></li><li><p>如何制造 AI？</p><ul><li><strong>&quot;Thinking&quot; by &quot;Searching&quot;</strong>：“思考”即“搜索” (Behaviorism)，类似搜索引擎，信息检索，在已有知识寻找最佳答案（大脑积累知识，面对问题，就是搜索大脑已有知识（但无法确定大脑是如何搜索的））<ul><li>Knowledge Graph 知识图谱</li></ul></li><li><strong>&quot;Learning&quot;</strong>：学习知识，发现新的知识<ul><li>什么是新的知识?（知识-&gt;模式-&gt;稳定的关联关系）</li><li><strong>pattern</strong> 模式识别（机器学习的前身）（语言的语法是一种模式，物理规律也是模式）</li><li>模式识别和机器学习的区别在于：前者喂给机器的是各种特征描述，从而让机器对未知的事物进行判断；后者喂给机器的是某一事物的海量样本，让机器通过样本来自己发现特征，最后去判断某些未知的事物。（机器学习在挖掘数据最终找到模式） （模式识别=数据挖掘）</li></ul></li><li><strong>&quot;Thinking&quot; by &quot;Learning&quot;</strong>：“思考”即“学习”，Known Data-&gt;Model-&gt;Unknown Data<ul><li>Model 模型就是对模式的大概猜测</li><li>y=f(x), f()就是模式</li><li>由于模式很多种，模型则用 y = ax+b 去猜测，算法则调整a和b参数</li><li>机器学习就是科学研究的自动化（确定变量-&gt;做实验-&gt;找到变量之间规律）</li></ul></li></ul></li><li><p>机器学习的基本框架</p><ul><li>一系列可能函数 &amp; 训练数据 -&gt; 通过算法 -&gt; 选出最好的函数</li><li><strong>Supervise Learning</strong> 监督学习，模型只需要找到输入和标记之间的关联关系（标记是人工的，不是自己手标的，就是已标好数据（这种要成本））</li><li><strong>Semi-Supervised Learning</strong> 半监督学习，少量部分样本标记的监督学习</li><li><strong>UnSupervise Learning</strong> 无监督学习，无标记，模型自己总结出类别（聚类）</li><li><strong>Reinforcement Learning</strong> 强化学习，利用间接”标记“来学习<ul><li>围棋的”输赢“，样本是棋局，直接标记是棋子在哪个位置是最好的（但没有这种标记，没有人知道哪里是最好的），间接标记是这个棋局是黑白输赢结果</li><li>online learning or 反复学习</li><li>输出是有反馈，对模型进行奖励机制</li></ul></li><li>基于规则的模型：人定义”特征“，人定义特征和输出之间的关系</li><li>基于统计的模型：人定义”特征“，模型确定特征和输出之间的关系（特征工程）<ul><li>cat？= 0.1*毛色+0.2*耳朵形状+0.3*眼睛形状 ...</li></ul></li><li>深度学习模型：人不定义”特征“，模型确定原始信息和输出之间的关系（可以达到 end-to-end model）（人类选择的特征未必是最好的）<ul><li>深度学习可以发现特征（通过是神经网络学习原始信息获得高阶特征，一些人类未必发现的特征）</li><li>越深越能发现复杂特征</li></ul></li></ul></li><li><p>AI的一些重要问题</p><ul><li><p>标记、model、feature</p></li><li><p>什么是好模型？</p><ul><li><p>（泛化能力）描述性，但难以具体化，不可计算（欠拟合Underfitting）</p></li><li><p>（性能）具体可计算，适合范围小，描述性差（过拟合<strong>Overfitting</strong>）（模型复杂性越高容易过拟合）（难以避免）</p></li><li><p>两者折中比较难</p></li><li><p>机器学习的最终目标是在未知数据上效果最好</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/overfitting.png" alt="overfitting" style="zoom: 50%;"></p></li><li><p>严格上，数据需要分 训练集，开发集，测试集（避免虚假结果）</p></li><li><p>”成功就是最大的失败“（越成功会越保守，而事物是发展的，会在新事物上会越失败）</p></li><li><p>面对企业：</p><ul><li>基于规则的模型：问题简单，大量已知知识（可解释性好；基于人归纳，会比较抽象，越抽象越鲁棒性）</li><li>基于统计的模型：数据量不大，有一些明确的特征（可解释性一般，但都能猜到；基于数据归纳，不够抽象）</li><li>深度学习模型：数据量<strong>大</strong>，算力高，没有明确特征，先验知识缺乏（黑箱子，可解释性<strong>差</strong>）</li></ul></li><li><p>面对科研：越复杂越好</p></li></ul></li><li><p><strong>Global Knownledge 世界知识</strong>（人工智能选特征选模型，仍需要辅助的知识（经验知识，常识））</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/global_knownledge.png" alt="global knownledge" style="zoom: 50%;"></p><ul><li>人的学习不需要太多数据样本（小样本学习）</li><li>人可以小样本学习，也是有经验知识（先验知识），但儿童学习语言无法解释（儿童无先验知识，”大脑有普遍语法存在“）</li><li>有了先验知识，就可以对训练样本要求少一些（小样本学习），只学习特殊的知识即可。</li></ul></li><li><p><strong>Explainable</strong> 可解释性</p><ul><li><p>为什么模型会这么决策？</p></li><li><p>有了可解释性，可追溯源头，从根本上改进它。</p></li><li><p>自动驾驶事故率低于人类司机，为什么我们却不信任它？</p><p>（因为自动驾驶出事故的原因不可解释，出事故是概率性的）</p></li><li><p>刷脸支付，出错也是不可解释的</p></li></ul></li><li><p><strong>Ethics</strong> 伦理问题</p><ul><li><p>若有意识的机器人是否拥有人权？</p></li><li><p>AI通过用户的非隐私数据获得隐私数据</p><p>（识别用户的性格或者需求，左右用户做出选择（尤其在选举或者个性化推荐））</p><p>（搜索引擎（<strong>主动获取信息</strong>）是用户信息入口，会影响国家整体发展）</p><p>（现在约50%信息是依照个性推荐（<strong>被动获取信息</strong>），会导致个性分化、信息茧房，会加大偏见和隔阂，局限在自己圈子，最终导致社会撕裂，不接受他人，无法全面认识世界）</p></li><li><p>若AI能预测一个人的犯罪概率，是否在犯罪前先控制他？</p></li></ul></li></ul></li></ol><h4 id="two.-machine-learning-foundations-from-台大林軒田">Two. Machine Learning Foundations from 台大林軒田</h4><ol type="1"><li><p><strong>machine learning = sample data + blurry pattern + not easily programmable definition</strong></p></li><li><p>Data Mining -&gt; feature -&gt; Machine Learning (在机器学习选择特征时，尽量选取特征之间相关性小的特征，相关性越大，则越冗余，就没意义了)</p></li><li><p>Machine Learning use data to compute hypothesis g that approximates target f. (Machine Learning ∈ Statitics)</p></li><li><p><strong>Perceptron 感知器</strong>，h(x) = sign(Σwi xi - threshold) (i =1,2,...)</p><ul><li><p>模拟神经细胞，接收信号，整合起来（<strong>加权求和</strong>），接收整体的信号超过某个阈值，则激活神经细胞</p></li><li><p>将threshold融入权重w，作为w0，h(x) = sign(Σwi xi) (i =0,1,2,...) = sign(w^T x)，大于0为正例，小于0为负例。（线性感知器）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Perceptron.png" alt="Perceptron" style="zoom:33%;"></p><ul><li><p>令 h(x)=0， x2为纵轴，x1为横轴，x2 = -w1/w2 * x1 - w0/w2，右图分类效果较好（w1比w2大，即x1特征比x2特征更重要）</p></li><li><p>PLA Perceptron Learning Algorithm 寻找最优划分的线性函数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_1.png" alt="update weight" style="zoom:43%;"></p></li><li><p>前提是线性可分的，则可以在有限步内停止，每次调整都会更接近完美分类面；若非线性，则无法停止。(大多数情况是非线性的，有noise；可用pocket算法，在非线性情况下，在一定调整步数下，选择错误率最低的结果)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_3.png" alt="weight update" style="zoom: 50%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_2.png" alt="PLA" style="zoom:43%;"></p></li></ul></li><li><p>perceptrons &lt;-&gt; linear (binary) classifiers</p></li></ul></li><li><p><strong>Types of Learning</strong> 机器学习的类型</p><ul><li>output space<ul><li>binary or more binary <strong>Classification</strong> （预测类别，划分样本）</li><li><strong>Regression</strong> （预测一个实数，样本的拟合问题，连接样本）<ul><li><strong>可以用回归任务做分类</strong></li><li>先算出一个数，设定阈值，然后可以分类</li></ul></li><li><strong>Structured</strong> learning (一维（序列结构，语法结构学习），二维（图结构），三维（蛋白质结构，分子结构)）</li></ul></li><li>data label<ul><li>supervised, semi-superivised, unsupervised, reinforcement</li></ul></li><li>protocol f =&gt; (x,y)<ul><li><strong>Batch</strong> Learning 批量学习（大量样本）</li><li><strong>Online</strong> Learning 在线学习（在线 =&gt; 持续学习，不断接收数据（少量），更新模型）<ul><li>Online + Batch，先大批量数据学习一个模型，再持续接收少量数据，更新模型（更新模型部分，并不是完全重新学习，否则就是多次批量学习了）</li><li>垃圾邮件分类，先训练通用的模型，根据用户个性再调整，形成个性化（每次再垃圾箱找到需要的邮件，即分错样本，就会为模型产生少量数据）（是否垃圾邮件，对每个人的意义不一样）</li><li>PLA，Reinforcement Learning</li></ul></li><li><strong>Active</strong> Learing 主动学习<ul><li>属于一种 Online Learning</li><li>同样基于少量样本调整模型，但Active Learning 模型主动向用户获取数据，Online Learning 是被动获得数据</li><li>垃圾邮件分类，若删除多个同用户的邮件，Active Learning会提问是否标记其邮件为垃圾邮件，若是，立即标记该用户为重要特征；而Online Learning则是等待用户标记垃圾邮件，需要多次标记才可以。</li></ul></li></ul></li><li>input space<ul><li><strong>concrete</strong> features 具体特征（物理意义明确）</li><li><strong>Raw</strong> features 原始特征（图片的像素，亮度，黑白）</li><li><strong>Abstract</strong> feature 抽象特征（没用任何物理意义）</li></ul></li></ul></li><li><p><strong>Feasibilityof Learning</strong> 学习的可行性</p><ul><li><p>是否可以学习知识？</p></li><li><p>训练样本是有限的，无法保证能学习到最好的 f()，只能逼近</p></li><li><p>Hoeffding's Inequality，模型在训练样本的错误率v，模型在整体样本错误率u</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/hoeffding.png" alt="hoeffding" style="zoom:43%;"></p><ul><li>&quot;v = u&quot; is probably approximately correct (PAC)</li></ul></li><li><p>在训练集效果好，在测试集的效果也会好的概率？</p><ul><li><p>有M个候选函数，则错误率就很大，则过拟合。（前提是数据在候选函数之间相互独立（线性无关））</p></li><li><p>若M个候选函数中存在线性相关的候选函数，即不是<strong>相互独立</strong>，则M不是无穷大，则有希望减少过拟合</p></li><li><p>Ein 测试集错误率，Eout未知数据错误率（机器学习做两件事，模型在训练集使Ein变小，再使Ein和Eout尽可能相等）</p></li><li><p>样本N越大，则结果越可靠</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pac.png" alt="PAC" style="zoom:43%;"></p><ul><li><p>相同数据（x1 x2）喂入两个候选函数（两条红线），得到结果一样，则两个候选函数<strong>相关</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/error.png" alt="error" style="zoom:43%;"></p></li><li><p>对PLA而言，分类是候选函数将样本一分为2（则分类相同的候选函数相关，则两个函数视为等价），n个样本，最多也是2^n个<strong>类别</strong>，即<strong>M=2^n</strong>，则<strong>M不是无穷大</strong>的（对所有问题，都不是无穷大的）</p></li><li><p>实际情况是 <strong>M&lt;&lt;2^n</strong></p></li><li><p>growth function 成长函数，给定n个样本，返回实际可分类别数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/growth_funtion.png" alt="growth_funtion" style="zoom:43%;"></p></li><li><p>问题不同，成长函数不一样（成长函数上限则为break point突破点）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different.png" alt="different" style="zoom:43%;"></p></li><li><p>机器学习要达到的目标</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Ein_Eout.png" alt="Ein_Eout" style="zoom:43%;"></p></li><li><p>参数的个数是决定模型复杂度的核心指标</p></li><li><p>自由度=这个模型的有多少参数，一个参数是一个维度，参数越多，自由度越高</p><ul><li><p>VC维=参数个数</p></li><li><p>VC维 the formal name of maximum non-break point</p></li><li><p>break point是成长函数的上限，k决定了成长函数的最多参数数量，从而决定了vc维</p></li><li><p>dvc = min_k-1</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/VC_dimension.png" alt="VC_dimension" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li><li><p><strong>Regression</strong> 回归</p><ul><li><p>Noise: 样本标记错误（正例标记成反例）</p></li><li><p>Probabilistic 概率函数：对输出不是确定性的，都是概率性的（容忍存在Noise）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different_error.png" alt="different_error" style="zoom:43%;"></p></li><li><p>分类：判断sample是否符合目标f()</p></li><li><p>回归：让sample离目标f()越近（error用平方，是在最低点是可微的，用绝对值是不可微的）</p><ul><li>用回归无法直接做分类，但可以缩小分类的范围，err_0/1 &lt;= err_sqr</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/logistics_regression.png" alt="logistics_regression" style="zoom:43%;"></p></li><li><p><strong>logistic regression</strong> （非线性回归）</p><ul><li>err 需要计算两个概率分布的差值（KL散度）</li><li>当数据为病人的特征数据，但患病只有0/1（患病与不患病），则需要求出整体患病的概率分布。</li><li>极大似然估计：给定输入输出，确定一个分布。</li><li>用logistics regression 训练一个分布接近极大似然估计的分布</li></ul></li><li><p><strong>Gradient Descent</strong> 梯度下降，用于update weight（随机梯度下降，是随机采样点，大方向和直接梯度下降是一致的，但复杂度翻倍）（步长=学习率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Gradient Descent.png" alt="Gradient Descent" style="zoom: 50%;"></p></li></ul></li><li><p><strong>Multiclass Classification </strong>多分类问题</p><ul><li><p>四分类拆成多个二分类问题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Multiclass Classification.png" alt="Multiclass Classification" style="zoom:50%;"></p><ul><li><p>正例大大少于负例，样本不平衡</p></li><li><p>四分类分成两类，形成一对一对的分类</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pairwise classifier.png" alt="pairwise classifier" style="zoom:43%;"></p></li></ul></li><li><p>Nonlinear Transform 训练分类</p></li><li><p>将非线性转成线性</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/nonlinear.png" alt="nonlinear" style="zoom:43%;"></p></li></ul></li><li><p><strong>Regularization</strong> 正则化</p><ul><li><p>缩小高次空间，控制在一定区间内，防止过拟合同时仍具有高次空间的能力</p></li><li><p>降低复杂度，减轻过拟合（缩减候选函数的个数M）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/regularization coefficient.png" alt="regularization coefficient" style="zoom:43%;"></p></li><li><p>可加入loss function一起训练正则化</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Regression.png" alt="Regression" style="zoom:43%;"></p></li><li><p>L1（有一堆特征，但有些是无用的，用L1可去除一些无用特征），L2（常用，比较柔和）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/l1_l2.png" alt="l1_l2" style="zoom:43%;"></p></li></ul></li></ol><h4 id="three.-nlp">Three. NLP</h4><ol type="1"><li><p>what is NLP?</p><ul><li><p>Turning Test 基于 NLP</p></li><li><p>感知智能 -&gt; CV</p></li><li><p>认知智能 -&gt; NLP</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_nlp.png" alt="text_nlp" style="zoom:43%;"></p></li><li><p>语义角色标注（施动者，受动者，描述）</p></li><li><p>理解 <strong>NLU: L -&gt; R</strong>; 生成 <strong>NLG: R -&gt; L</strong></p></li><li><p>让机器get到语言中的meaning</p></li><li><p>NLP 中不允许存在歧义（程序语言没有歧义，自然语言是存在歧义的）</p><ul><li>NLP 需要解决语义之间歧义</li></ul></li><li><p>创造一个 <strong>interlingua 中间语</strong>，允许所有自然语言均可以翻译成 interlingua，自然语言是动态的，则 interlingua 几乎不可创造<em>（没有 interlingua，就很难表示 meaning）</em></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_translate.png" alt="ml_translate" style="zoom:50%;"></p></li><li><p>对自然语言做语法分析，只能越来越逼近 interlingua</p></li><li><p>在深度学习，则用 respentation / embedding 向量来表示语义的 meaning</p></li><li><p>知识图谱 -&gt; 让机器获取先验知识（前期需要NLP处理数据挖掘实体之间的关系）</p></li></ul></li><li><p>NLP's hard point</p><ul><li><p>“哈士奇”不管在哪都是“哈士奇”；“同志”在不同语境表示不一样</p></li><li><p>歧义 &amp; 动态</p></li><li><p><strong>语言的本质是谎言</strong>。真话：能正确反映真正事实的话，谎言：不能正确表达真正事实的话（同一句话，不同人理解不一样，都带有各自的偏见，所有不能正确表达真正的事实）</p></li><li><p>符号系统：人类创造符号来表达信息，语言是其中一种。</p></li><li><p><strong>所指：meaning；能指：表达meaning的工具</strong></p><ul><li>所指，能指之间规律不可寻，具有任意性，但在特定的时间，地点可以有局部确定的规律（人类可根据古代壁画符号风格判断年份）</li></ul></li><li><p>语言的任意性所导致的歧义性、动态性，乃至非真实性是语言处理的根本性困难（非真实性：描述抽象概念，没有实体对应（白马非马））</p></li><li><p>基本歧义（语法结构、词义、词性...）</p></li><li><p>旧知识 -&gt; 先验知识 -&gt; 先验知识 + 小样本 -&gt; 新知识</p></li><li>乔姆斯基：存在一些普遍语法，并非局部的，是所有语言学的共性<ul><li>例如小孩子就可以小样本学习语言，但没有先验知识（并非多次听到语言，毕竟是教不会动物说话）</li><li>“递归”，语言存在递归结构</li><li>递归存在（语言/语义）自指结构（是产生悖论的主要原因之一）（“这句话是错的”）</li></ul></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics.png" alt="rule_statistics" style="zoom: 49%;"><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics_2.png" alt="rule_statistics" style="zoom:32%;"></p><ul><li>只需要判断<strong>特定词汇</strong>，就可以使用CNN（只需要判断句子中有“高兴”词语，就可以判定情感），但判断整句话的所有词，则需要使用RNN</li></ul></li><li><p>语言理解的关键：<strong>背景知识</strong>（上下文）</p><ul><li>在NLP中，所有词都是有歧义，必须要有一个固定场景，才能确定一个词的意思</li><li>语用即语义：词是在场景怎么用的，就是语义</li><li>例如描述人，重点在人与其他事物的关系，并不是人体内在结构</li><li>graph 是表示事物之间的关系</li><li>学习基于事物之间关联，相关性</li><li><strong>相关性</strong>恰恰是破解<strong>任意性</strong>（歧义&amp;动态）的钥匙！</li><li>NLP 用上下文约束自然语言的任意性</li></ul></li><li><p>表示学习（利用上下文表示语义）</p><ul><li>基于特征的可解释表示</li><li>基于深度学习编码的不可解释表示</li><li>与其他非语言对象相结合的表示：如网络表示学习</li><li>作为其他学习模型的输入：深度学习模型、线性学习模型</li></ul></li><li><p><strong>word embedding</strong> 词向量表示学习：<strong>基于上下文用向量表示这个词</strong>，向量则可以计算的</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_embedding.png" alt="word_embedding" style="zoom:43%;"></p><ul><li>坑：不在一个语义空间的词不能像比较；即使向量数值一样，意义不一样，词就是不一样（人名和电影不在一个空间）</li><li>两个word embedding之间差值，可以表示两者的关系</li></ul></li><li><p>预训练模型（通用知识的自动获取）：表示学习、语言模型、针对特定任务的与训练</p><ul><li><p>基于很大数据学习最基本的（几何）元素，作为其他模型的输入</p></li><li><p>属于传统机器学习（已知最基本元素，用于训练提取）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pre_training.png" alt="pre_training" style="zoom:43%;"></p></li><li><p>深度学习（黑盒子，不知道特征是否重要，用深度学习抽取特征）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DL.png" alt="DL" style="zoom:43%;"></p></li></ul></li><li><p>填补先验知识和模型能力（模型搜索空间）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_Process.png" alt="ml_Process" style="zoom:43%;"></p></li></ul></li><li><p>NLP基本任务</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/NLP_task.png" alt="NLP_task" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/essential model.png" alt="essential model" style="zoom:43%;"></p></li><li><p>NLP's essential models (<strong>Linear Models</strong>)</p><ul><li><p><strong>EM算法</strong>（<strong>Expectation-maximization algorithm</strong> 期望最大化算法）</p><ul><li><p>“猜测隐藏在文字背后的信息”</p></li><li><p>在概率模型中寻找参数<strong>最大似然估计</strong>或者<strong>最大后验估计</strong>的算法, 其中概率模型依赖于无法观测的<strong>隐性变量</strong>。</p><ul><li>观察结果依赖于隐藏状态。只能看到观察结果,看不到隐藏状态。如何知道隐藏状态生成观察结果的概率(模型参数)?</li></ul></li><li><p>知道其中一个，可以互相推导 （有输入输出（观察值是输入），做监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM.png" alt="EM" style="zoom:43%;"></p></li><li><p>当两个都不知道（即 只有输入，没有输出，则为无监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM algorithm.png" alt="EM algorithm" style="zoom:43%;"></p></li><li><p>EM算法一定能收敛，但只能局部最优，无法全局最优，可通过尝试多个初始值（瞎猜参数）来改进最优效果</p></li></ul></li><li><p><strong>ME (Maximum Entropy) 最大熵模型</strong></p><ul><li><p>“用特征去束缚语言的任意性”</p></li><li><p>信息熵：用来描述信息的不确定性</p><ul><li><p><strong>一个物理系统越无序（无能力流动，则无序），则能量越小，信息熵越大；越有序（能力按有序方向流动），能量越大，信息熵越小</strong></p></li><li><p>一个体系的能量达到完全均匀分布时，这个系统的熵就达到最大值</p></li><li><p>封闭系统总熵时不断增大的（能量传递完成，达到均衡），局部会出现熵减小的情况</p></li><li><p>能力来自于差异（判读是否有动能/势能/热能，对比其周围是否存在差异，有差异才存在能力流动）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Entropy.png" alt="Entropy" style="zoom:43%;"></p></li></ul></li><li><p><strong>分布均匀 &lt;=&gt; 熵最大 =&gt; 最合理结果</strong></p></li><li><p>信息熵越大信息量越大（信息的不确定性越强）（熵越大的系统承载信息的能力越大）</p></li><li><p>最大熵：保留全部的不确定性,把风险降到最小</p></li><li><p>最大熵原理指出,需要对一个随机事件的概率分布进行预测时,我们的预测应当<strong>满足全部已知的条件</strong>，而<strong>对未知的情况不要做任何主观假设</strong>。在这种情况下,概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。</p></li><li><p>条件熵</p><ul><li><p>在给定输入的情况下，计算输出概率，实际上在计算以输入为条件的条件熵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Condition Entropy.png" alt="Condition Entropy" style="zoom:43%;"></p></li></ul></li><li><p>最大熵模型：求解带约束(特征函数)的最优化问题</p><ul><li>引入拉格朗日乘子,定义拉格朗日函数,转化为特征加权和</li></ul></li></ul></li><li><p><strong>隐马尔可夫链 HMM</strong></p><ul><li><p>“语言是一个串”</p></li><li><p>一个隐状态序列产生一个观察值序列。每个隐状态依赖于前一个隐状态</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM.png" alt="HMM" style="zoom:43%;"></p><ul><li>转移概率：隐状态之间转移（转换）的概率</li><li>发射概率：隐状态产生观察值的概率</li></ul></li><li><p>HMM能解决的问题</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Model.png" alt="HMM_Model" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Trasfer.png" alt="HMM_Trasfer" style="zoom:43%;"></p><ul><li>有监督的情形：知道观察序列对应的状态值，直接对训练语料进行统计计数即可，即最大似然</li><li>无监督的情形：不知道观察序列对应的状态值，只知道可能的状态集合（用EM）</li></ul></li><li><p><strong>生成与判别</strong></p><ul><li><p>“纵观全局 or 聚焦一处” 分别对应 生成 or 判别</p></li><li><p>机器学习有两大类模型：生成式模型、判别式模型</p></li><li><p>生成模型：学习得到<strong>联合概率分布P(x,y)</strong>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。</p></li><li><p>判别模型：学习得到<strong>条件概率分布P(y|x)</strong>，即在特征x出现的情况下标记y出现的概率。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/generation and discrimination.png" alt="generation and discrimination" style="zoom:43%;"></p><ul><li>已知生成模型可以得到各个判别模型；已知判别模型无法得到生成模型，除非已知所有可能的判别关系</li><li>判别模型：<ul><li>优点：所需数据量小,计算量小，对单一类别判定准确率高。可随意增加新特征。</li><li>缺点：无法全局优化，只能完成目标任务，没有提供额外信息的潜力，应用范围受限。</li></ul></li><li>生成模型：<ul><li>优点：信息全面，可实现全局优化。</li><li>缺点：所需数据量大，计算量大，增加新特征的计算成本高。</li></ul></li></ul></li><li><p>为什么HMM是生成式模型?</p><ul><li>建模所有状态之间的转移关系和状态与所有词汇的发射关系</li><li>所有隐状态概率都要计算</li></ul></li><li><p>最大熵是判别式模型</p><ul><li>给定条件，计算结果（计算条件概率）</li></ul></li></ul></li><li><p><strong>MEMM 最大熵隐马：最大熵 + HMM</strong> (偏向最大熵)</p><ul><li><p>在解决序列标注问题时，HMM的输入信息只有参数(π, A, B)和观察序列(即每个状态对应的字)，<strong>没有办法接受更丰富的特征</strong>(例如更多的上下文文字等)。为了解决这个问题，提出了MEMM模型。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM.png" alt="MEMM" style="zoom: 33%;"></p></li><li><p>MEMM模型在预测当前状态时,将前一个状态和与当前观察值相关的一组特征一起做为最大熵模型的输入,来预测当前状态。MEMM与HMM不同，是判别式模型。（有点像RNN）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM model.png" alt="MEMM model" style="zoom:43%;"></p></li><li><p>HMM计算产生整个观察序列的最优状态序列，是全局最优。</p></li><li><p>MEMM计算单个观察值判定的单个最优状态，是局部最优。</p></li></ul></li><li><p><strong>CRF (Conditional Random Field) 条件随机场 (域)</strong></p><ul><li><p>“在更加宽广的上下文上进行判别”</p></li><li><p>HMM是特殊的CRF</p></li><li><p>CRF计算由整个观察序列判定的最优状态序列，是<strong>全局最优</strong>。其中，每个可能的状态序列的概率这样计算：对于序列中的每个状态计算一组特征函数值，然后计算所有状态的特征函数值之和并归一化。</p></li><li><p>判别式模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CRF.png" alt="CRF" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_MEMM_CRF.png" alt="HMM_MEMM_CRF" style="zoom:43%;"></p></li></ul></li><li><p><strong>SVM (support vector machine) 支持向量机</strong></p><ul><li><p>“从线性到非线性分类”</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM.png" alt="SVM" style="zoom: 33%;"></p></li><li><p>支持向量机(SVM)同最大熵一样是一种常用的线性(Log linear)分类器。</p></li><li><p>SVM求使得Margin最大的分类面,并将<strong>Margin上的向量称为支持向量</strong>（绿边上的向量点）。</p></li><li><p>通过计算样本与哪一类支持向量的内积更大来判断样本类别。</p></li><li><p>对于线性不可分的样本集,可以将其投射到高维空间中来分割。</p></li><li><p>SVM用核函数来方便计算高维空间中样本点之间的内积：核函数可以在原空间中计算高位空间中的内积。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM model.png" alt="SVM model" style="zoom:43%;"></p></li><li><p><strong>kernel function</strong> （在原维度计算kernel函数就可视为在高维做计算）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/kernel function.png" alt="kernel function" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Topic Models</strong> 主题模型</p><ul><li><p>LSA, pLSA, LDA</p></li><li><p>判断图片相似度：匹配像素</p></li><li><p>判断两篇文章相似度：匹配词汇，每篇文章词汇分布</p><ul><li><p>或者匹配词汇出现频率</p></li><li><p>下图有误，行不全是D1，是表示不同文章</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_match.png" alt="word_match" style="zoom:43%;"></p></li><li><p>词汇在文章出现，也相当于文章的上下文</p></li><li><p>避免选择类似冠词the出现频率过高，或者频率过低但又很重要的词汇</p></li><li><p>逆文档频率（term frequency-inverse document frequency, TF-IDF）</p><ul><li><p>TF-IDF 越高，则改词汇就很重要</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/TF-IDF.png" alt="TF-IDF" style="zoom:43%;"></p></li><li><p>但这个词汇向量只在乎了词语的出现频率（属于基于词袋），忽略了语义，词汇的信息（词汇序列才产生信息，与DNA碱基对序列同理）</p></li></ul></li><li><p>主题模型是基于词袋的模型</p></li><li><p>可用于检索，是否与某个‘词’相关，可以使用，但想知道与这个‘词’更细节的语义则不行</p></li></ul></li><li><p><strong>潜在语义分析LSA（Latent Semantic Analysis）</strong></p><ul><li><p>主题就是一个潜在语义</p></li><li><p>n个文章 -&gt; k个主题 -&gt; m词汇</p></li><li><p>文章-主题矩阵（文章涉及主题分布），主题-词汇矩阵（主题涉及词汇分布）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSA.png" alt="LSA" style="zoom: 25%;"></p></li><li><p>拥有右边矩阵，想得到左边两个矩阵</p></li><li><p>SVD 奇异值分解（矩阵分解）（存在负值，负值不符合物理意义的解释）</p></li><li><p>LSA只是形式上拟合了文档-主题-词汇的关系,但并没有真正表达这种关系</p></li></ul></li><li><p><strong>概率潜在语义分析 pLSA</strong></p></li><li><p>在LSA基础上，输出概率值，才可赋予物理意义</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA.png" alt="pLSA" style="zoom:43%;"></p></li><li><p>箭头表示依赖关系</p></li><li><p>P(d) 文章被抽中的概率（属于先验概率）</p></li><li><p>P(z|d) 给定文章，主题的概率，P(w|z) 给定主题，词汇的概率（两个属于后验概率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA model.png" alt="pLSA model" style="zoom:43%;"></p></li><li><p>求出所有边的概率，则可以得到 文本-主题矩阵，主题-词汇矩阵</p></li><li><p><strong><em>只知道观察值，想知道内部的两个隐状态，则用 EM算法</em></strong></p></li><li><p><strong>潜在狄利克雷分配 LDA</strong>（latent Dirichlet allocation）</p><ul><li><p>pLSA中单词和主题的先验分布都假设是均匀分布的，也就是假设我们对他们的先验分布一无所知。这种假设使得pLSA比较容易出现过拟合。</p></li><li><p>文档生成话题和话题生成单词的过程是典型的多项分布,在贝叶斯学习中，狄利克雷分布常作为多项分布的先验分布使用 LDA 将狄利克雷分布做为话题和单词生成的先验分布</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA.png" alt="LDA" style="zoom:43%;"></p></li><li><p>给文章根据狄利克雷分布随机分配个主题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA model.png" alt="LDA model" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Deep Learning Models</strong> 深度学习模型</p><ul><li><p>属于生成式模型</p></li><li><p>线性模型：所有特征加权求和，通过激活函数，则成为线性感知机</p><ul><li>logistic（缩小输出范围）</li><li>softmax（所有值变为概率分布，总值=1）</li><li>KL散度（交叉熵）评估输出概率分布和真实分布的差距</li></ul></li><li><p>人工神经网络</p><ul><li>神经元激活规则<ul><li>主要是指神经元输入到输出之间的映射关系,一般为非线性函数。</li></ul></li><li>网络的拓扑结构<ul><li>不同神经元之间的连接关系。</li></ul></li><li>学习算法<ul><li>通过训练数据来学习神经网络的参数。</li></ul></li></ul></li><li><p>ANN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ANN.png" alt="ANN" style="zoom: 50%;"></p></li><li><p><strong>全连接前馈神经网络</strong></p><ul><li>在前馈神经网络中,各神经元分别属于不同的层。整个网络中无反馈，信号从输入层向输出层单向传 播,可用一个有向无环图表示。</li><li>全连接复杂度高，发挥全部能力</li><li>容易过拟合（使用dropout价格低复杂度，避免过拟合）</li><li>通用近似定理<ul><li>对于具有线性输出层和至少一个使用 “挤压” 性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够,它可以以任意精度来近似任何从一个定义在实数空间中的有界闭集函数</li><li>“挤压” 性质：将输入数值范围挤压到一定的输出数值范围</li><li>不是“挤压”性质的就是线性的</li></ul></li><li>反向传播更新参数</li></ul></li><li><p>深度学习三个步骤</p><ul><li>定义网络 -&gt; 损失函数 -&gt; 优化</li></ul></li><li><p>梯度爆炸</p><ul><li>若初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后,可能会导致求导的结果很大，形成梯度爆炸</li></ul></li><li><p><strong>梯度消失</strong></p><ul><li>若使用标准化初始w，那么各个层次的相乘都是0-1之间的小数,而激活函数f的导数也是0-1之间的数,其连乘后,结果会变的很小，导致梯度消失</li></ul></li><li><p>Activation Function</p><ul><li>sigmoid是非0均值（相当于加了一个偏置），还计算指数（指数计算相对复杂）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/activation function.png" alt="activation function"></p></li><li><p><strong>CNN</strong></p><ul><li><p>让每个神经元不代表一个像素，而是代表一个区域，而且，区域更容易捕捉局部特征</p></li><li><p>生物学上局部感受野</p></li><li><p>结构特点：<strong>局部连接，权重共享</strong></p></li><li><p>同时使用多组卷积核,每个负责提取不同特征</p></li><li><p><strong>padding</strong> 在图片外面填充一圈0（在没有padding，则边缘像素被访问概率相对对较低）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/conv.png" alt="conv" style="zoom: 33%;"></p></li><li><p><strong>pooling 池化</strong>：卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CNN.png" alt="CNN" style="zoom:43%;"></p></li><li><p>优点：善于提取特征、适用于分类、可并行、效率高</p></li></ul></li><li><p><strong>RNN</strong></p><ul><li><p>假设每次输入都是独立的，也就是说每次网络的输出只依赖于当前的输入</p></li><li><p>一个网络的输出做为另一个网络的输入</p><ul><li>y3是取决于前面y1, y2 (但重要程度是不一样的，越近越重要（但不是什么时候都符合这个，在序列较长时，存在远距离相关，则需要LSTM/GRU）)，但没有考虑到后者y4, y5（若需要考虑上下文，则需要双向RNN）</li><li>最后的y则包含所有信息</li><li>马尔科夫链每个状态只由前一个状态影响 而RNN每一个节点由前面所有节点影响</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN.png" alt="RNN" style="zoom:43%;"></p></li><li><p>LSTM （长短期记忆神经网络）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p>GRU （降低复杂度，能达到LSTM的效果）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GRU.png" alt="GRU" style="zoom:43%;"></p></li><li><p>各种类型RNN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN model.png" alt="RNN model" style="zoom:43%;"></p></li><li><p>层叠循环神经网络：可以捕捉更加抽象的内涵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deep RNN.png" alt="deep RNN" style="zoom:43%;"></p></li><li><p>双向循环神经网络：可以捕捉两侧的上下文信息</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/BRNN.png" alt="BRNN" style="zoom:43%;"></p></li><li><p>递归神经网络 Recursive Neural Network</p><ul><li>自然语言的句法结构</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Recursive NN.png" alt="Recursive NN" style="zoom:43%;"></p><ul><li><p>递归神经网络实在一个有向图无循环图上共享一个组合函数</p></li><li><p>叶子节点为输入</p></li><li><p>对于有歧义的句子（句法的歧义，<strong>不知道（形容）词语指向哪个主语</strong>）或者图片（图片中<strong>物品属于哪个主人</strong>），（需要从属关系的时候）可以用递归神经网络，用树的结构区别句法的结构和物品所属</p></li><li><p>可以退化为循环神经网络（属于RNN的特例）</p></li></ul></li><li><p>优点：善于累积序列信息、适用于序列标注或编码、不可并行、效率低</p></li></ul></li><li><p><strong>Attention</strong></p><ul><li><p>基于RNN的机器翻译中的注意力现象：源语言词汇对每个目标语的依赖程度不同</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/autoenocder.png" alt="autoenocder" style="zoom:43%;"></p></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/attention.png" alt="attention" style="zoom:43%;"></p></li></ul></li><li><p>different network</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different network.png" alt="different network" style="zoom:43%;"></p></li><li><p>word embedding</p><ul><li>one-hot(独热)编码<ul><li>向量维度为数据库中总词汇数,每个词向量在其对应词处取值为1，其余处为0</li><li>存在的问题: 维度灾难，语义鸿沟</li></ul></li><li>分布式表示 Distributed Representation</li></ul></li><li><p>假设一个单词的语义和这个单词的上下文是相关的，我们可以使用这个单词的上下文来表示这个单词的语义信息</p></li><li><p>延申：语义相似的单词也应该具有相似的上下文。</p><ul><li><p>上下文(context): 在附近出现的所有单词的集合。--&gt; 窗口window</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DR.png" alt="DR" style="zoom:43%;"></p></li><li><p>如何训练分布式</p><ul><li><p>共现矩阵 Co-occurrence Matrix</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/COM.png" alt="COM" style="zoom:43%;"></p></li><li><p>潜在语义分析 LSA (Latent Semantic Analysis)</p></li></ul></li><li><p>奇异值分解 Singular Value Decomposition</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVD.png" alt="SVD" style="zoom:43%;"></p></li><li><p>前馈神经网络语言模型 FNNLM</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM.png" alt="FNNLM" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM_2.png" alt="FNNLM_2" style="zoom:43%;"></p></li><li><p>Word2Vec</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word2vec.png" alt="word2vec" style="zoom:43%;"></p></li><li><p>CBOW (continuous bag-of-words)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CBOW.png" alt="CBOW" style="zoom:43%;"></p></li><li><p>Skip-Gram</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/skip-gram.png" alt="skip-gram" style="zoom:43%;"></p></li></ul></li><li><p>Pre-training 预训练</p><ul><li>先验知识 -&gt; 学习模型 &lt;- 经验数据<ul><li>模型选择</li><li>参数设定</li><li>领域知识</li></ul></li><li>预训练模型提供了先验知识，不需要知道后面的任务目标，获得embedding</li><li>越深层，特征越具体</li><li>小样本学习 = 预训练 + 微调</li><li>目标任务与云训练模型最好是同类型的，效果会更好</li><li>RNN:不能准确捕捉远距离依赖，不能并行<ul><li>解决并行问题:给每个词编码，然后在词编码上用NN输出一个向量，NN可以并行。可以并行的网络可以做的更深</li><li>解决远距离依赖问题：给每个词编码的时候用注意力机制</li></ul></li><li>Transformer是一个典型的Encoder-Decoder模型，最初用于机器翻译。其中间部分(Encoder的输出)，是一个句子的向量表示。因此,Transformer的Encoder部分可以用作句子向量的预训练模型。</li></ul></li><li><p><strong>GNN</strong></p><ul><li><p>卷积模型、序列模型</p></li><li><p>无论卷积还是序列模型，实际上都假定输入对象的结构是一个<strong>均匀</strong>的网络。换言之，就是基本元素(像素、词汇)之间的关系结构是处处相同的。（符合欧式距离）</p></li><li><p>但是，现实中元素之间的结构并不总是均匀的。而任意图才是元素结构的一般化表示,网格与序列都只是一般图的特例</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/non_eu_and eu.png" alt="non_eu_and eu" style="zoom:43%;"></p></li><li><p>具有一般图结构的对象十分广泛，都无法用普通的CNN和RNN有效处理</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_data.png" alt="graph_data" style="zoom:43%;"></p></li><li><p>若使用基于局部特征的方法来处理，一般图，如何定义卷积核的尺寸和方法? (<strong>CNN -&gt; GCN</strong>)</p></li><li><p>若使用序列的方法来处理一般图，如何给出序列的行走路线? (<strong>RNN -&gt; deepwalk</strong>)</p></li><li><p>通过NN获得<strong>embedding</strong>：包含 feature information + structure information</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_process.png" alt="graph_process" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/gnn_process.png" alt="gnn_process" style="zoom:43%;"></p></li><li><p><strong>SDNE (Structural deep network embedding)</strong></p><ul><li><p>同时优化一阶和二阶相似度</p></li><li><p>每个结点用一个自编码器来重建领域信息,从而建模二阶相似度</p></li><li><p>节点之间使用拉普拉斯特征映射(反映节点之间的距离)来惩罚使得相邻节点距离较远的编码结果</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SDNE.png" alt="SDNE" style="zoom: 50%;"></p></li></ul></li><li><p><strong>DeepWalk</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deepwalk.png" alt="deepwalk" style="zoom:43%;"></p></li><li><p><strong>Node2Vec</strong></p><ul><li><p>与DeepWalk的最大区别在于，node2vec采用有偏随机游走,在广度优先(bfs)和深度优先(dfs)图搜索之间进行权衡,从而产生比DeepWalk更高质量和更多信息量的嵌入</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/node2vec.png" alt="node2vec" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/bsf_dsf.png" alt="node2vec_" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/dsf_bsf.png" alt="dsf_bsf" style="zoom:43%;"></p></li></ul></li><li><p><strong>Metapath2vec</strong>: 异质性网络中的顶点表示</p><ul><li><p>随机路径必须符合预设的若干元路径(Metapath)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/metapath2vec.png" alt="metapath2vec" style="zoom:43%;"></p></li></ul></li><li><p><strong>LINE</strong>: explicitly preserves both first-order and second-order proximities.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LINE.png" alt="LINE" style="zoom:43%;"></p></li><li><p><strong>PTE</strong>: learn heterogeneous text network embedding via a semi-supervised manner.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/PTE.png" alt="{TE" style="zoom:43%;"></p></li><li><p><strong>GCN</strong></p><ul><li><p>以每个节点为核心，将其邻域设为卷积范围，卷积方法是汇聚邻居节点的信息做为核心节点的表示。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GCN.png" alt="GCN" style="zoom:43%;"></p></li></ul></li><li><p><strong>GAT</strong></p><ul><li><p>基本的GCN中邻居节点的权重是平均的。</p></li><li><p>GAT中邻居节点的权重是可以训练的参数。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GAT.png" alt="GAT" style="zoom:43%;"></p></li></ul></li><li><p>GCN和GAT 是Transductive learning: 训练语料包含待标注语料，标注在训练过程中完成。</p><ul><li>优点：质量高</li><li>缺点：扩展性差(标注新样本需要全局重新训练)</li><li>GCN和GAT的缺点：网络的任何变化都要重新进行全局训练 (类似word embedding)</li></ul></li><li><p><strong>GraphSage</strong> 是Inductive learning：训练语料不包含待标注语料，先训练获得模型,然后泛化到测试语料上。</p><ul><li><p>GraphSage学习一个由邻居节点形成中心节点表示的神经网络模型(聚合函数)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage.png" alt="graphsage" style="zoom:43%;"></p></li><li><p>GraphSage是分层的，类似神经网络的层次。每一层的节点表示由前一层的邻居节点通过聚合函数获得。</p></li><li><p>随着层次的推进，每个结点实际上不仅可以获得邻居结点的信息，还可以获得更远距离的结点的信息。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage2.png" alt="graphsage2" style="zoom:43%;"></p></li><li><p>GraphSage的参数学习需要设计一个损失函数。（有监督/无监督）</p></li><li><p>对于无监督学习，损失函数应该让临近的节点的拥有相似的表示。</p></li></ul></li><li><p>文本分类: <strong>Text-GCN 2019</strong></p><ul><li><p>以文档和词汇为结点构造异质性网络，训练获得文档的向量表示并分类到类别。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_gcn.png" alt="text_gcn" style="zoom:43%;"></p></li></ul></li><li><p><strong>关系抽取: 2018</strong></p><ul><li><p>以依存句法树做为GCN的输入图,得到词汇的表示,进而分类词汇是否为关系标记词</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/关系抽取.png" alt="关系抽取" style="zoom:43%;"></p></li></ul></li><li><p><strong>个性化推荐: 2018</strong></p><ul><li><p>建立用户-用户-物品关系图</p></li><li><p>在关系图上分别得到用户和物品的表示</p></li><li><p>基于用户和物品的表示建立Rating预测模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/个性化推荐.png" alt="个性化推荐" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;机器学习-machine-learning-from-tju&quot;&gt;机器学习 machine learning from TJU&lt;/h3&gt;
&lt;h4 id=&quot;one.-绪论&quot;&gt;One. 绪论&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;什么是智能？&lt;/p&gt;
&lt;u
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>RNN_LSTM</title>
    <link href="http://yoursite.com/2020/05/06/RNN-LSTM/"/>
    <id>http://yoursite.com/2020/05/06/RNN-LSTM/</id>
    <published>2020-05-06T12:11:28.000Z</published>
    <updated>2020-05-06T12:18:07.061Z</updated>
    
    <content type="html"><![CDATA[<h4 id="rnn">RNN</h4><ul><li><p><strong>recurrent</strong> (it performs the same function for every input)</p></li><li><p>the output of the current input depends on the <strong>past one</strong> computation</p></li><li><p>RNN can use their <strong>internal state (memory)</strong> to process sequences of inputs</p></li><li><p>In RNN, all the inputs are <strong>related</strong> to each other (In other neural networks, all the inputs are independent of each other)</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/rnn.png" alt="rnn" style="zoom:50%;"></p></li><li><p>activation function is <strong>tanh()</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/tanh.png" alt="tanh" style="zoom:43%;"></p></li><li><p>output</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/yt.png" alt="yt" style="zoom:43%;"></p></li><li><p>Advantages</p><ul><li><strong>RNN</strong> can model sequence of data so that each sample can be assumed to be dependent on previous ones</li><li><strong>RNN</strong> are even used with convolutional layers to extend the effective pixel neighborhood.</li></ul></li><li><p>Disadvantages</p><ul><li>Gradient vanishing and exploding problems</li><li>Training an RNN is a very difficult task</li><li>It cannot process very long sequences if using <em>tanh</em> or <em>relu</em> as an activation function</li></ul></li></ul><h4 id="lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</h4><ul><li><p>LSTM is a modified version of RNN, which makes it easier to <strong>remember</strong> past data in memory</p></li><li><p>The <strong>vanishing gradient</strong> problem of RNN is resolved here</p></li><li><p>LSTM is well-suited to classify, process and predict <strong>time series given time lags of unknown duration</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p><strong>Input gate</strong> — discover which value from input should be used to modify the memory</p><ul><li><strong>Sigmoid</strong> function decides which values to let through <strong>0,1.</strong></li><li><strong>tanh</strong> function gives weightage to the values which are passed deciding their level of importance ranging from<strong>-1</strong> to <strong>1</strong></li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/input_gate.png" alt="input_gate" style="zoom:43%;"></p></li><li><p><strong>Forget gate</strong> — discover what details to be discarded from the block</p><ul><li><strong>sigmoid</strong> function looks at the previous state(<strong>ht-1</strong>) and the content input(<strong>Xt</strong>) and outputs a number between <strong>0</strong> (<em>omit this</em>) and <strong>1</strong>(<em>keep this</em><strong>)</strong> for each number in the cell state <strong>Ct−1</strong>.</li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/forget_gate.png" alt="forget_gate" style="zoom:43%;"></p></li><li><p><strong>Output gate</strong> — the input and the memory of the block is used to decide the output</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/output_gate.png" alt="output_gate" style="zoom: 33%;"></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;rnn&quot;&gt;RNN&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;recurrent&lt;/strong&gt; (it performs the same function for every input)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the output of t
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Amino_acids_proteins</title>
    <link href="http://yoursite.com/2020/05/06/Amino-acids-proteins/"/>
    <id>http://yoursite.com/2020/05/06/Amino-acids-proteins/</id>
    <published>2020-05-06T11:17:00.000Z</published>
    <updated>2020-10-26T03:07:49.964Z</updated>
    
    <content type="html"><![CDATA[<h4 id="amino-acids-proteins">Amino acids &amp; proteins</h4><ul><li><p><a href="https://www.bilibili.com/video/BV1aK41157GP?from=search&amp;seid=9348904521485452131" target="_blank" rel="noopener">what is protein</a></p></li><li>所有蛋白质由21种（基本单元）氨基酸构成</li><li>Amino acids 由 carbon (C), Oxygen (O), Hydrogen (H), Nitrogen(N), Sulfur (S) 构成<ul><li>硒代半胱氨酸是唯一的含有一个 Sel (硒原子) 的标准氨基酸</li></ul></li><li>Amino acids 由 Amino Group (氨基), Carboxyl Group (羧基), Side Chain (侧链), Alpha Carbon (中央碳原子) 组成<ul><li>side chain 是不同氨基酸的唯一不同的部分，它决定了氨基酸的性质<ul><li><strong>Hydrophobic</strong> Amino acids 疏水性氨基酸具有丰富碳侧链，因此不能很好与水相互作用</li><li><strong>Hydrophilic</strong> Amino acids 亲水性（极性）氨基酸可以很好与水相互作用</li><li><strong>Charged</strong> Amino acids 带电荷的氨基酸与带相反电荷的氨基酸或其他分子相互作用</li></ul></li></ul></li><li><strong>primary structure（一级结构）</strong>是通过DNA编码的线性氨基酸序列，蛋白质中的氨基酸通过连接一个氨基酸氨基与另一个氨基酸羧基的肽键相连。每次肽键结合时都会释放一个水分子，相连的碳、氮、氧原子的序列构成了protein backbone（蛋白质的骨架）。</li><li>这些蛋白质链通常折叠成两种类型 <strong>secondary structure（二级结构）</strong><ul><li>alpha helix（螺旋）<ul><li>通过附近的氨基酸的氨基和羧基之间的 hydrogen bond (氢键) 稳定下来的右手螺旋线圈</li></ul></li><li>beta sheet（折叠）<ul><li>当两个或者多个相邻的链被氢键固定，形成 beta sheet</li></ul></li></ul></li><li><strong>tertiary structure（三级结构）</strong><ul><li>蛋白质链的三维形状</li><li>这种形状由构成链的氨基酸的性质决定</li><li>许多蛋白质形成球状，把疏水侧链包围在<strong>内部</strong>，远离周围的水</li><li>膜结合蛋白外面聚集着疏水残基，以便它们可以与膜中的脂质相互作用</li><li>带电荷的氨基酸允许蛋白质与具有互补电荷的分子相互作用</li><li>许多蛋白质的功能依赖于它们的三维形状<ul><li>血红蛋白形成一个袋状，以在中心保持血红素，一种含有铁原子的小分子，用来与氧气结合</li></ul></li></ul></li><li>quaternary structure（四级结构）<ul><li>两条或者更多条多肽链可以通过几个亚基结合在一起，形成一个功能分子</li><li>血红蛋白的四个亚基相互作用以便他们的符合物可以在肺部吸收更多氧气，并将其他释放到体内</li></ul></li><li>protein size<ul><li>大多数蛋白质小于光的波长</li><li>血红蛋白分子的尺寸约为6.5nm</li></ul></li><li>蛋白质的三维形状决定了他们的功能<ul><li>defense（防御）<ul><li>antibody 抗体灵活的手臂通过识别并与病原体结合以识别它们，作为免疫系统破坏的目标来保护我们远离疾病</li></ul></li><li>communication<ul><li>insulin 胰岛素是一种小而稳定的蛋白质可以在血液中旅行时保持形状，来调节血糖</li></ul></li><li>enzymes</li><li>alpha 淀粉酶是一种在唾液中消化淀粉的酶</li><li>transport<ul><li>钙泵由镁辅助并由ATP提供动力，在每次肌肉收缩后，将钙离子移动回肌浆网</li></ul></li><li>storage<ul><li>铁蛋白是一种带通道的球形蛋白质，根据有机体的需要，允许铁原子进入和退出，在铁蛋白内部形成一个空间，使铁原子附着在其内壁，铁蛋白以无毒形式储存铁。</li></ul></li><li>structure<ul><li>胶原蛋白形成强大的三重螺旋，用来在整个身体中支撑结构。胶原蛋白分子可以形成细长的原纤维，并聚集形成胶原纤维，这种类型胶原蛋白存在于皮肤和筋中</li></ul></li></ul></li><li>CDS (coding region 基因编码区，<a href="https://www.omicsclass.com/article/805" target="_blank" rel="noopener">Coding DNA Sequence</a>)<ul><li>完整一段基因，能翻译成蛋白质的区域是“间隔的、不连续的”（蛋白质编码序列和非蛋白质编码序列两部分组成）</li><li>编码序列（编码区（基因序列）中可翻译成蛋白质的序列）：exon 外显子</li><li><p>非编码序列（编码区（基因序列）中不可翻译成蛋白质的序列）：intron 内含子</p><figure><img src="https://github.com/soloistben/images/raw/master/protein/exon_intron.jpeg" alt="exon_intron"><figcaption>exon_intron</figcaption></figure><figure><img src="https://github.com/soloistben/images/raw/master/protein/mRNA_protein.jpg" alt="Gene"><figcaption>Gene</figcaption></figure></li><li>启动子（属于ORF的调控序列）是在DNA上，是作用于转录阶段，mRNA不包含启动子</li><li><font color="red">对于真核生物的大部分ORF在转录时，是包括外显子和内含子的。转录后，RNA在内含子处进行自我切割，只有外显子可以转录为成熟mRNA</font></li><li>mRNA上的CDS区域外显子的核苷酸--翻译--&gt;氨基酸</li><li><strong>CDS</strong>（mRNA）是<a href="https://weibo.com/ttarticle/p/show?id=2309404030921119537751" target="_blank" rel="noopener">mRNA上从起始密码子到终止密码子之间的RNA序列</a>（指编码一段蛋白产物的序列，是与蛋白质密码子一一对应的序列）</li><li><strong>ORF</strong>（DNA）是open reading frame的缩写，翻译成开放阅读框，基因的有意编码部分也就是开放阅读框（ORF）</li><li><strong>基因组DNA分为基因序列和非基因序列——基因序列就是一个完整的表达盒它包括ORF和ORF的调控序列——ORF转录后经加工，使得内含子被切除，外显子组成mRNA序列——mRNA包括了不能翻译的UTR序列和能翻译的CDS。</strong></li><li><p>CDS是ORF中不包含UTR的外显子部分</p><figure><img src="https://github.com/soloistben/images/raw/master/protein/CDS.png" alt="CDS"><figcaption>CDS</figcaption></figure></li></ul></li><li>氨基酸是蛋白质最小的结构单位</li><li>氨基酸+ … +氨基酸=肽</li><li>肽+ … +肽=蛋白质<ul><li>肽和氨基酸、蛋白质本就是同根生的物质，但在不同的类别中，各具特点和功能</li><li>蛋白质水解时，是相互缠绕、折叠和组合的结构首先遭到破坏，成为多肽；继续水解，肽键断裂，成为寡肽；彻底水解，就是氨基酸了</li></ul></li><li>人体吸收蛋白质主要形式是小分子活性多肽片段和游离氨基酸<ul><li>相对氨基酸的吸收，以多肽形式具有易吸收、主动吸收、优先吸收、完全吸收、可作为信使等特点</li><li>除了吸收更快以外，肽还具有的特殊的，甚至氨基酸和蛋白质都不具有的生理功能</li></ul></li><li><strong>10个以上氨基酸组成的肽被称为多肽，</strong></li><li><strong>2至9个氨基酸组成的就叫做寡肽</strong></li><li><p><strong>2至4个氨基酸组成的就叫做小分子肽或小肽</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;amino-acids-proteins&quot;&gt;Amino acids &amp;amp; proteins&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1aK41157GP?from=search&amp;a
      
    
    </summary>
    
    
    
      <category term="basic protein" scheme="http://yoursite.com/tags/basic-protein/"/>
    
  </entry>
  
  <entry>
    <title>Master_Eng</title>
    <link href="http://yoursite.com/2019/12/02/Master-Eng/"/>
    <id>http://yoursite.com/2019/12/02/Master-Eng/</id>
    <published>2019-12-02T04:51:06.000Z</published>
    <updated>2019-12-04T07:52:55.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="master-eng-硕士英语">#### Master Eng 硕士英语</h2><h5 id="presentation">Presentation</h5><ul><li>Audience 面向大部分受众</li><li>Visual Aids (PPT)</li><li>Presentor<ul><li>eye contact</li><li>story teller</li><li>body language</li><li>jargon 避免过多专业术语，用最简单的话让他人明白，必须要用时，给出解释</li><li>修辞, 自黑</li></ul></li></ul><h5 id="essay-小论文">Essay 小论文</h5><ul><li>unity: 强调一段要完整，主题鲜明</li><li>coherence 段句之间要求连贯</li><li>1段：Topic + support + conclusion</li><li>essay VS project<ul><li>same topic</li><li>same length</li><li>same structure</li><li>essay: literature review, project: do experiment</li></ul></li><li>An essay has three main parts: an introduction, a body, and a conclusion. Introduction consists of two part: general statements and <strong>thesis statement</strong> which plays specific role in the role in the essay, stating the specific topic, listing subtopics of the main topic, and indicating the pattern of organization of the essay, and writer’s position or point of view.  So a good thesis statement of introduction is first step of a well-written essay, at the same time, main body follow subtopics to expand supports, and conclusion responds the main topic of thesis statement.</li></ul><h5 id="interview">Interview</h5><ul><li><strong>First impressions</strong>: we need to be punctual and neat in our appearance, greeting each person with smile.</li><li><strong>Preparation</strong>: we need to know ourselves and the company, preparing some based questions for interviewers.</li><li><strong>Ending</strong>: Asking for a business card or ensuring the information about interviewer's name, and e-mail address, we can send a <strong>thank-you note</strong>.</li></ul><h5 id="documentation-or-reference">Documentation or Reference</h5><ul><li>APA (American Psycholopicaly Association) 用于硕士论文</li><li>MLA (Modern Language Association) 文学 （姓，年份，页码）</li><li>in-text citation（文内引用，每段引用后加括号（姓，年份））</li><li>Reference （文后引用）</li><li>姓. 名（年份）：书名（斜体）. 出版社：xxx. 页码（双写小写p）</li><li>book eg: <strong>Mills. S (2019): <em>Language</em>. London: Oup. pp16-17.</strong></li><li>journals eg: <strong>Mills. S(2019): Language. <em>Journal Name</em>. London:Oup. pp16-17. </strong></li><li>Internet eg: <strong>Mills (or Unknowner) (2019): <em>Language</em>. website. [date] </strong></li></ul><p><em>In school, many learning things depend on IQ, but in work, EQ + Money + Relarionship.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;master-eng-硕士英语&quot;&gt;#### Master Eng 硕士英语&lt;/h2&gt;
&lt;h5 id=&quot;presentation&quot;&gt;Presentation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Audience 面向大部分受众&lt;/li&gt;
&lt;li&gt;Visual Aids (P
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>algorithm</title>
    <link href="http://yoursite.com/2019/11/14/algorithm/"/>
    <id>http://yoursite.com/2019/11/14/algorithm/</id>
    <published>2019-11-14T12:44:27.000Z</published>
    <updated>2020-01-06T07:36:20.708Z</updated>
    
    <content type="html"><![CDATA[<h4 id="lcs-longest-common-subsequence">LCS Longest Common Subsequence</h4><ul><li>运用动态规划方法查找给定两个序列的最大公共子序列（子序列之间的字符可以不连续，若子串substring的字符必须连续）</li><li>由序列尾部开始：<ul><li>若两个序列尾部字符相等：LCS[i][j] = LSC[i-1][j-1] +1</li><li>若两个序列尾部字符不相等：LCS[i][j] = max(LSC[i-1][j], LSC[i][j-1])</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/lcs.png" alt="LCS"> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'GXTXAYB'</span></span><br><span class="line">str2 = <span class="string">'AGGTAB'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lcs</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    L = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#print(L)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                L[i][j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                L[i][j] = L[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                L[i][j] = max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    print(L)</span><br><span class="line">    <span class="keyword">return</span> L[<span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(lcs(str1, str2))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 4.]]</span><br><span class="line">4.0</span><br></pre></td></tr></table></figure><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/dynamic_lcs.png" alt="dynamic_lcs"> ###### Pseudocode <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: LCS(A, B)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">L[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then L[i][j]=L[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span> </span><br><span class="line">        <span class="keyword">else</span> L[i][j]=max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> L</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure></p><hr><h4 id="sa-sequence-alignment">SA Sequence Alignment</h4><ul><li>运用动态规划方法对给定两个序列做对比（原用于基因DNA链碱基的序列对比）</li><li>对比结果有三种情况：<ul><li>match 相等</li><li>unmatch 不想等</li><li>gap 缺额（两个序列长度不一致）</li></ul></li><li>使用打分原则（match不扣分，unmatch和gap扣不同的分数）</li></ul><p>eg. A G C T A U T<br>[第一A和最后T属于match，第二或第三U属于unmatch，第三或第二属于gap] [若match不扣分，unmatch扣3分，gap扣2分，则本次对比=5分，扣分（惩罚分）越低越好] <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'AGGGCT'</span></span><br><span class="line">str2 = <span class="string">'SAGE'</span></span><br><span class="line">p_unmatch = <span class="number">3</span></span><br><span class="line">p_gap = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SA</span><span class="params">(str1, str2, p_unmatch, p_gap)</span>:</span></span><br><span class="line">    P = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">                P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                P[i][j] = P[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                P[i][j] = min(&#123;P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_unmatch, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap&#125;)</span><br><span class="line">            </span><br><span class="line">    print(P)</span><br><span class="line">    <span class="keyword">return</span> P[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(SA(str1, str2, p_unmatch, p_gap))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.  2.  4.  6.  8.]</span><br><span class="line"> [ 2.  3.  2.  4.  6.]</span><br><span class="line"> [ 4.  5.  4.  2.  4.]</span><br><span class="line"> [ 6.  7.  6.  4.  5.]</span><br><span class="line"> [ 8.  9.  8.  6.  7.]</span><br><span class="line"> [10. 11. 10.  8.  9.]</span><br><span class="line"> [12. 13. 12. 10. 11.]]</span><br><span class="line">11.0</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: SA(A, B, p_gap, p_xy)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">P[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">        P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then P[i][j]=P[i<span class="number">-1</span>][j<span class="number">-1</span>] </span><br><span class="line">        <span class="keyword">else</span> P[i][j]=min(P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_xy, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap)</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> P</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure><h4 id="set-cover">Set Cover</h4><ul><li>给定一个集合B，和一些子集sets，在子集中找到最好的覆盖原集合的子集（贪心算法）</li><li>在t时刻，选择子集设为St，当前还剩集合元素个数为nt，以选子集的元素集合为P</li><li>则在子集中，每次选择与原集合<strong>相交</strong>最多元素的子集，且符合公式|St|&gt;= nt/|P|</li></ul><h5 id="s-0123456789">S = {0,1,2,3,4,5,6,7,8,9}</h5><h5 id="sets-01-23-142-235-17-46-368-79">Sets = {0,1}, {2,3}, {1,4,2}, {2,3,5}, {1,7}, {4,6}, {3,6,8}, {7,9}</h5><p>Let nt be the number of uncovered elements after step t, P be optimal selection Choose max (sets ∩ S) in each step: + Step 1: n1 = 10, S1 = {1,4,2}, P = S1 = {1,2,4} + Step 2: n2 = 7, S2 = {3,6,8}, P = S1US2 = {1,2,3,4,6,8} + Step 3: n3 = 4, S3 = {7,9}, P = S1US2US3 = {1,2,3,4,6,7,8,9} + Step 4: n4 = 2, S4 = {0,1}, P = S1US2US3Us4 = {0,1,2,3,4,6,7,8,9} + Step 5: n5 = 1, S5 = {2,3,5}, P = S1US2US3US4US5 = {0,1,2,3,4,5,6,7,8,9} = S</p><h5 id="so-cover-142u368u79u01u235-s">so cover {1,4,2}U{3,6,8}U{7,9}U{0,1}U{2,3,5} = S</h5><p>set cover problem need to obey <strong>|St+1| ≥ nt/|P| </strong> + if choose l S1| = 2 in step 1, n1 = 10, and n1/|P| = 10/2 = 5, so we need to choose a set containing 5 elements in step 2. That is contradictory. + if choose l S1| = 3 in step 1, n1 =10, and n1/|P| = 10/3 = 3, so we need to choose a set containing 3 elements in step 2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">B = set([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">sets = &#123;&#125;</span><br><span class="line">sets[<span class="number">1</span>] = set([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">sets[<span class="number">2</span>] = set([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">sets[<span class="number">3</span>] = set([<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">sets[<span class="number">4</span>] = set([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">sets[<span class="number">5</span>] = set([<span class="number">1</span>,<span class="number">7</span>])</span><br><span class="line">sets[<span class="number">6</span>] = set([<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">sets[<span class="number">7</span>] = set([<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>])</span><br><span class="line">sets[<span class="number">8</span>] = set([<span class="number">7</span>,<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">P = set()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> B:</span><br><span class="line">    best_sets = <span class="keyword">None</span></span><br><span class="line">    sets_covered = set()</span><br><span class="line">    <span class="keyword">for</span> sets_num, sets_value <span class="keyword">in</span> sets.items():</span><br><span class="line">        covered = B &amp; sets_value    <span class="comment"># 选择交集最大的set</span></span><br><span class="line">        <span class="keyword">if</span> len(covered) &gt; len(sets_covered):</span><br><span class="line">            sets_covered = covered</span><br><span class="line">            best_sets = sets_num </span><br><span class="line">    B -= sets_covered</span><br><span class="line">    print(best_sets,<span class="string">':'</span>,sets_covered)</span><br><span class="line">    P.add(best_sets)</span><br><span class="line">print(P)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3 : &#123;1, 2, 4&#125;</span><br><span class="line">7 : &#123;8, 3, 6&#125;</span><br><span class="line">8 : &#123;9, 7&#125;</span><br><span class="line">1 : &#123;0&#125;</span><br><span class="line">4 : &#123;5&#125;</span><br><span class="line">&#123;1, 3, 4, 7, 8&#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bellman_ford.png" alt="bellman_ford"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/bell.png" alt="bell"></p><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow1.png" alt="Network_Flow"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow2.png" alt="Network_Flow"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow3.png" alt="Network_Flow"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow4.png" alt="Network_Flow"></p><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine1.png" alt="Turing_machine"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine2.png" alt="Turing_machine"> <img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine3.png" alt="Turing_machine"></p><h4 id="p-vs-np">P vs NP</h4><ul><li>P 问题：现阶段可以用计算机在多项式时间内解决的问题(<strong>P</strong>olynomial Time)</li><li>NP问题：现阶段无法在多项式时间内解决，但可以在多项式时间内验证的问题(<strong>N</strong>on-deterministic <strong>P</strong>olynomial Time)</li><li>若P=NP，则任何问题均可用计算机破解，则没有加密可言，任意生物、医疗、经济、科技问题全部可以解决。</li><li>NP-complete：很多很难的NP问题，本质上是同一个问题（卡在了相同问题上），可以用简单的<strong>多项式时间</strong>转换（数学：<strong>这些NP问题中真正困难的部分</strong>），能解决NP-complete问题就可以解决NP问题（数独游戏、蛋白质折叠（治疗癌症问题））</li><li>证明P是否等于NP，就是一个NP问题</li><li>NP-hard = NP-complete + 各种指数级别问题（下棋的计算或验证）(NP-hrad问题 是假定有无限时间和空间) <img src="https://github.com/soloistben/images/raw/master/algorithm_image/NP.png" alt="NP"></li><li>P-SAPCE：在无限时间，只使用多项式数量的空间，就可以求解（包括NP-complete）</li><li>BPP: 多项式时间内有几率求解问题 （BQP = BPP的量子计算）（包括P）</li><li>比NP-hrad更难的问题 = &quot;没有任何电脑在任意时间或空间可解出来的问题&quot;（指数层叠，多项式层叠）</li><li>所有问题 -&gt; 在给定空间、时间下，什么可以计算出来（已经不在是探讨计算的特性，而是寻找时间和空间的本身性质） <a href="https://www.bilibili.com/video/av19085452?from=search&amp;seid=7087981409362725035" target="_blank" rel="noopener">b站</a></li></ul><ol type="1"><li>算法中线性结构，数据元素之间存在一对一的线性关系。</li><li>算法符号，Θ：渐近紧确界，O：渐近上界，Ω：渐近下界；o：非渐近紧确的上界（O提供的渐近上界可能是也可能不是渐近紧确的），ω：非渐近紧确的下界</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;lcs-longest-common-subsequence&quot;&gt;LCS Longest Common Subsequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;运用动态规划方法查找给定两个序列的最大公共子序列（子序列之间的字符可以不连续，若子串substring的字符必
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>GNN</title>
    <link href="http://yoursite.com/2019/08/19/GNN/"/>
    <id>http://yoursite.com/2019/08/19/GNN/</id>
    <published>2019-08-19T05:39:28.000Z</published>
    <updated>2019-08-20T01:40:45.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="note-of-graph-neural-networks-a-review-of-methods-and-applications">NOTE of Graph Neural Networks: A Review of Methods and Applications</h4><ol type="1"><li>As a unique non-Euclidean data structure for machine learning, graph analysis focuses on node classification, link prediction, and clustering.</li><li>Why GNN?</li></ol><ul><li>Composed to CNN, GNN is highly expressive representations</li><li>shared weights reduce the computational cost compared with traditional spectral graph theory; multi-layer structure is the key to deal with hierarchical patterns, which captures the features of various sizes.</li><li>CNN can only operate on regular Euclidean data like images (2D grid) and text (1D sequence), GNN can.</li><li>graph embedding learns to represent graph nodes, edges or sub-graphs in low-dimensional vectors.</li><li>Deep-Walk, which is first graph embedding, is based on representation learning, word embedding and Skip-Gram model. Just like node2vec. However,<ul><li>no parameters are shared between nodes, so it means the number of parameters grows linearly with the number of nodes.</li><li>The direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs</li></ul></li></ul><ol start="3" type="1"><li>GNN model input and/or output consisting of elements and their dependency.</li><li>There isn’t a natural order of nodes in the graph. (CNN &amp; RNN can’t input node of no special order)</li><li>In the standard neural networks, the dependency information is just regarded as the feature of nodes, not edge which represents the information of dependency between two nodes in a graph!</li><li>GNN update the hidden state of nodes by a weighted sum of the states of their neighborhood.</li><li>Human brain is almost based on the graph which is extracted from daily experience.</li><li>GNN can learn the <em>reasoning</em> graph from large experimental data.</li><li><strong>Message Passing Neural Network (MPNN)</strong> could generalize several graph neural network and graph convolutional network approaches.</li><li><strong>Non-local Neural Network (NLNN)</strong> unifies several “self-attention”-style methods.</li><li>Both of MPNN&amp;NLNN focus on specific application domains and can’t provide a review over other graph attention models.</li><li><strong>Graph Network (GN) </strong>has strong capability to generalize other models. However, the graph network model is highly abstract and only gives a rough classification of the applications.</li><li>Graph neural networks suffer from over-smoothing and scaling problems. There are still no effective methods for dealing with dynamic graphs as well as modeling non-structural sensory data.</li><li>Original framework:</li></ol><ul><li>Notations <img src="https://github.com/soloistben/images/raw/master/gnn_image/1.jpg" alt="chinese"> <img src="https://github.com/soloistben/images/raw/master/gnn_image/2.png" alt="eng"></li><li>The target of GNN is to learn a state embedding <strong>h_v ∈ Rs </strong>which contains the information of neighborhood for each node.</li><li>Let <strong>f</strong> be a parametric function, called local transition function, that is shared among all nodes and updates the node state according to the input neighborhood.</li><li>Let <strong>g</strong> be the local output function that describes how the output is produced.</li><li><strong>h_v = f (x_v, x_co[v], h_ne[v], x_ne[v]) o_v = g (h_v, x_v)</strong> (x is the features of v)</li><li><strong>Vectorize: H = F (H, X) O = G (H, XN)</strong> (F is global translation function, G is global output function)</li><li>The value of H is the fixed point of Eq.3 and is uniquely defined with the assumption that F is a contraction map.</li><li>GNN uses the following classic iterative scheme for computing the state. <strong>Ht+1 = F(Ht, X)</strong></li><li>With the target information (t_v for a specific node) for the supervision. <strong>loss = ⅀ (t_i − o_i)</strong></li></ul><ol start="15" type="1"><li>Limitations:</li></ol><ul><li>GNN is inefficient to update the hidden states of nodes iteratively for the fixed point.</li><li>There are also some informative features on the edges which cannot be effectively modeled in the original GNN.</li><li>It’s unsuitable to use the fixed points if we focus on the representation of nodes instead of graphs because the distribution of representation in the fixed point will be much smooth in value and less informative for distinguishing each node.</li></ul><ol start="16" type="1"><li><strong>Variants of Graph Neural Networks</strong> to release the limitations:</li></ol><ul><li>different graph types:<ul><li>Directed Graphs. Directed edges can bring more information than undirected edges.</li><li>Heterogeneous Graphs.<ul><li>The simplest way to process heterogeneous graph is to convert the type of each node to a one-hot feature vector which is concatenated with the original feature.</li><li>For each neighbor group, GraphInception treats it as a sub-graph in a homogeneous graph to do propagation and concatenates the propagation results from different homogeneous graphs to do a collective node representation. (heterogeneous graph attention network, <strong>HAN</strong>)</li><li>[https://zhuanlan.zhihu.com/p/47040007]</li></ul></li><li>Graphs with Edge Information.<ul><li>Converting the graph to a bipartite graph where the original edges also become nodes and one original edge is split into two new edges which means there are two new edges between the edge node and begin/end nodes. (The encoder of <strong>G2S</strong> uses the following aggregation function for neighbors).</li><li>Adapting different weight matrices for the propagation on different kinds of edges. When the number of relations is very large, r-GCN introduces two kinds of regularization to reduce the number of parameters for modeling amounts of relations: basis and block diagonal-decomposition</li></ul></li><li>Dynamic Graphs. It has static graph structure and dynamic input signals.</li></ul></li><li>several modifications:<ul><li><strong>Graph Convolutional Network (GCN)</strong>: convolutions to the graph domain<ul><li>spectral approaches,使用谱分解的方法，应用图的拉普拉斯矩阵分解进行节点的信息收集<br></li><li>non-spectral (spatial) approaches,直接使用图的拓扑结构，根据图的邻居信息进行信息收集</li></ul></li><li><strong>Gated graph neural network (GGNN)</strong>: using the gate mechanism like GRU or LSTM in the propagation step to diminish the restrictions in the former GNN models and improve the long-term propagation of information across the graph structure. Tree LSTM、Graph LSTM and Sentence LSTM</li><li><strong>Graph Attention Network (GAT)</strong> : incorporates the attention mechanism into the propagation step.<ul><li>GAT computes the hidden states of each node by attending over its neighbors, following a self-attention strategy.<br></li><li>Gated Attention Network (GAAN) also uses the multi-head attention mechanism. However, it uses a self-attention mechanism to gather information from different heads to replace the average operation of GAT.</li></ul></li><li><strong>Residual connection</strong>: aiming to achieve better results as more layers make each node aggregate more information from neighbors, because more layers could also propagate the noisy information from an exponentially increasing number of expanded neighborhood members.<ul><li>Highway GCN</li><li>Jump Knowledge Network, selects from all of the intermediate representations (which ”jump” to the last layer) for each node at the last layer, which makes the model adapt the effective neighborhood size for each node as needed. uses three approaches of concatenation, max-pooling and LSTM-attention in the experiments to aggregate information. (The Jump Knowledge Network performs well on the experiments in social, bioinformatics and citation networks. It could also be combined with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks to improve their performance.)</li></ul></li><li><strong>Hierarchical Pooling</strong>, Complicated and large-scale graphs usually carry rich hierarchical structures which are of great importance for node-level and graph-level classification tasks</li></ul></li></ul><ol start="17" type="1"><li>Training Method:</li></ol><ul><li>Sampling<ul><li>GraphSAGE replaced full graph Laplacian in GCN with learnable aggregation functions, which are key to perform message passing and generalize to unseen nodes. With learned aggregation and propagation functions, GraphSAGE could generate embeddings for unseen nodes.</li><li>PinSage, By simulating random walks starting from target nodes, this approach chooses the top T nodes with the highest normalized visit counts.</li><li>FastGCN, Instead of sampling neighbors for each node, FastGCN directly samples the receptive field for each layer.<br></li><li>adaptive sampler could find optimal sampling importance and reduce variance simultaneously</li></ul></li><li>Receptive Field Control, a control-variate based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate.</li><li>Data Augmentation, To solve the limitations, the authors proposed Co-Training GCN and Self-Training GCN to enlarge the training dataset.</li><li>Unsupervised Training, Graph auto-encoders (GAE) aim at representing nodes into low-dimensional vectors by an unsupervised training manner.</li></ul><ol start="18" type="1"><li>Frameworks:</li></ol><ul><li><strong>Message Passing Neural Networks (MPNN)</strong> unified GNN &amp; GCN. It abstracts the commonalities between several of the most popular models for graph-structured data.<ul><li>message passing</li><li>Readout computes a feature vector for the whole graph</li></ul></li><li><strong>Non-local Neural Networks (NLNN)</strong> unified several self-attention for capturing long-range dependencies with deep neural networks. It computes the response at a position as a weighted sum of the features at all positions. List the choices for f function:<ul><li>Gaussian (natural choice)</li><li>Embedded Gaussian</li><li>Dot product</li><li>Concatenation.</li></ul></li><li><span style="border-bottom:2px dashed red;"><strong>Graph Networks (GN) </strong>unified MPNN &amp; NLNN and so on.</span><ul><li>Graph definition</li><li>GN block contains<ul><li>three “update” functions, φ_e, φ_h &amp; φ_u,</li><li>three “aggregation” functions, ρ ( The ρ functions must be invariant to permutations of their inputs and should take variable numbers of arguments) <img src="https://github.com/soloistben/images/raw/master/gnn_image/3.png" alt="func"></li></ul></li><li>Computation steps <img src="https://github.com/soloistben/images/raw/master/gnn_image/4.png" alt="steps"></li><li>Design Principles<ul><li><strong>Flexible representations</strong><ul><li>One can simply tailor the output of a GN block according to specific demands of tasks</li><li>be applied to both structural scenarios where the graph structure is explicit and non structural scenarios where the relational structure should be inferred or assumed.</li></ul></li><li><strong>Configurable within-block structure</strong>. Based on different structure and functions settings, a variety of models (such as MPNN, NLNN and other variants) could be expressed by the GN framework.</li><li><strong>Composable multi-block architectures.</strong><ul><li>Arbitrary numbers of GN blocks could be composed in sequence with shared or unshared parameters.</li><li>utilizes GN blocks to construct an encode process decode architecture and a recurrent GN-based architecture.</li><li>Other techniques for building GN based architectures could also be useful, such as skip connections, LSTM- or GRU-style gating schemes and so on.</li></ul></li></ul></li></ul></li></ul><ol start="19" type="1"><li>Applications of GNN:</li></ol><ul><li>supervised, semi-supervised, unsupervised and reinforcement learning</li><li>Structural Scenarios<ul><li>Physics</li><li>Chemistry and Biology</li><li>Knowledge graph</li></ul></li><li>Non-Structural Scenarios<ul><li>Image<ul><li>Visual Reasoning</li><li>Semantic Segmentation</li></ul></li><li>Text<ul><li>Text classification</li><li>Sequence labeling</li><li>Neural machine translation</li><li>Relation extraction</li><li>Event extraction</li><li>Other applications</li></ul></li></ul></li><li>Other Scenarios<ul><li>Generative Models</li><li>Combinatorial Optimization</li></ul></li></ul><ol start="20" type="1"><li>Problems</li></ol><ul><li>Shallow Structure</li><li>Dynamic Graphs</li><li>Non-Structural Scenarios</li><li>Scalability</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;note-of-graph-neural-networks-a-review-of-methods-and-applications&quot;&gt;NOTE of Graph Neural Networks: A Review of Methods and Applicati
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>DL_RNN</title>
    <link href="http://yoursite.com/2019/07/29/DL-RNN/"/>
    <id>http://yoursite.com/2019/07/29/DL-RNN/</id>
    <published>2019-07-29T01:22:01.000Z</published>
    <updated>2019-07-29T06:48:00.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="sequence-model-序列模型包括rnn">Sequence Model 序列模型（包括RNN）：</h4><p>1、<strong>Speech Recognition 语音识别</strong>：输入一段语音，输出英文句子（输入输出都属于Sequence Data序列数据） 2、<strong>Music generation 音乐生成</strong>：输入可以是空集（可以不输入，可以是数字、音乐风格），输出是音符（属于序列） 3、<strong>Sentiment classification 情感分类</strong>：输入是评语/一段话（属于序列），输出是衡量情感的标记 4、<strong>DNA Squence analysis DNA序列分析</strong>：输入给定的DNA序列，输出在DNA上标记匹配某种蛋白质的 5、<strong>Machine Transaction 机器翻译</strong>：输入英语一句话，输出中文的翻译 6、<strong>Video Activity Recognition 视频动作识别</strong>：输入以帧单位的视频，输出描述语句 7、<strong>Name Entity Recognition 命名实体识别</strong>：输入一句话，输出在句子中识别到的名字（常用于搜索引擎识别特殊名词） 8、在序列模型训练中，有输入输出都是序列数据（可能等长，可能类型不同），也有仅输入或输出才是序列模型， 9、命名实体识别在输入句子中，以每个单词为单位，<span style="border-bottom:2px dashed red;">输出一维向量，匹配位置，是名字的标记1，否则标记0（用X&lt;t&gt;来表示单词所在时序序列，Tx表示输入的序列长度，Ty表示输出的序列长度）</span>（这个例子也属于<strong>NLP（Natural Language Processing自然语言处理）</strong>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/17.png" alt="image"> 10、NLP中，首要解决的是怎样表示序列里单独的单词，然后制作一个单词表/字典（在商用中字典单词经常会有3w到5w，甚至10w、100w+） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/18.png" alt="image"> 11、在输入句子序列中，<span style="border-bottom:2px dashed red;">字典个数=每个向量的维度</span>（用one-hot的方式标记向量，用1标出在字典的位置，其他为0，一维向量）<span style="border-bottom:2px dashed red;">（若是遇到不再字典的单词，就创建一个叫做Unknown Word的伪造单词，来表示不再字典的单词）</span> <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/19.png" alt="image"></p><h4 id="recurrent-neural-network-rnn">Recurrent Neural Network RNN:</h4><p>1、若是用传统的神经网络，输入x则全是one-hot的向量，而且维度会很大，输出也相同；缺点是在不同例子中可以有不同长度，Tx与Ty不一定相等，并且<span style="border-bottom:2px dashed red;">从文本位置上学到的特征并没有共享使用（若是一个人名首次被识别人名之后，在第二处，直接就可以识别成人名</span>，这才是想要的结果（计算机视觉，就是学习了小部分特征，立马推广到图片的其他地方）） 2、<span style="border-bottom:2px dashed red;">RNN：由左到右，每次执行都添加如激活值a[i]（第一个a[0]一般初始化为0，作为伪激活），每一层都会结合上一层所得到的激活值（代表前面所有层的信息）一起预测出y，则每一层都有了联系，参数得到共享</span>（缺点是只得到前面层的信息，得不到后面层的信息，因为有时仅靠前面信息无法推定该词回事人名的单词，会有<strong>Bidirectional Recurrent Neural Network双向循环神经网络（BRNN）</strong>解决该问题） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/20.png" alt="image"> 3、 <strong>a&lt;t&gt; = g(Waa*a&lt;t-1&gt;+Wax*X&lt;t&gt;+ba); y&lt;t&gt; = g(Wya*a&lt;t&gt;+by)</strong>（W下标，谁在前面就是求谁，在后就要乘以谁）（常用激活函数是tanh/ReLU，通常是<span style="border-bottom:2px dashed red;">tanh</span>，有其他方式可以避免梯度消失问题）（输出部分的激活函数，若是二分问题，则用sigmoid函数） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/21.png" alt="image"> 4、 设Wa=[Waa Wax]，若Waa维度是(100,100)，Wax是(100,10000)，则Wa是(100,10100)，简化<strong>a&lt;t&gt; = g(Wa[a&lt;t-1&gt;, X&lt;t&gt;]+ba)</strong>，[a&lt;t-1&gt;, X&lt;t&gt;]维度是(10100,100)是a在上部分，X在下部分，结果a&lt;t&gt;维度是(100,100)；简化<strong>y&lt;t&gt; = g(Wy*a&lt;t&gt;+by)</strong>仅有一个下标表示输出什么类型的量 5、<strong>RNN反向传播backpropagation through time</strong>：基本与RNN的方向相反（前向传播顺着时间走，反向传播逆着时间走）；<span style="border-bottom:2px dashed red;">某个单词预测的损失函数使用交叉熵损失函数，整个损失值单个单词损失值之和</span> <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/22.png" alt="image"></p><h4 id="不同基本模块的rnn">不同基本模块的RNN：</h4><p>1、Tx=Ty的RNN架构属于多对多类型 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/23.png" alt="image"> 2、在情感分析的RNN中，输入是一句评语，输出判定正/负面评价（0/1二分类问题），多对一类型则在设计神经网络时，不必像多比多那样每次循环都要输出，仅在最后在输出0/1即可 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/24.png" alt="image"> 3、在音乐生成的RNN中，输入是空集也可以，输出是一段音乐（序列），一对多类型在设计神经网络时，仅在第一次循环输入，后续循环的输入是上次循环的结果y <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/25.png" alt="image"> 4、在多对多类型中还有Tx≠Ty的情况，机器翻译就属于其一（每种语言文字单词不全是一对一），则在设计神经网络时，(encode)就先循环输入x，(decode)输入完毕在循环输出y <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/26.png" alt="image"> 5、一对一结构则是一种标准类型的神经网络，不需要RNN也可以</p><h4 id="language-model-语言模型">Language Model 语言模型：</h4><p>1、<span style="border-bottom:2px dashed red;">语言模型：输入句子（文本序列 y），进行估计句子中各个单词出现的可能性/概率</span>（学习文字描述风格，在运用风格写文章）（给出部分单词，预测下一个单词） 2、训练集要包含 large cropus of English text 很大的英文文本语料库（cropus语料库是自然语言处理中的专业名词，意思是很多很长、用英文句子、组成的文本） 3、整个流程： + 预处理：输入一句话序列(cats average 15 hours of sleep a day)；<strong>tokenize标记化</strong>，将句子中单词转成one-hot向量（字典中的索引），<span style="border-bottom:2px dashed red;">增加一个&lt;EOS&gt;作为句子的结尾</span>，被标记为末尾y&lt;Ty&gt;（若将句号看出标志，其他符号也需要看成标志）；在出现不在字典中的单词时，将其标记问<strong>UNK(Unknown Word)</strong> + 设计RNN序列概率模型，<span style="border-bottom:2px dashed red;">初始化激活值a&lt;0&gt;和输入值x&lt;1&gt;为零向量，计算出a&lt;1&gt;，再softmax预测出y&lt;1&gt;概率，得出第一个单词的概率；再第二个循环中x&lt;2&gt;=y&lt;1&gt;，重复操作</span>（<strong>通过训练，预测出下一个单词，也就是学习了造句风格，在测试中可以遇到训练过的单词，会更多概率选中与风格相匹配的单词</strong>）；第一层算出概率P(y&lt;1&gt;)，第二层在第一层基础上算出概率P(y&lt;2&gt;|y&lt;1&gt;)，第三层则是P(y&lt;3&gt;|y&lt;1&gt;,y&lt;2&gt;)...最后<strong>P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;)=P(y&lt;1&gt;)P(y&lt;2&gt;|y&lt;1&gt;)P(y&lt;3&gt;|y&lt;1&gt;,y&lt;2&gt;)</strong> <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/27.png" alt="image"> + <strong>Sampling novel sequences对新序列进行采样</strong>：在训练序列模型之后，一个非正式的方法（对序列采样）可以得知这个模型学习了什么；<span style="border-bottom:2px dashed red;">对训练得到的概率分布P(y&lt;1&gt;,y&lt;2&gt;,...,y&lt;Tx&gt;)进行采样，来生成一个新的单词序列；首先初始化激活值a&lt;0&gt;和输入值x&lt;1&gt;为零向量，第一层softmax预测到的时所有单词作为第一个单词概率分布，然后进行随机采样（用np.random.choice），将第一层得到的y&lt;1&gt;输出到第二层，若第一个单词选择了“the”，则第二层计算出在“the”情况下，下一个单词概率，然后再次随机采样，一直循环、采样到结束</span>（有可能会出现UNK，所以在采用时拒绝出现未录入字典的单词，直到不是UNK的单词） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/28.png" alt="image"></p><p>4、<strong>Character-level language model 基于字符的语言模型</strong>：将每个单词字母全部拆分训练，优点是不会出现UNK，缺点是会得到太多太长的序列（一般一个句子就10~20个单词）不利于<strong>捕捉句子间的依赖关系</strong>（句子较前部分预测句子较后部分），并且计算成本高 5、NLP趋势是使用<strong>word level language model基于词汇的语言模型</strong>，（在计算能力好的情况下，处理大量未知文本或者专业词汇，使用基于字符的语言模型会更好）</p><h4 id="梯度消失">梯度消失：</h4><p>1、RNN常用tanh激活函数，因而存在有梯度消失问题，影响反向传播 2、基础的RNN仍是<span style="border-bottom:2px dashed red;">不擅长于捕捉长期依赖效应</span>（主语与谓语相隔较远时，时态和单双数的变化，类似的长期依赖），这需要在RNN对主语的<strong>长期记忆</strong>，才能在后面对谓语有联系 3、基础的RNN模型会有很多局部影响，<span style="border-bottom:2px dashed red;">比如y&lt;3&gt;就会受到其附近的值影响，在后面的值很难受到前面的影响，这个区域都很难反向传播到序列前面部分，因此神经网络很难调整前面的计算（缺点）</span> 4、RNN也会出现梯度爆炸，相对容易发现，会直接让神经网络崩溃，参数数值会很大，甚至NaN，溢出；解决方法：<strong>gradient clipping梯度修剪</strong>（观察梯度向量，若其大于某个threshold阈值，缩放梯度向量，保证它不会太大，这就是最大值修剪）</p><h4 id="gated-recurrent-unit-gru门控循环单元">Gated Recurrent Unit GRU门控循环单元：</h4><p>1、GRU能让RNN在深层网络中捕捉依赖关系，解决梯度消失问题 2、RNN中隐藏单元的可视化 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/29.png" alt="image"> 3、GRU加入新变量c表示memory cell细胞，记忆细胞（在后面动词看到c时会知道主语是单数复数）：在时间t处，c&lt;t&gt;=a&lt;t&gt;（但在LSTMs他俩不相等）<strong>用c~&lt;t&gt; = tanh(Wc[c&lt;t-1&gt;,x&lt;t&gt;+bc)代替了c&lt;t&gt;</strong> 4、GRU中心思想时，有个门<strong>Γu = sigmoid((Wu[c&lt;t-1&gt;,x&lt;t&gt;+bu)</strong>（Γ 范围是0到1，u代表更新gate，sigmoid的输出范围是[0,1]） 5、用c~更新c的等式，然后<span style="border-bottom:2px dashed red;">Γ 决定是否要更新它</span>（假设c&lt;t&gt;=1时表示单数，0表示复数，经过中间隔着很久，到达动词，c&lt;t&gt;仍是等于1，则动词显示单数；Γu 就决定什么时候去更新c&lt;t&gt;这个值）<strong>c&lt;t&gt; = Γu*c~&lt;t&gt; + (1-Γu)*c&lt;t&gt;</strong>（元素对应乘积）（<span style="border-bottom:2px dashed red;">当Γu=1时，c&lt;t&gt;=1，对于主谓语之间的语句 Γu=0，也就是c&lt;t&gt;=c&lt;t-1&gt;，不用进行更新</span>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/30.png" alt="image"> 6、因为sigmoid的值，Γu很容易取到0，则c&lt;t&gt;一直维持值不变，因为Γ很接近0，可能是0.000001，c&lt;t&gt;不变，a&lt;t&gt;值也维持不变，这就不会出现梯度消失了 7、c&lt;t&gt;可以是个向量，若有100个隐含依赖，则可以是100维，Γ也是相同维度，就是100bit的向量（里面几乎全是0或者1）（Γ不会确切的等于0或者1，会是0到1的中间值，为了理解可以当成0和1） 8、完整的GRU公式（会多一个Γr相关门）（GRU和LSTM都十分常用） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/31.png" alt="image"></p><h4 id="long-short-term-memory-lstm长短期记忆网络">Long Short-Term Memory LSTM长短期记忆网络：</h4><p>1、比GRU更加有效，是比GRU更通用更强 2、在LSTM中得公式， + c~&lt;t&gt; = tanh(Wc[a&lt;t-1&gt;,x&lt;t&gt;+bc) + Γu = sigmoid((Wu[a&lt;t-1&gt;,x&lt;t&gt;+bu) update gate + Γf = sigmoid((Wf[a&lt;t-1&gt;,x&lt;t&gt;+bf) forget gate + Γo = sigmoid((Wo[a&lt;t-1&gt;,x&lt;t&gt;+bo) output gate + c&lt;t&gt; = Γu*c~&lt;t&gt; + Γf*c&lt;t-1&gt;（Γf代替1-Γu） + a&lt;t&gt; = Γo*tanh(c&lt;t&gt;) <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/32.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/33.png" alt="image"></p><p>3、设置了forget gate和update gate，则LSTM很容易把c&lt;0&gt;一直传递下去 4、在三种gate的计算下，只需要a&lt;t-1&gt;和x&lt;t&gt;，也可以加入c&lt;t-1&gt;，这称为<strong>peephole connection窥视孔连接</strong>（则gate的值也由记忆细胞的值一同决定） 5、GRU实在LSTM中做出的简化，GRU设计更简单，只有两个gate，容易创建更大规模更深的网络</p><h4 id="bidirectionnal-rnn-双向rnn">Bidirectionnal RNN 双向RNN：</h4><p>1、因为单向的 RNN中的单个单词只被其前面的单词所影响，但很多情况影响其的意义来源于后面的内容，因此需要双向RNN 2、BRNN，也是一个Acyclic graph无环图，正反向独立运行，（正反向前向传播均属于是前向传播）（可以由前后内容一同判断一个单词）缺点是，<span style="border-bottom:2px dashed red;">需要完整的序列才能预测任意位置，例如做语音识别，需要等这个人把话说完才能进行识别</span> <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/34.png" alt="image"> 3、<strong>y&lt;t&gt; = g(Wy[a→&lt;t&gt;,a←&lt;t&gt;]+by)</strong></p><h4 id="deep-rnns">Deep RNNs:</h4><p>1、在标准的RNN上，垂直方向多加三趟循环，加上水平时间线上输入x<tx>，已经是很深层了（与卷积100层不一样），会有很多隐含层，计算量十分大 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/35.png" alt="image"> 2、a[2]&lt;3&gt; = g(Wa[2][a[2]&lt;2&gt;, a[1]&lt;3&gt;]+ba[2]) 3、第1层有Wa[1]、ba[1]；第2层有Wa[2]、ba[2]；第3层有Wa[3]、ba[3]</tx></p><h4 id="word-representation-词汇表征">Word Representation 词汇表征：</h4><p>1、<strong>Embedding 词嵌入</strong>：语言表示的一种方式，<span style="border-bottom:2px dashed red;">可以让算法自动的理解一些类似的词</span>（比如 男人女人、国王王后）（意思是将众多的单词按特征归类） 2、通过词嵌入可以构建NLP应用，即使模型的训练集相对小也可以（需要通过一些方法消除词嵌入的偏差） 3、用one-hot向量表示单词在字典的位置，会把单词都独立起来，这样泛化能力不强（<span style="border-bottom:2px dashed red;">可以通过300个特征来联系这些单词，但是实际上联系效果还是不够</span>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/36.png" alt="image"> 4、<strong>特征化</strong>：可以将300维的特征向量嵌入到二维空间，即可可视化了（常用可视化算法是t-SNE算法，非线性）；可以更直观发现特征相似的词都聚集在一起 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/37.png" alt="image"> 5、<strong>命名实体识别词嵌入的迁移学习</strong>：在大量无标签文本中学习大量词嵌入（也可以下载已训练好的模型，则可以将自己少量的训练集迁移学习，除非训练集很大，否则不需要再次调整词嵌入，直接使用就好） 6、词嵌入的运用十分广泛（<strong>named entity recognition命名实体识别</strong>、<strong>text summarization文本摘要</strong>、<strong>co-reference resolution 文本解析</strong>、<strong>parsing指代消解</strong>）；在语言模型之类的训练，则较少用到（因为会有大量数据） 7、词嵌入特征是能帮助实现<strong>analogy reasoning类比推理</strong>（虽然没有在NLP中着重运用，但能让人看到词嵌入干了什么，能干什么）；当男人能推出女人时，国王也能推出女王，得出他们差别都在性别（<strong>Eman-Ewomen ≈ Eking-Equeen</strong>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/38.png" alt="image"> 8、求<strong>Ew</strong>(Equeen)：<strong>argmax sim(Ew, Eking-Eman+Ewomen)</strong> 相似最大化；通过方程找导最理想结果（准确率一般只有30%~70%）（常用的similarity function相似度函数时<strong>cosine similarity余弦相似度 sim(u,v)=(u^T * v)/(||u|| ||v||)，就是求u、v的俩向量的夹角Φ余弦值</strong>（cos函数中，Φ=0时，cos值是1；Φ=2PI 时，cos值是-1；就是两者越是相似，角度Φ越小，越接近1，越大则不相似则会接近-1））（<span style="border-bottom:2px dashed red;">相同关系的推断：线性，同特征的推断会使向量趋向平行</span>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/39.png" alt="image"> 9、还可以用<strong>平方距离/欧式距离||u-v||^2</strong>求差异表示相似度 10、学习词嵌入就是学习一个嵌入矩阵，嵌入矩阵E，维度(300,1000)，与one-hot向量o，维度(1000,1)；<strong>E·o=e</strong>（o向量在E中的<strong>嵌入向量e</strong>(300,1)）（但不常用矩阵乘积的方法找e，计算量太大，矩阵乘积慢，会有单独函数方法来直接找到E中的哪一列） 11、词嵌入的NLP过程，求得每个嵌入向量e，最终汇聚于全连接层和softmax输出，做出多个单词的概率计算，最高概率则是预测单词（也可以仅看前4个单词即可推出，适当减少参数；或者仅提供其前面的/附近的一个单词也可） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/40.png" alt="image"> 12、<strong>Word2Vec算法的skip-gram模型</strong>：（用于学习词嵌入更高效）抽取上下文单词于目标词匹配，来构造一个监督学习问题（Content c “orange” -&gt; Target t “juice”）（<strong>o_c -&gt; E -&gt; e_c -&gt; softmax -&gt; y<sup><strong>）（softmax：</strong>p(t|c)=(e</sup>(θt^T * e_c))/(Σj e<sup>(θj</sup>T·e_c)</strong>)（θt是一个输出t的参数））（softmax损失函数：<strong>L(y^,y)=-Σi yi log y^</strong>） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/41.png" alt="image"> 13、关键的p(t|c)中分母运算太大使得操作变慢；解决方法：使用hierarchical分级的softmax分类器（类似折中查找一样，每次二分为一查找，提高速度（也类似哈夫曼树，使用频率高的词靠近root）） 14、<strong>Negative Sample 负采样</strong>：（与skip-gram模型做的相类似，但效率会更好）构造新的监督学习，给定一对单词，去预测是否是一对content-target的组合（<strong>context：orange-juice 输出target 1属于正样本；context：orange-king 输出target 0属于负样本</strong>）（k次随机在字典中寻找词与orange匹配得出负样本，提供作为训练集）（数据集小的话k一般取5<sub>20；数据集大k选2</sub>5即可）；可以使用<strong>logistic回归模型</strong>，p(y=1|c,t)=sigmoid(θt^T·e_c)；在整个训练中，训练一个正样本、随机k个负样本，就不需要每次在softmax中分母运算高纬度求和 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/42.png" alt="image"> 15、在负采样中负样本的word如何选？推出该公式用于选取负样本的word（对词频的3/4次方，再求在总和的比例） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/43.png" alt="image"> 16、（难）<strong>GloVe word vector 词向量算法</strong>：用词表示全局变量，明确context-target的关系Xij（i、jb表示两个词，Xij=Xji（若i、j相邻，则不符合这个对称等式）可将i、j定义为两个位置相近的单词，假设左右各10词的距离）；Xij可获取单词i、j出现位置相近时或者彼此接近时的频率的计数器 17、（难）<strong>GloVe's Model</strong>：最小化它们的差值 <strong>minimize=ΣiΣj f(Xij)((θi<sup>T·ej+bi+bj-logXij)</sup>2</strong>；表示两个单词关系多紧密（防止Xij=0，导致无穷大，需添加一个weighting term加权项f(Xij)项，当Xij=0，f(Xij)=0）；<span style="border-bottom:2px dashed red;">不给频繁词过大的权重，不给少用词太小的权重</span>；θ和e是对称的，将其颠倒或排序，都可以输出最佳结果；训练方法是将他们一致地初始化，然后梯度下降来最小化输出，当每个词处理完之后，取平均值（e(find)=(e_w+θ_w)/2） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/44.png" alt="image"> 18、<strong>Sentiment classification情感分析</strong>：分细一段评语，判断是否喜欢讨论的东西；这个任务的标记训练集可能没有那么多，可用词嵌入；可用于收集消费者的评价 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/45.png" alt="image"> 19、训练嵌入矩阵E会很大维度（若在很大训练集上训练E，则会学到很多知识）；在特征向量求得需要平均化在送入softmax； <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/46.png" alt="image"> 20、只把词的特征向量，加起来，可能就理解不了“反话”，则需要RNN <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/47.png" alt="image"> 21、<strong>debiasing word embedding 词嵌入消除偏见</strong>：在使用词嵌入时需要减少或者消除词嵌入（一个完成词嵌入可能会输出Man:Computer_Programmer as Woman:Homemake / father:doctor as mother:nurse这种性别歧视的话语）；<span style="border-bottom:2px dashed red;">根据训练模型使用的文本，词嵌入能反映出性别、种族、年龄、性取向等的这些偏见</span>（这些偏见都跟社会经济状态相关，机器学习作出决策时不应该存在偏见） 22、步骤：1）辨别出想要减少的或者消除的特定偏见趋势（将(e_he - e_she)和(e_male - e_female)这类的男女性别的差值求和再简单取平均值）；2）neutralize step中和步，对那些定义不确切的词，可以将其处理下，避免偏见；3）Equalize pairs 均衡步，类似girl和boy能够有一致的相似度（距离中立词语能有相等距离） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/48.png" alt="image"></p><h4 id="basic-models-基础模型">Basic Models 基础模型：</h4><p>1、<strong>seq2seq的机器翻译（法语转英语）</strong>：将法语句子序列转成输入向量x，英语句子为y； <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/49.png" alt="image"> 输入需要先经过一个<strong>encoder network编码网络</strong>（属于RNN，隐含单元可以是GRU/LSTM）（<span style="border-bottom:2px dashed red;">输入法语，在输出一个能代替法语的 向量，更好进行训练</span>）；在后面可以建立一个<strong>decoder network解码网络</strong>（输出英语，直到输出结束标志）（这一部分RNN与语言模型的RNN基本一致） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/50.png" alt="image"> 2、<strong>Image captioning图像描述</strong>（用一句话描述图片）：在卷积神经网络（AlexNet）中先识别图片（可以算作图像的编码网络（得到一个4096的一维向量）），然后用RNN生成图片的描述（作为解码网络） 3、可以将机器翻译称为有条件的语言模型<strong>P(y&lt;1&gt;,y&lt;2&gt;,...,y&lt;Ty&gt;|x)</strong>；<span style="border-bottom:2px dashed red;">在机器翻译可能会翻译出多条句子，虽然没错但不是最好的翻译</span>（最好的翻译结果句子的P(y|x)是最高的），则需要一种算法将找到合适的y值，再将其最大化（常用算法为<strong>Beam Search束搜索</strong>）（Greedy search贪心搜索算法不可用，最常用的搭配不见得是最好的翻译结果，并且每个词都需要遍历所有去寻找最好的，计算量是字典总数的指数级别） 4、<strong>Beam Search集束搜索</strong>： + 首先在解码网络中，需要在字典10000的单词中，挑选出输出的第一个单词P(y&lt;1&gt;|x)，会考虑选多个的单词（<span style="border-bottom:2px dashed red;">参数B，称为beam width集束宽，B=3则一次会考虑三个单词</span>）将所挑选的单词概率存入计算机内存里； + 在所选的三个单词，分别再其基础上选择第二个单词P(y&lt;2&gt;|x,y&lt;1&gt;)，<span style="border-bottom:2px dashed red;">但最主要是找到最大概率<strong>P(y&lt;1&gt;,y&lt;2&gt;|x) = P(y&lt;1&gt;|x) * P(y&lt;2&gt;|x,y&lt;1&gt;)</strong></span>；第二步就有30000种可能了，然后在这30000可能中找出3个最高的概率的y&lt;1&gt;,y&lt;2&gt;继续下个30000的概率选择；主要乘积概率公式如下： <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/51.png" alt="image"></p><ul><li>重复类似第二步的操作，每一步增加一个单词，直至识别结束符号；当B=1时，就变成了贪婪搜索</li></ul><p>5、<strong>Length normalization 长度归一化</strong>：对集束搜索的稍微调整；因为通过beam search公式所求概率通常小于1，多次小于1的概率乘积，会变得很小很小，导致数值下溢，计算机精度不够无法精准存储；<span style="border-bottom:2px dashed red;">改造公式为对数的求和，更加稳定，不容易出现四舍五入的误差避，免数值下溢（对数函数单调递增，所以最大化logP(y|x)与最大化P(y|x)是一回事）</span>； 6、（因为每次都会乘以小于1的概率，所以也会偏向更少数量的输出）<span style="border-bottom:2px dashed red;">因为最短也不一定是最好的输出，所以不再最大化概率了，改为归一化，求概率对数的平均值</span>；在取平均数时，如何选择分母才会更好，在分母设置参数α，Ty<sup>a作为分母会更柔和一点，α可以设置为0.7；α=0则等于没有归一化，α=1则等于完全用长度来归一化；用调整后的α会得到更好的结果 7、对于B的选择，一般越多还是越好，但过多计算成本就会变高；B很大的情况：会得到更好的结果，计算会慢一些，占用内存大；B很小的情况：结果不会很好，但计算快，占用内存少；<span style="border-bottom:2px dashed red;">一般在实际使用会设置B=10</span>；设置为100就太大了，但在科研需要更好的效果则会设置成1000或者3000 8、运用广度优先搜索或者深度优先搜索可能会比集束搜索速度更快，但不能保证一定找到最大化的精准的最大值 9、误差分析可以节省时间，用更多时间投入更有用的工作 10、集束搜索算法是一种<strong>approximate search近似搜索算法/heuristic search启发式搜索算法</strong>，不直接输出最好的句子，而是记录多个好句子，再输出最好的 11、人工翻译的句子记为y*、RNN输出结果记为y</sup>；人工翻译比RNN输出要好的前提下，对比P(y*|x)和P(y^|x)（忽略归一化复杂情况下） + case1：P(y*|x)&gt;P(y^|x)，则beam search出问题了（加大B），没有找到更大的y^ + case2：P(y*|x)&lt;=P(y<sup>|x)，则是RNN出问题了（加深网络），y*翻译试比y</sup>好的，但RNN算出P(y*|x)&lt;=P(y^|x)</p><p>12、持续多次句子翻译的对比P(y*|x)和P(y^|x)，出现最多错的就是最主要误差问题</p><h4 id="bleu-score">Bleu score：</h4><p>1、用于代替人类去评估机器翻译结果，衡量准确性（给定一个翻译结果，计算出一个分数，接近人类的翻译就高分）；衡量机器翻译质量的方法之一是<span style="border-bottom:2px dashed red;">观察输出结果的每一个词，看其是否出现在人工翻译参考当中</span>（精确度）（但是出现个别情况，输出所有单词一样，并且其都在人工翻译中出现，会出现精度很好，但翻译出的句子不行）；改良后的方法：<span style="border-bottom:2px dashed red;">记录每一个词在出现在人工翻译中的次数，在多个人工翻译句子里面出现最多次数视为上限，上限为分子，在机器翻译的结果中出现的次数为分母</span>；<strong>体现出翻译结果与人工翻译的重叠层度</strong>（若是与人工翻译一致，则P=1.0）（可以下载已有的模型） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/52.png" alt="image"> 2、<strong>Bleu score on brigrams（二元组）（相邻两个单词）</strong>：（unigrams一元组）（也会有trigrams三元组） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/53.png" alt="image"> 3、一元组精度公式：<strong>P1=Σ</strong>unigram∈y^ * <strong>Count</strong>clip<strong>(unigram) / Σ</strong>unigram∈y^ * <strong>Count(unigram)</strong> 4、n元组精度公式：<strong>Pn=Σ</strong>n-gram∈y^ * <strong>Count</strong>clip<strong>(n-gram) / Σ</strong>n-gram∈y^ * <strong>Count(n-gram)</strong> 5、结合Bleu score，采用BP（<strong>brevity penalty简短惩罚</strong>）的惩罚因子作为调整因子，<strong>BPexp(1/4 Σ4 Pn)</strong>（对P1、P2、P3、P4取均值）；<span style="border-bottom:2px dashed red;">若是输出一个很短的翻译则会得到很高的精确度，但简短的输出也不一定是最好</span>；机器翻译长度大于人工翻译，BP=1 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/54.png" alt="image"></p><h4 id="难attention-model-注意力模型">（难）Attention Model 注意力模型：</h4><p>1、在翻译一段文本中，人类大部分是翻译一部分，再翻译下一部分，而不是看完全部，再靠记忆去由零开始翻译 2、Bleu score在短文翻译和长达30字以上的文本翻译都会比较难，所以分数较低 3、注意力模型就会让其变得更像人类一样翻译文本，在长文本中表现良好 4、<span style="border-bottom:2px dashed red;">利用BRNN，但每层的输出并不会直接输出翻译结果，而是输出<strong>α&lt;t,t'&gt;“注意力权重”</strong>用于接入新的RNN的<strong>S&lt;t&gt;“注意力权重集”</strong>（注意力权重则是一个注意力权重集对BRNN这层的所需要的注意力），S&lt;t&gt;的输入会是对应单词和其上下文单词（多个输入），再逐个输出翻译的单词</span> <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/55.png" alt="image"> 5、上下文的定义是<span style="border-bottom:2px dashed red;">被注意力权重除权的不同步时间中的特征值</span>；BRNN多个输出α&lt;t,t'&gt;相加得出<strong>c&lt;t&gt;</strong>，c&lt;t&gt;在统一输入注意力权重集（<strong>c&lt;t&gt;=Σα&lt;t,t'&gt;*a&lt;t'&gt;</strong>）（<strong>Σα&lt;t,t'&gt;=1</strong>）（用softmax来确保这些权重加起来等于1） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/56.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/57.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/58.png" alt="image"> 6、如果想要决定花多少注意力在t'的激活值上，很大程度取决于上一个时间步的s&lt;t-1&gt;的隐藏状态的激活值 7、缺点是算法复杂度是O(n^3)，总参数个数会是Tx*Ty，但是一部分一部分翻译的话，这个消耗还是很可接受 8、注意力模型还被应用于将任何形式的时间表达方法转化成标准时间显示方式 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/59.png" alt="image"> 9、注意力权重的可视化，需要更多注意力的色块会更亮 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/60.png" alt="image"></p><h4 id="speech-recognition-语音识别">Speech Recognition 语音识别：</h4><p>1、seq2seq在音频上的应用（输入音频，输出文本） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/61.png" alt="image"> 2、音频数据的常见预处理步骤：运行这个原始的音频片段，然后生成一个<strong>spectrogram声谱图</strong>（不同颜色表示声波能量的大小）；<strong>false blank output伪空白输出</strong>（模仿人耳）也常应用于预处理 3、曾经也有人工设计的<strong>phonemes音位</strong>作为基本单元（就是听发音判别单词） 4、采用<strong>CTC损失函数（connectionist temporal classification）</strong>来做语音识别；例如&quot;the quick brown fox&quot;一句话十秒，每秒100hz，则有1000层输入（模型较为简单，但很深）；虽然输入有1000，但输出是没有1000，则需要用到CTC，整合输出&quot;ttt__h_eee____ __qq__...&quot;类似的以赫兹输出音频（CTC：将空格之间的重复字符折叠起来成为一个单词） <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/62.png" alt="image"></p><h4 id="trigger-word-detect触发检字">Trigger word detect触发检字：</h4><p>1、类似&quot;ok，google！&quot;一样的唤醒语音系统，然后发出一条语音指令，再识别后执行某些事 2、设备一直检测着附近的音频，输入音频，提取出音频中的特征，将特征输入RNN，再没有识别到关键字的时候，输出0，检测到关键字时输出1；缺点是0太多了，导致训练集很不平衡；<span style="border-bottom:2px dashed red;">一个暴力解决方法（在输出1后，固定一定时间保持输出1，达到一定平衡）</span>，起码是训练集更加平衡些，更好于训练 <img src="https://github.com/soloistben/images/raw/master/deeplearning_image/63.png" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;sequence-model-序列模型包括rnn&quot;&gt;Sequence Model 序列模型（包括RNN）：&lt;/h4&gt;
&lt;p&gt;1、&lt;strong&gt;Speech Recognition 语音识别&lt;/strong&gt;：输入一段语音，输出英文句子（输入输出都属于Sequen
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Blockchain</title>
    <link href="http://yoursite.com/2019/07/19/Blockchain/"/>
    <id>http://yoursite.com/2019/07/19/Blockchain/</id>
    <published>2019-07-19T00:51:03.000Z</published>
    <updated>2019-11-14T12:46:46.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>比特币和以太坊均是区块链技术下一种<strong>crypto-currency加密货币</strong></li><li><strong>比特币：</strong>密码学基础、比特币的数据结构、共识协议和系统实现、挖矿算法和难度调整、比特币脚本、软分叉和硬分叉、匿名和隐私保护</li><li><strong>以太坊：</strong>基于账户的分布式账本、数据结构（状态树、交易树、收据树）、GHOST协议、挖矿（memory-hard mining puzzle）、挖矿难度调整、权益证明（Casper the Friendly Finality Gadget(FFG)）、智能合约</li></ul><h4 id="密码学基础">密码学基础：</h4><p>1、加密货币是不加密的 ，所有信息是公开的，交易金额与时间都是公开的 2、比特币中用到密码学的两大功能：<strong>哈希</strong>、<strong>签名</strong> 3、<strong>cryptographic hash function 哈希</strong>：有两大性质（<strong>collision resistance哈希碰撞</strong>）（<strong>hiding计算过程是单向的、不可逆的</strong>） 4、哈希碰撞：当有两个互不相等的输入x和y，而H(x)=H(y)），则称为哈希碰撞；其是不可避免地，但是客观存在；作用是在H(m)计算出来，很难几乎找不到有另外m'的H(m')=H(m)，则是<span style="border-bottom:2px dashed red;">没有办法再对m作出修改后，不被检测出来（找不到人为的哈希碰撞）</span> 5、hiding：计算过程无法逆推，<span style="border-bottom:2px dashed red;">则是哈希结果并不会泄露原本信息</span>（前提是输入也是足够大，输入分布要比较均匀，取值概率差不多，否则可以蛮力破解） 6、<strong>collision resistance + hiding = digital commitment = digital equivalent of a sealed envelope</strong> （sealed envelope表示现实生活中的公信机制） 7、在输入数量数值不够大时，<span style="border-bottom:2px dashed red;">需要随机数nonce</span>，则H(x)，变为H(x||nonce)，才不会被蛮力破解 8、除了密码学基础两大性质之后，还需要一个性质<strong>puzzle friendly</strong>（<span style="border-bottom:2px dashed red;">指就算得到哈希结果，事先是不知道哪个输入会得到该结果</span>） 9、挖矿就是寻找nonce，nonce与块头的其他信息组合，然后需要H(block header)&lt;=target（区块链时链表，每块有个块头，块头有很多域设置信息，其中一个就是设置nonce）（<span style="border-bottom:2px dashed red;">挖矿就是不断试不同nonce，然后放入块头进行哈希运算，得出结果落在target space指定范围内</span>）（只有不断试nonce才行，没有其他方法，这也代表了工作量）（一旦找到nonce发布出去，计算一次哈希就可以验证是否正确，称为difficult to solve解决很难，easy to verify验证很简单） 10、比特币中所用的哈希函数：<strong>SHA-256</strong>（Secure Hash Algorithm） 11、比特币账户，去中心化，在本地创立公钥与私钥（public key, private key）即可，就是一个账户，运用asymmetric encryption algorithm非对称加密算法（<strong>加密用公钥，解密用私钥</strong>）（公钥是公开的（相当于账号），私钥是保留本地的（相当于账户密码），<span style="border-bottom:2px dashed red;">发送方用接收方的公钥加密信息，接收方用其私钥解密</span>） 12、公钥 私钥适用于签名，每次交易，<span style="border-bottom:2px dashed red;">发送方需要在交易信息发送前用私钥进行签名</span>（才能确保是发送方发出，而不是冒名顶替，可以用公钥验证是否本人） 13、两个人公钥私钥相同的概率可忽略不计（前提是生成公钥私钥的随机源要好，否则就会出现该情况）</p><h4 id="比特币的数据结构">比特币的数据结构：</h4><p>1、hash pointer哈希指针：需要存地址和哈希值 2、区块链与普通链表区别是：用了哈希指针代替了普通指针（第一个块是系统产生的，称为genesis block创世纪块），这个结构可以实现tamper-evident log防篡改日志，<span style="border-bottom:2px dashed red;">一旦篡改区块的哈希信息，后面区块哈希信息不匹配</span>，就连接不上了（有了这个机制，可以仅保留目前几个区块就可以了，要是需要前面的区块，可以向系统索取） 3、Merkle Tree（结构之一）：哈希指针的二叉树（叶子节点都是data block数据块（交易信息），非叶子节点都是hash point哈希指针）（知道一个root的哈希值，只要有一个地方发生改变，均可知道，只要一改变，哈希值就对不上了）（树比链表结构更优，遍历速度快） 4、每个区块分成block header块头和block body块身，交易信息在块身 5、<strong>merkle tree</strong>提供merkle proof证明 6、比特币节点分成两类：<span style="border-bottom:2px dashed red;">全节点（有块头块身，有交易信息，类似比特币客户端）清节点（仅保留块头，块头仅有root哈希值，用于个人用户，则无法知道交易是否被提交到区块链，需要merkle proof验证）</span> 7、merkle proof就是可以查询交易的叶子节点，一直往根节点查询，整个路径；<span style="border-bottom:2px dashed red;">需要向全节点请求提供图中红色节点哈希值，即可计算出root哈希值，再进行对比</span>即可知道是否存在该交易（但仅仅能验证该分支的正确性，其他分支无法验证），merkle proof过程称为proof of memebership / proofof inclusion，其复杂度为O(log(n)) <img src="https://github.com/soloistben/images/raw/master/block_chain_image/merkle_tree.jpg" alt="image"> 8、要证明一个交易节点不在merkle tree中，需要得到整个树，得到所有叶子节点，在排除，复杂度是O(n)；若是先将叶子节点的哈希值进行排序，则可以根据哈希值找到两个叶子节点（即范围），在验证一下俩节点哈希值往上到root哈希是否正确，若正确，则该交易点不在merkle树中 9、只要无环数据结构，都可以用上哈希指针，有环就不可以</p><h4 id="共识协议">共识协议：</h4><p>pass</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;比特币和以太坊均是区块链技术下一种&lt;strong&gt;crypto-currency加密货币&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;比特币：&lt;/strong&gt;密码学基础、比特币的数据结构、共识协议和系统实现、挖矿算法和难度调整、比特币脚本、软分叉和硬分
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
