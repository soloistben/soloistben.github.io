<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>MR.C</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-02T15:30:36.448Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>(soloistben)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dimensionality_Reduction</title>
    <link href="http://yoursite.com/2020/10/02/Dimensionality-Reduction/"/>
    <id>http://yoursite.com/2020/10/02/Dimensionality-Reduction/</id>
    <published>2020-10-02T13:10:36.000Z</published>
    <updated>2020-10-02T15:30:36.448Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>过拟合</p><ul><li>解决方法：增加数据、正则化、降维</li><li><p>原因：<strong>维度灾难</strong></p><ul><li>在没有很多数据集时，只能降维</li><li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li><li><p>从几何层面看：</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p><ul><li>2维正方形面积：1，圆形：pi*(0.5)^2</li><li>3维正方体体积：1，球体体积：4/3*pi*(0.5)^3 = K*(0.5)^3</li><li>D维超立方体体积：1，超球体体积： K*(0.5)^D</li><li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p></li><li><p>D维外超球体体积：K*1^D = K，环形体积：外超球体体积 - 内超球体体积 = K - K(1-e)^D</p><ul><li>V外/V内 = 1-(1-e)^D  =1（0&lt;e&lt;1，D趋向无穷大之后，(1-e)^D趋向于0）</li><li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li></ul></li></ul></li></ul></li></ul></li><li>Data<ul><li>N个p维样本 X（维度N×p）（设I为N维全1列向量）</li><li>样本均值 X～=1/N Σ x_i = 1/N X^T I（维度p×1）</li><li>方差 S = 1/N Σ (x_i - X~)(x_i - X~)^T = 1/N X^T (I - 1/N I I^T) (1-1/N I I^T)^T X = 1/N X^T H H^T X = 1/N X^T H X<ul><li>（维度p×p）</li><li>H =  I-1/N I I^T  centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li><li>H^T = H, H^2 = H H^T = (I-1/N I I^T) (I-1/N I I^T)^T = I-1/N I I^T = H</li><li>H^n = H</li></ul></li></ul></li><li><p>降维方法</p><ul><li>直接降维 （特征选择：人工选取重要特征 ）</li><li><p>线性降维</p><ul><li><p><strong>Principal Components Analysis PCA 主成分分析</strong></p><ul><li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p></li><li><p>最大投影方差</p><ul><li>找到一个u_1平面，使投影间距达到最大（投影到u_2平面，距离太小，没有意义）<ul><li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li><li>第1步：中心化：先将所有数据平移，利于计算，即 x_i - X～</li><li>第2步：投影到u_1平面：(x_i - X～)^T u_1（设定 |u_1|=1，即u_1^T u_1=1）</li><li>第3步：投影方差 J = 1/N Σ [(x_i - X~)^T u_1]^2 = 1/N Σ [u_1^T (x_i - X~) (x_i - X~)^T u_1] = u_1^T S u_1</li></ul></li><li>u_1~ = argmax u_1^T S u_1  s.t. u_1^T u_1=1<ul><li>使用拉格朗日求解</li><li>L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)</li><li>dL/du_1 = 2 S u_1 - 2λ u_1 = 0  ==&gt; <strong>S u_1 = λ u_1</strong> </li><li><strong>u_1为eigen-vector特征向量，λ为eigen-value特征值</strong></li></ul></li></ul></li><li>最小重构代价<ul><li>投影在u_1平面的点恢复到原来样子的代价</li><li>设从原p维降到q维（下面u代表特征）</li><li>x_i = Σ_k^p (x_i^T u_k) u_k, x_i ~ = Σ_k^q (x_i^T u_k) u_k  (用特征u_k描述样本x_i，(x_i^T u_k)为距离大小，u_k为单位大小，则第k维描述为(x_i^T u_k) u_k)</li><li>代价函数 L = 1/N Σ||x_i - x_i ~||^2 = Σ_(k=q+1)^p u_k T S u_k  = Σ_(k=q+1)^p λ_k (s.t. u_k^T u_k=1)</li><li>u_k = argmin L</li></ul></li></ul></li><li>MDS</li></ul></li><li><p>非线性降温</p><ul><li>流型</li><li>Isomap</li><li>LLE</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;过拟合&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解决方法：增加数据、正则化、降维&lt;/li&gt;
&lt;li&gt;&lt;p&gt;原因：&lt;strong&gt;维度灾难&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在没有很多数据集时，只能降维&lt;/li&gt;
&lt;li&gt;每增加一维，二值的特征，都是2的指数倍增
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>SVM</title>
    <link href="http://yoursite.com/2020/10/02/SVM/"/>
    <id>http://yoursite.com/2020/10/02/SVM/</id>
    <published>2020-10-02T08:50:04.000Z</published>
    <updated>2020-10-02T08:50:04.873Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Decision_Tree</title>
    <link href="http://yoursite.com/2020/10/02/Decision-Tree/"/>
    <id>http://yoursite.com/2020/10/02/Decision-Tree/</id>
    <published>2020-10-02T08:49:49.000Z</published>
    <updated>2020-10-02T09:02:15.922Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>基于数据特征构造决策树</p><ul><li>有向边</li><li><p>结点</p><ul><li>内部结点(internal node)-&gt;表示特征</li><li><p>叶子结点(leaf node)-&gt;表示类别</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p></li><li><p>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</p></li></ul></li><li>决策树表示给定特征条件下类的条件概率分布。<ul><li>一个条概率分布定义特征空间的一个划分上</li><li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li></ul></li><li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果<br><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li></ul></li><li><p>决策树学习过程：特征选择、决策树生成、剪枝</p><ul><li><p><strong>ID3算法</strong>（分类、多叉树）</p><ul><li><p>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）</p><ul><li>Information Gain 信息增益。信息增益越大对应的特征越重要</li><li>Entropy 熵，表示随机变量不确定的度量</li><li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"></p></li></ul></li><li><p>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</p></li><li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li></ul></li><li><p><strong>C4.5算法</strong>（分类、多叉树）</p><ul><li><p>特征选择</p><ul><li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p></li></ul></li><li><p>生成：与ID3算法类似</p></li><li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li><li>二叉树模型会比多叉树运算效率高</li><li>无剪枝</li></ul></li><li><p><strong>CART</strong> classification and regression tree（分类、回归、二叉树）</p><ul><li><p>分类</p><ul><li><p>特征选择</p><ul><li>Gini基尼指数</li><li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li><li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"></p></li><li><p>生成</p><ul><li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li><li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li></ul></li><li><p>损失函数</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p><ul><li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p></li></ul></li><li><p>回归</p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于数据特征构造决策树&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有向边&lt;/li&gt;
&lt;li&gt;&lt;p&gt;结点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内部结点(internal node)-&amp;gt;表示特征&lt;/li&gt;
&lt;li&gt;&lt;p&gt;叶子结点(leaf node)-&amp;gt;表示类别&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Statistics</title>
    <link href="http://yoursite.com/2020/09/11/Statistics/"/>
    <id>http://yoursite.com/2020/09/11/Statistics/</id>
    <published>2020-09-11T08:34:56.000Z</published>
    <updated>2020-10-02T09:05:52.467Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Statistics-for-Machine-Learning"><a href="#Statistics-for-Machine-Learning" class="headerlink" title="Statistics for Machine Learning"></a>Statistics for Machine Learning</h4><h5 id="One-两大派系"><a href="#One-两大派系" class="headerlink" title="One. 两大派系"></a>One. 两大派系</h5><ul><li><strong>频率派：统计机器学习</strong>（设计模型，找到loss function，设计算法。本质为优化问题）</li><li><strong>贝叶斯派：概率图模型</strong>（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型）</li><li>Data<ul><li>X: data, X={x_1, x_2, …, x_n}^T  Dimension(N, P)</li><li>θ: parameter,   X~p(X|θ)</li></ul></li><li><strong>频率派</strong><ul><li>θ为未知常数；X为随机变量</li><li>loss = P(X|θ) = Π P(x_i|θ)<ul><li>x_1, x_2, …, x_n之间独立同分布</li></ul></li><li>Maximum Likelihood Estimation 极大似然估计<ul><li>θ_MLE = argmax_θ log P(X|θ)</li></ul></li></ul></li><li><strong>贝叶斯派</strong><ul><li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li><li>posterior probability 后验概率<ul><li>P(θ|X) = P(X, θ)/P(X) = P(X|θ)P(θ)/P(X) = likelihood*prior / ∫_θ P(X|θ) dθ</li></ul></li><li>Maximum A Posterior Probability 最大后验概率<ul><li>找到θ在分布中最大值点</li><li>θ_MLE = argmax_θ P(X|θ) = argmax_θ P(X|θ)P(θ)</li><li>P(θ|X)其分母是不变的，则θ_MLE与分子成正比，只需要算分子</li></ul></li><li>贝叶斯估计<ul><li>P(θ|X) = P(X|θ)P(θ) / ∫_θ P(X|θ) dθ</li><li>必须完整计算整个分子式</li></ul></li><li>贝叶斯预测<ul><li>X (train), X~ (test),  X -&gt; θ -&gt; X~</li><li>训练数据通过学习参数θ，与测试数据关联</li><li>P(X~|X) = ∫_θ P(X~, θ|X) dθ = ∫_θ P(X~|X)P(θ|X) dθ</li></ul></li></ul></li></ul><h5 id="Two-Linear-Regression"><a href="#Two-Linear-Regression" class="headerlink" title="Two. Linear Regression"></a>Two. Linear Regression</h5><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p><p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线f(w) = w^T x + b（偏置b可先忽略）</p><ul><li><p>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）</p><ul><li>线性（属性线性、全局线性、系数线性）<ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li><li>系数非线性：神经网络（感知机）</li></ul></li><li>全局性<ul><li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li></ul></li><li>数据未加工<ul><li>预处理：PCA，流行</li></ul></li></ul></li><li><p><strong>矩阵表达</strong></p><ul><li>Least Squares 最小二乘估计法（最小平方法）<ul><li><strong>L(w) = Σ||w^T x_i - y_i||^2</strong> = Σ(w^T x_i - y_i)^2 = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y</li></ul></li><li><strong>w~ = argmin L(w)</strong></li><li>求导 dL/dw =  2 X^T X w - 2 X^T Y = 0  —-&gt; X^T X w = X^T Y<ul><li><strong>w~ = (X^T X)^-1 X^T Y</strong> (伪逆：(X^T X)^-1 X^T)</li></ul></li><li>x_3的误差为(w^T x_3 - y_3)，即所有误差分成一小段一小段</li></ul></li><li><p><strong>几何意义</strong></p><ul><li>f(w) = w^T x &lt;=&gt; f(β) = x^T β</li><li>可以将数据X看成p维的空间，Y是不在该p维空间内</li><li><p>目标：在p维空间中找到一条直线f(β)离Y最近，即Y在p维空间的投影</p><ul><li><p>若向量a与向量b垂直，则 a^T b = 0</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li><li><p>虚线为(Y - Xβ) 与X的p维空间垂直，X^T (Y-Xβ) = 0 —-&gt; β = (X^T X)^-1 X^T Y</p></li><li>误差分散在p个维度上</li></ul></li></ul></li><li><p><strong>概率角度</strong></p><ul><li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li><li>数据本身会带有噪声 ε~N(0, σ^2)</li><li>y = f(w) + ε = w^T x + ε<ul><li>y|x,w ~ N(w^T x, σ^2)  &lt;=&gt; <strong>P(y|x,w)</strong> =1/(sqrt(2*pi)*σ) exp(-(y - w^T x)^2/(2*σ^2))</li></ul></li><li>定义log-likelihood： <ul><li>L_MLE (w) = log P(y|x,w) = log Π P(y_i|x_i,w) = Σ log P(y_i|x_i,w) = Σ[log(1/(sqrt(2*pi)*σ)) + log(exp(-(y_i - w^T x_i)^2/(2*σ^2)))] = Σ[log(1/(sqrt(2*pi)*σ)) -(y_i - w^T x_i)^2/(2*σ^2)]</li><li>样本之间独立同分布 </li><li>w~ = argmax L_MLE (w) = argmax -(y_i - w^T x_i)^2/(2*σ^2) = argmin (y_i - w^T x_i)^2</li><li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li></ul></li></ul></li><li><p><strong>Regularization 正则化</strong> </p><ul><li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong><ul><li>最直接是加样本数据</li><li>降维or特征选择or特征提取 (PCA)</li><li>正则化（损失函数加个约束）：argmin [L(w)+λP(w)]</li></ul></li><li>L1 -&gt; Lasso<ul><li>P(w) = ||w||_1</li></ul></li><li>L2 -&gt; Ridge 岭回归<ul><li>P(w) = ||w||_2 = w^T w</li><li>权值衰减</li><li>J(w) = Σ||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y<ul><li>w~ = argmin J(w)</li><li>dJ/dw = 2 (X^T X + λ I)w - 2 X^T Y = 0, w~ = (X^T X + λ I)^-1 X^T Y</li><li>X^T X 是半正定矩阵+对角矩阵=(X^T X + λ I)正定矩阵，必然<strong>可逆</strong></li></ul></li></ul></li><li>贝叶斯的角度<ul><li>参数w服从分布，w~N(0,σ_0^2) —&gt; <strong>P(w)</strong> = 1/(sqrt(2*pi)*σ_0) exp(||w||^2/(2*σ_0^2))</li><li>P(w|y) = P(y|w)P(w) / P(y)</li><li>MAP: w~ = argmax P(w|y) = argmax P(y|w)P(w) = argmax log(P(y|w)P(w)) = argmax log[1/(2*pi*σ_0*σ) exp(-(y_i - w^T x_i)^2/(2*σ^2) -||w||^2/(2*σ_0^2))] = argmin [(y_i - w^T x_i)^2 + σ^2/σ_0^2||w||^2] = argmin [L(w)+λP(w)]</li><li>λ = σ^2/σ_0^2</li><li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li></ul></li></ul></li></ul><h5 id="Three-Linear-Classification"><a href="#Three-Linear-Classification" class="headerlink" title="Three. Linear Classification"></a>Three. Linear Classification</h5><ul><li><p>线性回归——&gt;激活函数，降维——-&gt;线性分类</p></li><li><p>硬分类：0/1</p><ul><li>线性判别分析 (Fisher)</li><li>感知机</li></ul></li><li><p>软分类：[0,1]区间内概率</p><ul><li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li><li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li></ul></li><li><p><strong>Perceptron 感知机</strong> (1957年)</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p><ul><li>判别模型</li><li>样本：{(x_i, y_i)}, N个</li><li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li><li>模型：f(x) = sign(w^T x + b) （w^T x大于等于0表示为1（分类正确），反之为-1（分类错误））</li><li>策略：loss function（被错误分类的样本个数）<ul><li>L(w) = Σ I{y_i * (w^T x_i) &lt; 0} （非连续函数，不可导）</li><li>L(w) = Σ -y_i w^T x_i, dL = -y_i x_i</li></ul></li><li>若是非线性可分，可是使用pocket algorithm</li></ul></li><li><p><strong>线性判别分析</strong></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p><ul><li>样本：N个p维样本，二分类(+1,-1)，正样本个数N_1，均值X_c1，方差S_c1，负样本个数N_2，均值X_c2，方差S_c2，（S_c1 = 1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T）</li><li>思想：类内小，类间大<ul><li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li><li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li><li>Z平面的法向量为最后找到的分类函数 w^T x（因为垂直，则Z平面即w向量）<ul><li>（前提设置||w||=1）</li><li>则样本点投影到Z平面为：|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i</li></ul></li></ul></li><li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong> <ul><li>N_1 = 1/N_1 Σ w^T x_i</li><li>S_1 = 1/N_1 Σ (w^T x_i - Z_1) (w^T x_i - Z_1)^T</li><li>类间：(Z_1-Z_2)^2</li><li>类内：S_1+S_2</li></ul></li><li>策略：L(w) = (Z_1-Z_2)^2 / (S_1+S_2) = [w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w] / [ w^T (S_c1+ S_c2) w ]<ul><li>分子 = [w^T (1/N_1 Σ x_i - 1/N_2 Σ x_i)]^2 = [w^T (X_c1 - X_c2)]^2 = w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w</li><li>分母 =  w^T S_c1 w +  w^T S_c2 w =  w^T (S_c1+ S_c2) w<ul><li>S_1 =  1/N_1 Σ (w^T x_i - 1/N_1 Σ w^T x_j)(w^T x_i - 1/N_1 Σ w^T x_j)^T=w^T [1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T] w = w^T S_c1 w</li></ul></li><li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li><li>L(w) = w^T S_b w / w^T S_w w</li><li>w~ = argmax L(w)<ul><li>dL/dw = 2*S_b w (w^T S_w w)^-1 + (w^T S_b w) * (-1) (w^T S_w w)^-2 * 2 * S_w w = 0</li><li>S_b w (w^T S_w w) = (w^T S_b w) S_w w （(w^T S_w w) 最终计算得一个实数，一维，没有方向）（求解w～关心的是方向，因为平面的大小可以缩放，所以意义不大）</li><li>w = (w^T S_w w)/(w^T S_b w) * S_w^-1 * S_b w，正比于(S_w^-1 * S_b w) ，正比于(S_w^-1 *(X_c1 - X_c2))</li><li>（S_b w = (X_c1 - X_c2)(X_c1 - X_c2)^T w，(X_c1 - X_c2)^T w为实数）</li><li>（若S_w是对角矩阵，各向同性，S_w正比于单位矩阵，则w正比于(X_c1 - tX_c2)</li></ul></li></ul></li><li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li></ul></li><li><p><strong>Logistic Regression</strong></p><ul><li>线性回归——&gt;sigmoid——-&gt;线性分类</li><li>判别模型</li><li>model<ul><li>sigmoid(x) = 1/(1+e^-x)，将w^T x映射到处于[0,1]区间的概率值p</li><li>p_1 = P(y=1|x) = sigmoid(w^T x) = 1/(1+e^(w^T x))</li><li>p_0 = P(y=0|x) = sigmoid(w^T x) = e^(w^T x)/(1+e^(w^T x))</li><li>综合表达：P(y|x) = p_1^y * p_0^(1-y)</li></ul></li><li>w~ = argmax P(Y|X) = argmax log Π P(y_i|x_i) = argmax Σ log P(y_i|x_i) = argmax Σ [y_i*log p_1 + (1-y_i)*log p_0] （-cross entropy）<ul><li>MLE &lt;=&gt; loss function (min cross entropy)</li></ul></li></ul></li><li><p><strong>Gaussian Discriminant Analysis</strong></p><ul><li>生成模型、连续<ul><li>y~ = argmax P(y|x) = argmax P(x|y)P(y)</li><li>分类：最终比较P(y=0|x)，P(y=1|x)大小</li><li>P(y|x)正比于P(x|y)P(y)，即联合概率P(x, y)</li></ul></li><li>Data：N个d维样本，二分类(0,1)，正样本个数N_1，方差S_1，负样本个数N_2，方差S_2</li><li><strong>prior probability</strong> <ul><li>先验概率服从伯努利分布</li><li>y ~ Bernoulli，P(y=1) = p，P(y=0) = 1-p</li><li>P(y) = p^y*(1-p)^(1-y)</li></ul></li><li><strong>conditional probability</strong><ul><li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li><li>x|y=1 ~ N(u_1, σ)</li><li>x|y=0 ~ N(u_2, σ)</li><li>方差一样（权值共享），均值不一样</li><li>P(x|y) = N(u_1, σ)^y * N(u_2, σ)^(1-y)</li></ul></li><li><strong>loss function</strong><ul><li>log MLE =&gt; L(θ) =  log Π P(x_i, y_i) = Σ log [P(x_i|y_i)P(y_i)] = Σ[log P(x_i|y_i) + log P(y_i)] =  Σ[log N(u_1, σ)^y_i * N(u_2, σ)^(1-y_i) + log p^y_i*(1-p)^(1-y_i)] = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ) + y_i*log p + (1-y_i)*log (1-p)]</li><li>θ = (u_1, u_2, σ, p)</li><li>θ ~ = argmax L(θ)</li><li>求解4个参数<ul><li>p<ul><li>相关部分 L = Σ [log p^y_i + log (1-p)^(1-y_i)]</li><li>dL/dp = Σ [y_i/p - (1-y_i)/(1-p)] = 0  =&gt;  Σ [y_i*(1-p)- (1-y_i)*p] = Σ (y_i - p) = 0</li><li>p~ = 1/N Σ y_i  = N_1/N（二分类（0,1），Σ y_i = N_1）</li></ul></li><li>u_1 （同理 u_2）<ul><li>相关部分 L = Σ y_i*log N(u_1, σ) = Σ y_i*log [1/((2*pi)^(d/2)*σ^(1/2)) exp((x_i-u_1)^T(x_i-u_1)/-2*σ)</li><li>u_1 = argmax L = argmax Σ y_i * [(x_i-u_1)^T(x_i-u_1)/-2*σ] =  argmax -1/2 Σ y_i * [(x_i-u_1)^T(x_i-u_1)σ^-1] = argmax -1/2 Σ y_i * [x_i^T σ^-1 x_i - 2*u_1^T σ^-1 x_i + u_1^T σ^-1 u_1] </li><li>dL/du_1 = -1/2 Σ y_i * [-2*σ^-1 x_i + 2*σ^-1 u_1] = 0  =&gt;   Σ y_i * (u_1 - x_i) = 0</li><li>u_1 = Σ y_i x_i / Σ y_i = Σ y_i x_i / N_1</li></ul></li><li>σ<ul><li>相关部分 L = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ)] = Σlog N(u_1, σ) +  Σlog N(u_2, σ)<ul><li>（二分类，非0即1，可以拆分算，可以省去y_i）</li></ul></li><li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li><li>σ = 1/N (N_1*S_1 + N_2*S_2)</li></ul></li></ul></li></ul></li></ul></li><li><p><strong>Naive Bayes</strong></p><ul><li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong><ul><li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li><li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li><li>最简单概率图模型</li></ul></li><li>生成模型、离散</li><li>Data<ul><li>X: data, (n, d), n个数据样本，每个d维向量</li><li>Y: class, Y={c_1, c_2, …,c_k}, k个类别</li><li>y: label, (1, n), n个标签</li></ul></li><li><strong>prior probability</strong> <ul><li>P(Y=c_k)</li><li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li><li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li><li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li><li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li><li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li></ul></li><li><strong>conditional probability</strong><ul><li>P(X=x|Y=c_k) = P(X^(1)=x^(1), …, X^(d)=x^(d)|Y=c_k) = ΠP(X^(j)=x^(j)|Y=c_k)</li><li>特征条件独立</li><li>^(1) 上标表示第1维度</li></ul></li><li><strong>joint probability distributions</strong><ul><li>联合概率分布</li><li>P(X, Y) = P(X|Y)P(Y)</li></ul></li><li><strong>posterior probability</strong><ul><li>P(Y=c_k|X=x) = P(X=x|Y=c_k)P(Y=c_k)/ΣP(X=x|Y=c_k)P(Y=c_k) = P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)/Σ P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li><li>则分类器为<ul><li>y=f(x) = argmax P(Y=c_k|X=x) = argmax P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li><li>分母不变，则仅于分子成正比</li><li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li></ul></li></ul></li><li><strong>loss function</strong><ul><li>最大后验概率转-&gt;期望风险最小化</li><li>L(Y, f(x)) = 1 if Y!=f(x) or 0 if Y==f(x)<ul><li>Y: train label, y=f(x) : predict label</li></ul></li><li>期望风险函数：R_exp(f) = E[L(Y, f(x))]<ul><li>根据联合概率分布：R_exp(f)  = E_x Σ[L(c_k|f(x))]P(c_k|X)</li></ul></li><li>f(x) = argmin Σ[L(c_k|f(x))]P(c_k|X) <ul><li>根据L(Y, f(x))函数展开，消去Y==f(x)项</li><li>f(x) = <strong>argmin ΣP(y!=c_k|X=x)</strong> = argmin (1-P(y=c_k|X=x)) = <strong>argmax P(y=c_k|X=x)</strong></li></ul></li><li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c_k类别的最大概率为多少）</li></ul></li><li>详情案例见统计学习方法(第二版)63页</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Statistics-for-Machine-Learning&quot;&gt;&lt;a href=&quot;#Statistics-for-Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;Statistics for Machine Learnin
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Bayes_MRF</title>
    <link href="http://yoursite.com/2020/08/12/Bayes-MRF/"/>
    <id>http://yoursite.com/2020/08/12/Bayes-MRF/</id>
    <published>2020-08-12T12:10:46.000Z</published>
    <updated>2020-09-12T02:59:36.299Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场"><a href="#Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场" class="headerlink" title="Bayes 贝叶斯 &amp; Markov Random Fields 马尔可夫随机场"></a>Bayes 贝叶斯 &amp; Markov Random Fields 马尔可夫随机场</h4><h5 id="One-前提"><a href="#One-前提" class="headerlink" title="One. 前提"></a>One. 前提</h5><ul><li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）<ul><li>Representation 表示<ul><li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li><li>无向图 Markov Network (无向，则结点的概率仅取决于1阶邻居)</li><li>高斯图 <ul><li>Gassian Bayes</li><li>Gassian Markov</li></ul></li></ul></li><li>Inference 推断<ul><li>精确推断</li><li>近似推断<ul><li>确定性近似（变分推断）</li><li>随机性近似（MCMC）</li></ul></li></ul></li><li>Learning 学习<ul><li>参数学习<ul><li>完备数据（非隐变量）（有向，无向）</li><li>隐变量（EM）</li></ul></li><li>结构学习 (学习更好的图结构，参数)</li></ul></li></ul></li><li><strong>高维随机变量</strong> P(x_1, x_2, …, x_p) 计算量太大<ul><li>边缘概率 P(x_i)</li><li>条件概率 P(x_j|x_i)</li></ul></li><li><strong>运算原则</strong><ul><li>sum rule: P(x_1) =  ∫ P(x_1, x_2) dx_2 (边缘概率)</li><li>poduct rule: P(x_1, x_2) = P(x_1)*P(x_2|X_1) = P(x_2)*P(x_1|X_2)</li><li>chain rule: P(x_1, x_2, …, x_p) = Π P(x_i|x_1, x_2, …, x_i-1)</li><li>bayesian rule: P(x_2|x_1) = P(x_1, x_2) / P(x_1) = P(x_1, x_2) / ∫ P(x_1, x_2) dx_2 =  P(x_2)*P(x_1|x_2) /  ∫ P(x_1, x_2) dx_2</li></ul></li><li><strong>缺点</strong><ul><li><font color="red">高维复杂 P(x_1, x_2, …, x_p) 计算量太大</font></li><li>简化 <ul><li>每个<strong>维度之间相互独立</strong>    (特性太强了)<ul><li>P(x_1, x_2, …, x_p) = Π P(x_i)</li><li>Naive Bayes 朴素贝叶斯 P(x|y) = Π P(x_i|y)</li></ul></li><li>Markov Property 马尔可夫特性<ul><li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li><li>x_i+1 只与 x_i相关，与其他 x_i-1,…,x_1无关</li></ul></li><li><strong>条件独立性</strong>（可降低计算复杂度）<ul><li>给定x_B情况下，集合x_A与集合x_C无关 （x_A，x_B，x_C无交集）</li><li>x_B 只与x_C相关<br><a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a>    <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li></ul></li></ul></li></ul></li></ul><h5 id="Two-Bayes"><a href="#Two-Bayes" class="headerlink" title="Two. Bayes"></a>Two. Bayes</h5><ul><li>链式法则 P(x_1, x_2, …, x_p) = Π P(x_i|x_1, x_2, …, x_i-1)</li><li><p>因子分解 P(x_1, x_2,…, x_p) = Π P(x_i|x_p(i)),  x_p(i)为x_i父亲集合 （条件独立性）<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p></li><li><p><strong>tail to tail</strong></p><ul><li>因子分解 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A)</li><li>链式法则 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A,B)<ul><li>则 P(C|A) = P(C|A,B)，则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，“倒V路径”</font>）</li><li>P(B|A)P(C|A,B) = P(B|A)P(C|A) = P(B,C|A)</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p></li><li><p><strong>head to tail</strong></p><ul><li>发生A时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞</font>）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p></li><li><p><strong>head to head</strong></p><ul><li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A,B有关系，不独立则难以分解</font>）</li><li>因子分解 -&gt; P(A,B,C) = P(A)P(B)P(C|A,B) （父亲结点先于子结点）</li><li>链式法则 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A,B)<ul><li>P(B) = P(B|A)，则C还没被观察，A,B相互独立</li></ul></li><li>这个模式是想判断 P(A|C) == P(A|C,B)，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p><ul><li>（<font color="red">若D被观测，路径也是是连通</font>）</li></ul></li><li><p>有向图是的条件独立性（证明发生x_B, x_A, x_C相互独立）</p><ul><li><p>D-separation</p><ul><li><p>x_A, x_B, x_C 三个集合两两无交集<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p></li><li><p>若A与C，存在B1，B2关系，且属于x_B集合；存在B3，B4关系，且不属于x_B集合</p></li><li>则符合：发生x_B, x_A, x_C相互独立 （<strong>全局马尔可夫性</strong>）</li><li><p>P(x_i|x_-i) = P(x_i, x_-i) / P(x_i) = p(x) / ∫ P(x_i) dx_i = Π P(x_j|x_p(j)) / ∫ Π P(x_j|x_p(j)) dx_i</p><ul><li>x_-i表示集合{x_1,…x_p}中去除x_i, <strong>x/x_i</strong></li><li>Π P(x_j|x_p(j)) 分成，与x_i有关和无关两部分</li><li><p>则 Π P(x_j|x_p(j)) / ∫ Π P(x_j|x_p(j)) dx_i无关部分则可以约去</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"></p></li><li><p>P(x_i|x_-i)  = P(x_i|x_p(i))，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket马尔可夫态</p></li><li>一个人与全世界的关系=一个人与身边人的关系</li></ul></li></ul></li></ul></li><li><p>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）</p><ul><li><p>离散</p><ul><li><p>单一</p><ul><li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; P(x|y) = ΠP(x_i|y=1)  (x 是p维)</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p></li></ul></li><li><p>混合</p><ul><li><strong>GMM</strong> 高斯混合模型 -&gt; 做聚类</li></ul></li><li>时间<ul><li><strong>Markov Chain</strong> 马尔可夫链</li><li><strong>Gaussian Process</strong> 无限维高斯分布</li></ul></li><li>动态模型 = 混合 + 时间<ul><li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li><li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li><li><strong>Partide Filter</strong> （非连续，非高斯）</li></ul></li></ul></li><li>连续<ul><li><strong>Gaussian Bayes Network</strong> 高斯图</li></ul></li></ul></li></ul><h5 id="Three-MRF"><a href="#Three-MRF" class="headerlink" title="Three. MRF"></a>Three. MRF</h5><ul><li>无向图</li><li>条件独立性，发生x_B时，x_A与x_C无关 （<strong>global markov</strong> 全局马尔可夫）<ul><li>存在x_A, x_C被x_B分割（对应 bayes D-separation）， 那么发生x_B时，x_A与x_C无关 </li></ul></li><li><p><strong>local markov</strong> 局部马尔可夫</p><ul><li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p></li></ul></li><li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p><ul><li>对于集合x_-i,-j(集合没有i，j结点), 任意两个点x_i, x_j相互独立, (i != j)</li></ul></li><li>相互等价，可以互推 global markov <-> local markov <-> pair markov <->  基于最大团的因子分解</-></-></-></li><li>clique团，最大团<ul><li>集合间的结点相互联通</li><li>在一个团无法添加结点，则是最大团</li><li>(c1, c2,…表示团)</li></ul></li><li><p>因子分解 <strong>P(x) = 1/Z Π Φ(x_ci)</strong> ， Z = Σ_x1…Σ_xp Π Φ(x_ci) </p><ul><li>Φ 势函数， 必须为正（大于0）<ul><li><strong>Φ (x_ci) = exp {-E(x_ci)}</strong>   (E为能量函数，也叫势函数)</li><li>Φ (x_ci)  <-> P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</-></li><li>P(x) = 1/Z Π Φ(x_ci) = 1/Z Π exp{-E(x_ci)} = 1/Z exp{-ΣE(x_ci)} </li><li>最大熵原理 =&gt; 指数族分布 (Gibbs Distribution &lt;=&gt; Markov Random Field)</li><li>统计物理</li></ul></li><li>ci最大团</li><li>Z 为联合概率分布的归一化因子</li><li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li><li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li><li>Pair-MRF 因子分解：<strong>P(x) = 1/Z Π Φ(x_i) Π Φ(x_i, x_j)</strong>   (考虑边)</li><li><p>最大后验概率推理（图像分割问题）：max_x P(x)</p><ul><li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"></p></li><li><p>假设θ(x_i) = -logΦ(x_i); θ(x_i, x_j) = -logΦ(x_i, x_j)</p></li><li><p>max_x P(x) -&gt; 能量最小化 -&gt; min_x E(x) = Σθ(x_i)  + Σθ(x_i, x_j)</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"></p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p></li><li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"></p></li><li><p>结点的设置势函数，一个前景一个背景</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"></p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场&quot;&gt;&lt;a href=&quot;#Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场&quot; class=&quot;headerlink&quot; title=&quot;Bayes 贝叶
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>HMM_CRF</title>
    <link href="http://yoursite.com/2020/08/09/HMM-CRF/"/>
    <id>http://yoursite.com/2020/08/09/HMM-CRF/</id>
    <published>2020-08-09T12:29:28.000Z</published>
    <updated>2020-08-12T12:11:58.941Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场"><a href="#Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场" class="headerlink" title="Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场"></a>Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场</h4><h5 id="One-Time-Sequence-Series"><a href="#One-Time-Sequence-Series" class="headerlink" title="One. Time Sequence / Series"></a>One. Time Sequence / Series</h5><ul><li>在时间序列中，起伏状态可以被观测到（股票走势就是一种时间序列，股票中的涨跌）</li><li>无法观测到的是时间序列的<strong>隐状态</strong>（股票中是否处于牛市状态，这类的就是隐状态）<ul><li>隐状态会有很多</li><li>当知道隐状态存在时，每个隐状态被观测时，都是相互独立（互不干扰）</li><li>隐状态之间是离散的</li></ul></li></ul><h5 id="Two-HMM"><a href="#Two-HMM" class="headerlink" title="Two. HMM"></a>Two. HMM</h5><ul><li>在概率中，隐马尔可夫模型对应所有状态是相互独立的</li><li><p><strong>P(q_t|q_t-1)</strong>  Transition Probability 转移概率</p><ul><li>P(q_t|q_t-1, q_t-2,…, q_1) = P(q_t|q_t-1)<ul><li>每个状态只取决于前面一个状态，而非前面所有状态</li><li>即当前隐状态到下一个隐状态的概率</li></ul></li><li>Discrete 离散的（用矩阵表示 A，维度(k, k)，k是开个隐状态）</li></ul></li><li><p><strong>P(y_t|q_t)</strong>  Emmission/Measurement Probability 发射概率 / 观测概率</p><ul><li>P(y_t|q_1,…, q_t-1, q_t, y_1,…, y_t-1) = P(y_t|q_t)<ul><li>已知当前状态 q_t，得到事实变化 y_t 的概率</li></ul></li><li>Discrete or Continuous</li><li>若是离散时，可以用矩阵表示 B，维度(k, L)，L是y的取值范围（也是离散的）</li><li>若是连续的，无法使用矩阵表示，y可能是连续的一个分布</li></ul></li><li><p>两个概率决定了整个隐马尔可夫模型 </p></li><li>在信号中用HMM也很多，时间轴每段范围的信号就相当与一个y_t<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm.png" alt="hmm" style="zoom:67%;"></li><li>每个隐状态的概率和为1，则矩阵行和为1</li><li>P(X) = ∫_y P(X, Y) dy </li><li><p><strong>P(y_1, y_2, y_3)</strong> = Σq_1 Σq_2 Σq_3 P(y_1, y_2, y_3, q_1, q_2, q_3) = Σq_1 Σq_2 Σq_3 <strong>P(y_3|q_3)P(q_3|q_2)*P(y_2|q_2)P(q_2|q_1)*P(y_1|q_1)P(q_1)</strong> </p><ul><li>P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|y_1, y_2, q_1, q_2, q_3)*P(y_1, y_2, q_1, q_2, q_3)</li><li>利用马尔可夫性质：P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|q_3)*P(q_3|y_1, y_2, q_1, q_2)*P(y_1, y_2, q_1, q_2) = P(y_3|q_3)P(q_3|q_2)*P(y_1, y_2, q_1, q_2)</li><li>P(y_3|q_3), P(q_3|q_2), P(y_2|q_2), P(q_2|q_1), P(y_1|q_1) 在马尔可夫参数A, B矩阵中可得</li><li><strong>P(q_1) 是初始状态</strong>，这需要问题给出</li><li>则马尔可夫模型需要三个参数，λ={A, B, P(q_1)} (假设y是离散的)</li></ul></li><li><p>马尔可夫模型有什么用？</p><ul><li>找50人讲10个单词（动物名字）</li><li><strong>λ_cat = argmax_λ log P(y_1, …, y_50|λ)</strong> <ul><li>当初始条件为λ，最大为说cat的情况</li><li>记录所有单词的λ</li><li>用高斯或者高斯混合模型计算λ</li></ul></li><li>当来了个新人，说其中一个单词，那么他说哪个单词概率最大？<ul><li><strong>P(y_new|λ_cat)</strong>, … 概率最大为结果</li><li>若所有该概率计算结果很小时，说明新人说的不是原10个单词</li></ul></li></ul></li><li><p>公式范化（计算量较大）<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm_function.png" alt="hmm_function" style="zoom: 67%;"></p><ul><li><p>一个定义<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB.png" alt="FB" style="zoom:67%;"></p></li><li><p>优化计算量</p><ul><li>alpha_i(t) = P(y_1, …, y_t, q_t=i)</li><li>alpha_i(1) = P(y_1, q_1=i) = P(y_i|q_1=i)P(q1) = b_i(y_1) P(q_1) </li><li>alpha_j(2) = P(y_1, y_2, q_2=j) = Σq_1P(y_1, y_2, q_1=i, q_2=j) = Σq_1 P(y_2|q_2=j)P(q_2=j|q_1=i)*P(y_1, q_1=i) = P(y_2|q_2=j) Σq_1 P(q_2=j|q_1=i)*alpha_i(1) = b_j(y_1) Σq_1 a_ij*alpha_i(1)</li><li>开始递归 alpha_j(t) = b_j(y_t) Σq_1 a_ij*alpha_i(t-1) = P(y_1, …, y_t, q_t=j) <ul><li><strong>P(y_1, …, y_t) = Σj P(y_1, …, y_t, q_t=j) = Σj alpha_j(t)</strong></li></ul></li><li>通过贝叶斯公式<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB_2.png" alt="FB_2" style="zoom:67%;"><ul><li>P(Y, q_t=i|λ) = P(Y, q_t=i)P(q_t=i) = P(y_1, …, y_t|q_t=i)P(y_t+1, …, y_T|q_t=i)P(q_t=i) =  P(y_1, …, y_t, q_t=i)P(y_t+1, …, y_T|q_t=i) = alpha_i(t) beta_i(t)</li></ul></li></ul></li></ul></li><li><p>如何学习HMM的参数</p><ul><li>λ = argmax_λ log P(Y|λ) （极大似然估计）</li><li>用EM学习<ul><li>E-step: Σq_1… Σq_t log(P(Y, Q))*P(Q,Y|θ^g) = Σq_1… Σq_t log(P(q_1)*ΠP(q_t|q_t-1)*ΠP(y_t|q_t)) <em> P(Q,Y|θ^g) = Σq_1… Σq_t [log(P(q_1)+Σlog(a_q_t-1,q_t)+Σlog(b_q_t(y_t))] </em> P(Q,Y|θ^g)</li><li>part 5,6还没看完(需要EM基础)</li></ul></li></ul></li></ul><p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLFze15KrfxbGPEHyjxddbbxVvLa5kilFf" target="_blank" rel="noopener">source video</a> <a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/dynamic_model.pdf" target="_blank" rel="noopener">pdf</a></p><h5 id="Three-CRF"><a href="#Three-CRF" class="headerlink" title="Three. CRF"></a>Three. CRF</h5><p>+ </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场&quot;&gt;&lt;a href=&quot;#Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spectral_Cluster</title>
    <link href="http://yoursite.com/2020/07/25/Spectral-Cluster/"/>
    <id>http://yoursite.com/2020/07/25/Spectral-Cluster/</id>
    <published>2020-07-25T05:19:54.000Z</published>
    <updated>2020-07-27T09:32:54.190Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spectral-Cluster-谱聚类"><a href="#Spectral-Cluster-谱聚类" class="headerlink" title="Spectral Cluster 谱聚类"></a>Spectral Cluster 谱聚类</h4><h5 id="一、分类与聚类"><a href="#一、分类与聚类" class="headerlink" title="一、分类与聚类"></a>一、分类与聚类</h5><p>1、分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类别标号y中。</p><p>2、聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。目的是个簇内的元素之间越相似，簇间的相似度越小。</p><h5 id="二、k-means-与-spectral-cluster"><a href="#二、k-means-与-spectral-cluster" class="headerlink" title="二、k-means 与 spectral cluster"></a>二、k-means 与 spectral cluster</h5><p>k-means 每次选择k个中心点，将每个数据点归类到离它最近的那个中心点所代表的簇中，迭代多次，一直到迭代了最大的步数或者前后 <strong>J</strong> 的值相差小于一个阈值为止。(u_k为中心点，r_nk 为数据点 x_n 被归类到 cluster k 的时候为 1 ，否则为 0 )<br>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/kmean.png" alt="kmean"></p><ul><li>结点与附近的结点相似度会更高，距离远的结点相似度更低；因此，对角线上颜色更亮，右上角左下角的区域更暗 </li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/affinity_matrix.png" alt="Similarity Matrix" style="zoom: 25%;"></p><p>然而，k-means初始的中心点是随机选的，每次选择结果不同，大部分情况结果还是令人满意，偶尔也会陷入局部最优。传统 k-means的x_n输入是每个结点的所有信息（即完整的N维度信息）。</p><p>spectral cluster 谱聚类只需要结点之间的相似度矩阵即可，并不需要结点的完整信息，因此不必像k-means那样要求N维的欧氏空间的向量。即抓住了结点之间的主要信息，排除了冗余信息，计算复杂度也更小，所以比传统聚类方法会更加健壮一些。</p><h5 id="三、谱的涵义"><a href="#三、谱的涵义" class="headerlink" title="三、谱的涵义"></a>三、谱的涵义</h5><p>对于谱的概念，简而言之，可以把谱认定为对一个信号（视频，音频，图像，图）分解成为一些简单的元素线性组合（小波基，图基）。为了使得这种分解更加有意义，可以使得这些分解的元素之间是线性无关的（正交的），也就是说这些分解的简单元素可以看作是信号的基。面对研究的东西，往往都会细分的更细小层面才能挖掘更主要的信息，犹如研究生物，则要细分到细胞、基因层面；研究物理，则要细分到质子、夸克层面；目前深度学习研究图片也是细分到像素层级，音频则细分到音频、音位层级。因此针对图也是如此。</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/spectrum.png" alt="spectrum" style="zoom: 50%;"></p><p>在信号处理中，谱就是傅立叶变换，它提供了不同频率下的正弦和余弦波作为基，将信号在这些基进行分解。</p><p>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/fourier.png" alt="fourier" style="zoom: 67%;"></p><p>在图中，“谱”则是指对图的拉普拉斯矩阵的特征分解，特征分解后正交化的特征向量就是对应的正交基，所有的特征值的全体统称为拉普拉斯矩阵的谱，最后则可以用特征向量和特征值来表示图的信息。</p><h5 id="四、Graph-与-Laplacian"><a href="#四、Graph-与-Laplacian" class="headerlink" title="四、Graph 与 Laplacian"></a>四、Graph 与 Laplacian</h5><ul><li><p>Graph G = (V, E, X)</p><p>结构信息：V 表示结点集，E 表示结点之间的边集，A 表示N维的邻接矩阵（若是无向图，则邻接矩阵内元素是0/1; 若是有向图，则邻接矩阵内元素的值是具体权重值w），D表示N维的度矩阵（对应结点的拥有邻居结点数，放置在矩阵对角线上）。</p></li></ul><p>​        特征信息：X表示结点的N维度信息。</p><ul><li><p><strong>为什么特征分解最终选择拉普拉斯矩阵而不是相似矩阵？</strong></p><p>因为拉普拉斯矩阵的独有属性，它是半正定矩阵，则特征值都是大于或等于0, 可以有多个0特征值，每个0特征值对应特征向量上的值大于0对应的节点之间具有连通性，对应一个子图。拉普拉斯矩阵拥有相似矩阵（邻接矩阵）的特性，又拥有独有特性，则更好表达图的信息。</p></li><li><p><strong>为什么需要使用归一化后的拉普拉斯矩阵？</strong></p><p>聚类的目的为两点：第一点是最小化簇间的相似度，第二点是最大化簇内相似度。未归一化的拉普拉斯矩阵仅能达到第一点，归一化的拉普拉斯矩阵    能达到两点要求。(具体如何达到两点的推导，见文章<strong><code>A Tutorial on Spectral Clustering</code></strong>)</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/laplacian.png" alt="Laplacain Matrix" style="zoom: 25%;"></p></li><li><p>特征分解</p><p>求解得到特征值、特征向量，小到大排序特征值，对应特征向量也排序，则最终得到选择前k个最小特征值对应的特征向量。</p><ul><li>在Eigenvector Matrix，第0列是特征值为0对应的特征向量，是个全1列向量（颜色相同）（因为第二步构造的是全连接矩阵，则仅有一个特征值为0）</li><li>第1列对应较亮色块是值大于0的情况，暗色块值小于0，因此较亮色块中对应结点属于一个簇；第2列则是黄色块部分属于一个簇，等。依次类推即可。</li><li>中间图为Eigenvector Matrix 通过t-SNE降到2维的结果，能将不同簇的结点都明显区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接。</li><li>右间图为Eigenvector Matrix 通过t-SNE降到3维的结果，在3D空间领域，也能将不同簇的结点区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接，黄色与棕色有一两个点连接。</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector.png" alt="Eigenvector Matrix" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_2d.png" alt="Eigenvector Matrix 2D" style="zoom: 25%;"><br><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_3d.png" alt="Eigenvector Matrix 3D" style="zoom: 50%;"></p><p>  即spectral cluster可以看作为node feature高维matrix data (n,d) 经过一个<a href="http://blog.pluskid.org/?p=290" target="_blank" rel="noopener">laplace mapping</a>降维得到一个低维embedding  (n, k)，embedding中融入更主要的信息，舍弃冗余信息。        </p><h5 id="五、Spectral-Clustering"><a href="#五、Spectral-Clustering" class="headerlink" title="五、Spectral Clustering"></a>五、Spectral Clustering</h5><p>​    获得embedding即执行k-means</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/orgin.png" alt="原数据图" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/k_means.png" alt="直接 k-means 的结果" style="zoom:25%;"></p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/SpectralClustering.png" alt="谱聚类结果" style="zoom:25%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Spectral-Cluster-谱聚类&quot;&gt;&lt;a href=&quot;#Spectral-Cluster-谱聚类&quot; class=&quot;headerlink&quot; title=&quot;Spectral Cluster 谱聚类&quot;&gt;&lt;/a&gt;Spectral Cluster 谱聚类&lt;/h4&gt;&lt;
      
    
    </summary>
    
    
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>machine_learning</title>
    <link href="http://yoursite.com/2020/05/06/machine-learning/"/>
    <id>http://yoursite.com/2020/05/06/machine-learning/</id>
    <published>2020-05-06T12:20:41.000Z</published>
    <updated>2020-05-06T12:30:41.607Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习-machine-learning-from-TJU"><a href="#机器学习-machine-learning-from-TJU" class="headerlink" title="机器学习 machine learning from TJU"></a>机器学习 machine learning from TJU</h3><h4 id="One-绪论"><a href="#One-绪论" class="headerlink" title="One. 绪论"></a>One. 绪论</h4><ol><li><p>什么是智能？</p><ul><li><strong>Self-adaption 自适应</strong> （迁移学习，通用AI模型 ( Artificial General Intelligence, 即strong AI)）</li><li><strong>Self-consciousness 自我意识</strong> （模糊决策）</li><li>运算智能：快速计算，存储</li><li>感知智能：人类五官的能力（视觉、听觉、触觉等）【已解决】</li><li>认知智能：大脑的能力（逻辑推理、知识理解、决策思考）<strong>（语言处理）</strong>（概念、意识、观念）（理解、思考、决策）【正在解决】</li></ul></li><li><p><strong>Turning Test</strong> 图灵测试：正常人分别和正常人、AI聊天，是否能分清人与AI（是否有用是哲学问题，AI是否伪装，从而不通过Turning Test）</p><p><strong>Behaviorism 行为主义</strong>，仅看行为是否符合智能，不管内部部分（有漏洞）</p><p><strong>Connectionism联结主义</strong>，只看内部构造（用神经网络模拟），符合大脑构造，则认为有智能（婴儿无法通过Turning test，但他结构是符合的）（但无法知道大脑构造，如何产生意识？）</p><p>模拟鸟的飞行，制造飞机（虽然达不到鸟内部的全部飞行系统，但能模拟飞行）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/turning test.png" alt="turning test" style="zoom: 33%;"></p></li><li><p>如何制造 AI？</p><ul><li><strong>“Thinking” by “Searching”</strong>：“思考”即“搜索” (Behaviorism)，类似搜索引擎，信息检索，在已有知识寻找最佳答案（大脑积累知识，面对问题，就是搜索大脑已有知识（但无法确定大脑是如何搜索的））<ul><li>Knowledge Graph 知识图谱</li></ul></li><li><strong>“Learning”</strong>：学习知识，发现新的知识<ul><li>什么是新的知识?（知识-&gt;模式-&gt;稳定的关联关系）</li><li><strong>pattern</strong> 模式识别（机器学习的前身）（语言的语法是一种模式，物理规律也是模式）</li><li>模式识别和机器学习的区别在于：前者喂给机器的是各种特征描述，从而让机器对未知的事物进行判断；后者喂给机器的是某一事物的海量样本，让机器通过样本来自己发现特征，最后去判断某些未知的事物。（机器学习在挖掘数据最终找到模式） （模式识别=数据挖掘）</li></ul></li><li><strong>“Thinking” by “Learning”</strong>：“思考”即“学习”，Known Data-&gt;Model-&gt;Unknown Data<ul><li>Model 模型就是对模式的大概猜测</li><li>y=f(x), f()就是模式</li><li>由于模式很多种，模型则用 y = ax+b 去猜测，算法则调整a和b参数</li><li>机器学习就是科学研究的自动化（确定变量-&gt;做实验-&gt;找到变量之间规律）</li></ul></li></ul></li><li><p>机器学习的基本框架</p><ul><li>一系列可能函数 &amp; 训练数据 -&gt; 通过算法 -&gt; 选出最好的函数</li><li><strong>Supervise Learning</strong> 监督学习，模型只需要找到输入和标记之间的关联关系（标记是人工的，不是自己手标的，就是已标好数据（这种要成本））</li><li><strong>Semi-Supervised Learning</strong> 半监督学习，少量部分样本标记的监督学习</li><li><strong>UnSupervise Learning</strong> 无监督学习，无标记，模型自己总结出类别（聚类）</li><li><strong>Reinforcement Learning</strong> 强化学习，利用间接”标记“来学习<ul><li>围棋的”输赢“，样本是棋局，直接标记是棋子在哪个位置是最好的（但没有这种标记，没有人知道哪里是最好的），间接标记是这个棋局是黑白输赢结果</li><li>online learning or 反复学习</li><li>输出是有反馈，对模型进行奖励机制</li></ul></li><li>基于规则的模型：人定义”特征“，人定义特征和输出之间的关系</li><li>基于统计的模型：人定义”特征“，模型确定特征和输出之间的关系（特征工程）<ul><li>cat？= 0.1*毛色+0.2*耳朵形状+0.3*眼睛形状 …</li></ul></li><li>深度学习模型：人不定义”特征“，模型确定原始信息和输出之间的关系（可以达到 end-to-end model）（人类选择的特征未必是最好的）<ul><li>深度学习可以发现特征（通过是神经网络学习原始信息获得高阶特征，一些人类未必发现的特征）</li><li>越深越能发现复杂特征</li></ul></li></ul></li><li><p>AI的一些重要问题</p><ul><li><p>标记、model、feature</p></li><li><p>什么是好模型？</p><ul><li><p>（泛化能力）描述性，但难以具体化，不可计算（欠拟合Underfitting）</p></li><li><p>（性能）具体可计算，适合范围小，描述性差（过拟合<strong>Overfitting</strong>）（模型复杂性越高容易过拟合）（难以避免）</p></li><li><p>两者折中比较难</p></li><li><p>机器学习的最终目标是在未知数据上效果最好</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/overfitting.png" alt="overfitting" style="zoom: 50%;"></p></li><li><p>严格上，数据需要分 训练集，开发集，测试集（避免虚假结果）</p></li><li><p>”成功就是最大的失败“（越成功会越保守，而事物是发展的，会在新事物上会越失败）</p></li><li><p>面对企业：</p><ul><li>基于规则的模型：问题简单，大量已知知识（可解释性好；基于人归纳，会比较抽象，越抽象越鲁棒性）</li><li>基于统计的模型：数据量不大，有一些明确的特征（可解释性一般，但都能猜到；基于数据归纳，不够抽象）</li><li>深度学习模型：数据量<strong>大</strong>，算力高，没有明确特征，先验知识缺乏（黑箱子，可解释性<strong>差</strong>）</li></ul></li><li><p>面对科研：越复杂越好</p></li></ul></li><li><p><strong>Global Knownledge 世界知识</strong>（人工智能选特征选模型，仍需要辅助的知识（经验知识，常识））</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/global_knownledge.png" alt="global knownledge" style="zoom: 50%;"></p><ul><li>人的学习不需要太多数据样本（小样本学习）</li><li>人可以小样本学习，也是有经验知识（先验知识），但儿童学习语言无法解释（儿童无先验知识，”大脑有普遍语法存在“）</li><li>有了先验知识，就可以对训练样本要求少一些（小样本学习），只学习特殊的知识即可。</li></ul></li><li><p><strong>Explainable</strong> 可解释性</p><ul><li><p>为什么模型会这么决策？</p></li><li><p>有了可解释性，可追溯源头，从根本上改进它。</p></li><li><p>自动驾驶事故率低于人类司机，为什么我们却不信任它？</p><p>（因为自动驾驶出事故的原因不可解释，出事故是概率性的）</p></li><li><p>刷脸支付，出错也是不可解释的</p></li></ul></li><li><p><strong>Ethics</strong> 伦理问题</p><ul><li><p>若有意识的机器人是否拥有人权？</p></li><li><p>AI通过用户的非隐私数据获得隐私数据</p><p>（识别用户的性格或者需求，左右用户做出选择（尤其在选举或者个性化推荐））</p><p>（搜索引擎（<strong>主动获取信息</strong>）是用户信息入口，会影响国家整体发展）</p><p>（现在约50%信息是依照个性推荐（<strong>被动获取信息</strong>），会导致个性分化、信息茧房，会加大偏见和隔阂，局限在自己圈子，最终导致社会撕裂，不接受他人，无法全面认识世界）</p></li><li><p>若AI能预测一个人的犯罪概率，是否在犯罪前先控制他？</p></li></ul></li></ul></li></ol><h4 id="Two-Machine-Learning-Foundations-from-台大林軒田"><a href="#Two-Machine-Learning-Foundations-from-台大林軒田" class="headerlink" title="Two. Machine Learning Foundations from 台大林軒田"></a>Two. Machine Learning Foundations from 台大林軒田</h4><ol><li><p><strong>machine learning = sample data + blurry pattern + not easily programmable definition</strong></p></li><li><p>Data Mining -&gt; feature -&gt; Machine Learning (在机器学习选择特征时，尽量选取特征之间相关性小的特征，相关性越大，则越冗余，就没意义了)</p></li><li><p>Machine Learning use data to compute hypothesis g that approximates target f. (Machine Learning ∈ Statitics)</p></li><li><p><strong>Perceptron 感知器</strong>，h(x) = sign(Σwi xi - threshold) (i =1,2,…)</p><ul><li><p>模拟神经细胞，接收信号，整合起来（<strong>加权求和</strong>），接收整体的信号超过某个阈值，则激活神经细胞</p></li><li><p>将threshold融入权重w，作为w0，h(x) = sign(Σwi xi)  (i =0,1,2,…) = sign(w^T x)，大于0为正例，小于0为负例。（线性感知器）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Perceptron.png" alt="Perceptron" style="zoom:33%;"></p><ul><li><p>令 h(x)=0， x2为纵轴，x1为横轴，x2 = -w1/w2 * x1 - w0/w2，右图分类效果较好（w1比w2大，即x1特征比x2特征更重要）</p></li><li><p>PLA  Perceptron Learning Algorithm 寻找最优划分的线性函数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_1.png" alt="update weight" style="zoom:43%;"></p></li><li><p>前提是线性可分的，则可以在有限步内停止，每次调整都会更接近完美分类面；若非线性，则无法停止。(大多数情况是非线性的，有noise；可用pocket算法，在非线性情况下，在一定调整步数下，选择错误率最低的结果)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_3.png" alt="weight update" style="zoom: 50%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_2.png" alt="PLA" style="zoom:43%;"></p></li></ul></li><li><p>perceptrons <-> linear (binary) classifiers</-></p></li></ul></li><li><p><strong>Types of Learning</strong> 机器学习的类型</p><ul><li>output space<ul><li>binary or more binary <strong>Classification</strong> （预测类别，划分样本）</li><li><strong>Regression</strong> （预测一个实数，样本的拟合问题，连接样本）<ul><li><strong>可以用回归任务做分类</strong></li><li>先算出一个数，设定阈值，然后可以分类</li></ul></li><li><strong>Structured</strong> learning (一维（序列结构，语法结构学习），二维（图结构），三维（蛋白质结构，分子结构)）</li></ul></li><li>data label<ul><li>supervised, semi-superivised, unsupervised, reinforcement</li></ul></li><li>protocol f =&gt; (x,y)<ul><li><strong>Batch</strong> Learning 批量学习（大量样本）</li><li><strong>Online</strong> Learning 在线学习（在线 =&gt; 持续学习，不断接收数据（少量），更新模型）<ul><li>Online + Batch，先大批量数据学习一个模型，再持续接收少量数据，更新模型（更新模型部分，并不是完全重新学习，否则就是多次批量学习了）</li><li>垃圾邮件分类，先训练通用的模型，根据用户个性再调整，形成个性化（每次再垃圾箱找到需要的邮件，即分错样本，就会为模型产生少量数据）（是否垃圾邮件，对每个人的意义不一样）</li><li>PLA，Reinforcement Learning</li></ul></li><li><strong>Active</strong> Learing 主动学习<ul><li>属于一种 Online Learning</li><li>同样基于少量样本调整模型，但Active Learning 模型主动向用户获取数据，Online Learning 是被动获得数据</li><li>垃圾邮件分类，若删除多个同用户的邮件，Active Learning会提问是否标记其邮件为垃圾邮件，若是，立即标记该用户为重要特征；而Online Learning则是等待用户标记垃圾邮件，需要多次标记才可以。</li></ul></li></ul></li><li>input space<ul><li><strong>concrete</strong> features 具体特征（物理意义明确）</li><li><strong>Raw</strong> features 原始特征（图片的像素，亮度，黑白）</li><li><strong>Abstract</strong> feature 抽象特征（没用任何物理意义） </li></ul></li></ul></li><li><p><strong>Feasibilityof Learning</strong> 学习的可行性</p><ul><li><p>是否可以学习知识？</p></li><li><p>训练样本是有限的，无法保证能学习到最好的 f()，只能逼近</p></li><li><p>Hoeffding’s Inequality，模型在训练样本的错误率v，模型在整体样本错误率u</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/hoeffding.png" alt="hoeffding" style="zoom:43%;"></p><ul><li>“v = u” is probably approximately correct (PAC)</li></ul></li><li><p>在训练集效果好，在测试集的效果也会好的概率？</p><ul><li><p>有M个候选函数，则错误率就很大，则过拟合。（前提是数据在候选函数之间相互独立（线性无关））</p></li><li><p>若M个候选函数中存在线性相关的候选函数，即不是<strong>相互独立</strong>，则M不是无穷大，则有希望减少过拟合</p></li><li><p>Ein 测试集错误率，Eout未知数据错误率（机器学习做两件事，模型在训练集使Ein变小，再使Ein和Eout尽可能相等）</p></li><li><p>样本N越大，则结果越可靠</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pac.png" alt="PAC" style="zoom:43%;"></p><ul><li><p>相同数据（x1 x2）喂入两个候选函数（两条红线），得到结果一样，则两个候选函数<strong>相关</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/error.png" alt="error" style="zoom:43%;"></p></li><li><p>对PLA而言，分类是候选函数将样本一分为2（则分类相同的候选函数相关，则两个函数视为等价），n个样本，最多也是2^n个<strong>类别</strong>，即<strong>M=2^n</strong>，则<strong>M不是无穷大</strong>的（对所有问题，都不是无穷大的）</p></li><li><p>实际情况是 <strong>M&lt;&lt;2^n</strong></p></li><li><p>growth function 成长函数，给定n个样本，返回实际可分类别数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/growth_funtion.png" alt="growth_funtion" style="zoom:43%;"></p></li><li><p>问题不同，成长函数不一样（成长函数上限则为break point突破点）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different.png" alt="different" style="zoom:43%;"></p></li><li><p>机器学习要达到的目标</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Ein_Eout.png" alt="Ein_Eout" style="zoom:43%;"></p></li><li><p>参数的个数是决定模型复杂度的核心指标</p></li><li><p>自由度=这个模型的有多少参数，一个参数是一个维度，参数越多，自由度越高</p><ul><li><p>VC维=参数个数</p></li><li><p>VC维 the formal name of maximum non-break point</p></li><li><p>break point是成长函数的上限，k决定了成长函数的最多参数数量，从而决定了vc维</p></li><li><p>dvc = min_k-1</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/VC_dimension.png" alt="VC_dimension" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li></ol><ol start="7"><li><p><strong>Regression</strong> 回归</p><ul><li><p>Noise: 样本标记错误（正例标记成反例）</p></li><li><p>Probabilistic 概率函数：对输出不是确定性的，都是概率性的（容忍存在Noise）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different_error.png" alt="different_error" style="zoom:43%;"></p></li><li><p>分类：判断sample是否符合目标f()</p></li><li><p>回归：让sample离目标f()越近（error用平方，是在最低点是可微的，用绝对值是不可微的）</p><ul><li>用回归无法直接做分类，但可以缩小分类的范围，err_0/1 &lt;= err_sqr</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/logistics_regression.png" alt="logistics_regression" style="zoom:43%;"></p></li><li><p><strong>logistic regression</strong> （非线性回归）</p><ul><li>err 需要计算两个概率分布的差值（KL散度）</li><li>当数据为病人的特征数据，但患病只有0/1（患病与不患病），则需要求出整体患病的概率分布。</li><li>极大似然估计：给定输入输出，确定一个分布。</li><li>用logistics regression 训练一个分布接近极大似然估计的分布</li></ul></li><li><p><strong>Gradient Descent</strong> 梯度下降，用于update weight（随机梯度下降，是随机采样点，大方向和直接梯度下降是一致的，但复杂度翻倍）（步长=学习率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Gradient Descent.png" alt="Gradient Descent" style="zoom: 50%;"></p></li></ul></li><li><p><strong>Multiclass Classification </strong>多分类问题</p><ul><li><p>四分类拆成多个二分类问题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Multiclass Classification.png" alt="Multiclass Classification" style="zoom:50%;"></p><ul><li><p>正例大大少于负例，样本不平衡</p></li><li><p>四分类分成两类，形成一对一对的分类</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pairwise classifier.png" alt="pairwise classifier" style="zoom:43%;"></p></li></ul></li><li><p>Nonlinear Transform 训练分类</p></li><li><p>将非线性转成线性</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/nonlinear.png" alt="nonlinear" style="zoom:43%;"></p></li></ul></li><li><p><strong>Regularization</strong> 正则化</p><ul><li><p>缩小高次空间，控制在一定区间内，防止过拟合同时仍具有高次空间的能力</p></li><li><p>降低复杂度，减轻过拟合（缩减候选函数的个数M）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/regularization coefficient.png" alt="regularization coefficient" style="zoom:43%;"></p></li><li><p>可加入loss function一起训练正则化</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Regression.png" alt="Regression" style="zoom:43%;"></p></li><li><p>L1（有一堆特征，但有些是无用的，用L1可去除一些无用特征），L2（常用，比较柔和）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/l1_l2.png" alt="l1_l2" style="zoom:43%;"></p></li></ul></li></ol><h4 id="Three-NLP"><a href="#Three-NLP" class="headerlink" title="Three. NLP"></a>Three. NLP</h4><ol><li><p>what is NLP?</p><ul><li><p>Turning Test 基于 NLP</p></li><li><p>感知智能 -&gt; CV</p></li><li><p>认知智能 -&gt; NLP</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_nlp.png" alt="text_nlp" style="zoom:43%;"></p></li><li><p>语义角色标注（施动者，受动者，描述）</p></li><li><p>理解 <strong>NLU: L -&gt; R</strong>;  生成 <strong>NLG: R -&gt; L</strong></p></li><li><p>让机器get到语言中的meaning</p></li><li><p>NLP 中不允许存在歧义（程序语言没有歧义，自然语言是存在歧义的）</p><ul><li>NLP 需要解决语义之间歧义</li></ul></li><li><p>创造一个 <strong>interlingua 中间语</strong>，允许所有自然语言均可以翻译成 interlingua，自然语言是动态的，则 interlingua 几乎不可创造<em>（没有 interlingua，就很难表示 meaning）</em></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_translate.png" alt="ml_translate" style="zoom:50%;"></p></li><li><p>对自然语言做语法分析，只能越来越逼近 interlingua</p></li><li><p>在深度学习，则用 respentation / embedding 向量来表示语义的 meaning</p></li><li><p>知识图谱 -&gt; 让机器获取先验知识（前期需要NLP处理数据挖掘实体之间的关系）</p></li></ul></li><li><p>NLP’s hard point</p><ul><li><p>“哈士奇”不管在哪都是“哈士奇”；“同志”在不同语境表示不一样</p></li><li><p>歧义 &amp; 动态</p></li><li><p><strong>语言的本质是谎言</strong>。真话：能正确反映真正事实的话，谎言：不能正确表达真正事实的话（同一句话，不同人理解不一样，都带有各自的偏见，所有不能正确表达真正的事实）</p></li><li><p>符号系统：人类创造符号来表达信息，语言是其中一种。</p></li><li><p><strong>所指：meaning；能指：表达meaning的工具</strong></p><ul><li>所指，能指之间规律不可寻，具有任意性，但在特定的时间，地点可以有局部确定的规律（人类可根据古代壁画符号风格判断年份）</li></ul></li><li><p>语言的任意性所导致的歧义性、动态性，乃至非真实性是语言处理的根本性困难（非真实性：描述抽象概念，没有实体对应（白马非马））</p></li><li><p>基本歧义（语法结构、词义、词性…）</p></li><li><p>旧知识 -&gt; 先验知识 -&gt; 先验知识 + 小样本 -&gt; 新知识</p></li><li><p>乔姆斯基：存在一些普遍语法，并非局部的，是所有语言学的共性</p><ul><li>例如小孩子就可以小样本学习语言，但没有先验知识（并非多次听到语言，毕竟是教不会动物说话）</li><li>“递归”，语言存在递归结构</li><li>递归存在（语言/语义）自指结构（是产生悖论的主要原因之一）（“这句话是错的”）</li></ul></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics.png" alt="rule_statistics" style="zoom: 49%;"><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics_2.png" alt="rule_statistics" style="zoom:32%;"></p><ul><li>只需要判断<strong>特定词汇</strong>，就可以使用CNN（只需要判断句子中有“高兴”词语，就可以判定情感），但判断整句话的所有词，则需要使用RNN</li></ul></li><li><p>语言理解的关键：<strong>背景知识</strong>（上下文）</p><ul><li>在NLP中，所有词都是有歧义，必须要有一个固定场景，才能确定一个词的意思</li><li>语用即语义：词是在场景怎么用的，就是语义</li><li>例如描述人，重点在人与其他事物的关系，并不是人体内在结构</li><li>graph 是表示事物之间的关系</li><li>学习基于事物之间关联，相关性</li><li><strong>相关性</strong>恰恰是破解<strong>任意性</strong>（歧义&amp;动态）的钥匙！</li><li>NLP 用上下文约束自然语言的任意性</li></ul></li><li><p>表示学习（利用上下文表示语义）</p><ul><li>基于特征的可解释表示</li><li>基于深度学习编码的不可解释表示</li><li>与其他非语言对象相结合的表示：如网络表示学习</li><li>作为其他学习模型的输入：深度学习模型、线性学习模型</li></ul></li><li><p><strong>word embedding</strong> 词向量表示学习：<strong>基于上下文用向量表示这个词</strong>，向量则可以计算的</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_embedding.png" alt="word_embedding" style="zoom:43%;"></p><ul><li>坑：不在一个语义空间的词不能像比较；即使向量数值一样，意义不一样，词就是不一样（人名和电影不在一个空间）</li><li>两个word embedding之间差值，可以表示两者的关系</li></ul></li><li><p>预训练模型（通用知识的自动获取）：表示学习、语言模型、针对特定任务的与训练</p><ul><li><p>基于很大数据学习最基本的（几何）元素，作为其他模型的输入</p></li><li><p>属于传统机器学习（已知最基本元素，用于训练提取）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pre_training.png" alt="pre_training" style="zoom:43%;"></p></li><li><p>深度学习（黑盒子，不知道特征是否重要，用深度学习抽取特征）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DL.png" alt="DL" style="zoom:43%;"></p></li></ul></li><li><p>填补先验知识和模型能力（模型搜索空间）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_Process.png" alt="ml_Process" style="zoom:43%;"></p></li></ul></li><li><p>NLP基本任务</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/NLP_task.png" alt="NLP_task" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/essential model.png" alt="essential model" style="zoom:43%;"></p></li><li><p>NLP’s essential models (<strong>Linear Models</strong>)</p><ul><li><p><strong>EM算法</strong>（<strong>Expectation-maximization algorithm</strong> 期望最大化算法）</p><ul><li><p>“猜测隐藏在文字背后的信息”</p></li><li><p>在概率模型中寻找参数<strong>最大似然估计</strong>或者<strong>最大后验估计</strong>的算法, 其中概率模型依赖于无法观测的<strong>隐性变量</strong>。</p><ul><li>观察结果依赖于隐藏状态。只能看到观察结果,看不到隐藏状态。如何知道隐藏状态生成观察结果的概率(模型参数)?</li></ul></li><li><p>知道其中一个，可以互相推导 （有输入输出（观察值是输入），做监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM.png" alt="EM" style="zoom:43%;"></p></li><li><p>当两个都不知道（即 只有输入，没有输出，则为无监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM algorithm.png" alt="EM algorithm" style="zoom:43%;"></p></li><li><p>EM算法一定能收敛，但只能局部最优，无法全局最优，可通过尝试多个初始值（瞎猜参数）来改进最优效果</p></li></ul></li><li><p><strong>ME (Maximum Entropy) 最大熵模型</strong></p><ul><li><p>“用特征去束缚语言的任意性”</p></li><li><p>信息熵：用来描述信息的不确定性</p><ul><li><p><strong>一个物理系统越无序（无能力流动，则无序），则能量越小，信息熵越大；越有序（能力按有序方向流动），能量越大，信息熵越小</strong></p></li><li><p>一个体系的能量达到完全均匀分布时，这个系统的熵就达到最大值 </p></li><li><p>封闭系统总熵时不断增大的（能量传递完成，达到均衡），局部会出现熵减小的情况</p></li><li><p>能力来自于差异（判读是否有动能/势能/热能，对比其周围是否存在差异，有差异才存在能力流动）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Entropy.png" alt="Entropy" style="zoom:43%;"></p></li></ul></li><li><p><strong>分布均匀 &lt;=&gt; 熵最大 =&gt; 最合理结果</strong></p></li><li><p>信息熵越大信息量越大（信息的不确定性越强）（熵越大的系统承载信息的能力越大）</p></li><li><p>最大熵：保留全部的不确定性,把风险降到最小</p></li><li><p>最大熵原理指出,需要对一个随机事件的概率分布进行预测时,我们的预测应当<strong>满足全部已知的条件</strong>，而<strong>对未知的情况不要做任何主观假设</strong>。在这种情况下,概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。</p></li><li><p>条件熵</p><ul><li><p>在给定输入的情况下，计算输出概率，实际上在计算以输入为条件的条件熵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Condition Entropy.png" alt="Condition Entropy" style="zoom:43%;"></p></li></ul></li><li><p>最大熵模型：求解带约束(特征函数)的最优化问题</p><ul><li>引入拉格朗日乘子,定义拉格朗日函数,转化为特征加权和</li></ul></li></ul></li><li><p><strong>隐马尔可夫链 HMM</strong></p><ul><li><p>“语言是一个串”</p></li><li><p>一个隐状态序列产生一个观察值序列。每个隐状态依赖于前一个隐状态</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM.png" alt="HMM" style="zoom:43%;"></p><ul><li>转移概率：隐状态之间转移（转换）的概率</li><li>发射概率：隐状态产生观察值的概率</li></ul></li><li><p>HMM能解决的问题</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Model.png" alt="HMM_Model" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Trasfer.png" alt="HMM_Trasfer" style="zoom:43%;"></p><ul><li>有监督的情形：知道观察序列对应的状态值，直接对训练语料进行统计计数即可，即最大似然</li><li>无监督的情形：不知道观察序列对应的状态值，只知道可能的状态集合（用EM）</li></ul></li><li><p><strong>生成与判别</strong></p><ul><li><p>“纵观全局 or 聚焦一处” 分别对应 生成 or 判别</p></li><li><p>机器学习有两大类模型：生成式模型、判别式模型</p></li><li><p>生成模型：学习得到<strong>联合概率分布P(x,y)</strong>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。</p></li><li><p>判别模型：学习得到<strong>条件概率分布P(y|x)</strong>，即在特征x出现的情况下标记y出现的概率。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/generation and discrimination.png" alt="generation and discrimination" style="zoom:43%;"></p><ul><li>已知生成模型可以得到各个判别模型；已知判别模型无法得到生成模型，除非已知所有可能的判别关系</li><li>判别模型：<ul><li>优点：所需数据量小,计算量小，对单一类别判定准确率高。可随意增加新特征。</li><li>缺点：无法全局优化，只能完成目标任务，没有提供额外信息的潜力，应用范围受限。</li></ul></li><li>生成模型：<ul><li>优点：信息全面，可实现全局优化。</li><li>缺点：所需数据量大，计算量大，增加新特征的计算成本高。</li></ul></li></ul></li><li><p>为什么HMM是生成式模型? </p><ul><li>建模所有状态之间的转移关系和状态与所有词汇的发射关系</li><li>所有隐状态概率都要计算</li></ul></li><li><p>最大熵是判别式模型</p><ul><li>给定条件，计算结果（计算条件概率）</li></ul></li></ul></li><li><p><strong>MEMM 最大熵隐马：最大熵 + HMM</strong> (偏向最大熵)</p><ul><li><p>在解决序列标注问题时，HMM的输入信息只有参数(π, A, B)和观察序列(即每个状态对应的字)，<strong>没有办法接受更丰富的特征</strong>(例如更多的上下文文字等)。为了解决这个问题，提出了MEMM模型。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM.png" alt="MEMM" style="zoom: 33%;"></p></li><li><p>MEMM模型在预测当前状态时,将前一个状态和与当前观察值相关的一组特征一起做为最大熵模型的输入,来预测当前状态。MEMM与HMM不同，是判别式模型。（有点像RNN）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM model.png" alt="MEMM model" style="zoom:43%;"></p></li><li><p>HMM计算产生整个观察序列的最优状态序列，是全局最优。</p></li><li><p>MEMM计算单个观察值判定的单个最优状态，是局部最优。</p></li></ul></li><li><p><strong>CRF (Conditional Random Field) 条件随机场 (域)</strong></p><ul><li><p>“在更加宽广的上下文上进行判别”</p></li><li><p>HMM是特殊的CRF</p></li><li><p>CRF计算由整个观察序列判定的最优状态序列，是<strong>全局最优</strong>。其中，每个可能的状态序列的概率这样计算：对于序列中的每个状态计算一组特征函数值，然后计算所有状态的特征函数值之和并归一化。</p></li><li><p>判别式模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CRF.png" alt="CRF" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_MEMM_CRF.png" alt="HMM_MEMM_CRF" style="zoom:43%;"></p></li></ul></li><li><p><strong>SVM (support vector machine) 支持向量机</strong></p><ul><li><p>“从线性到非线性分类”</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM.png" alt="SVM" style="zoom: 33%;"></p></li><li><p>支持向量机(SVM)同最大熵一样是一种常用的线性(Log linear)分类器。</p></li><li><p>SVM求使得Margin最大的分类面,并将<strong>Margin上的向量称为支持向量</strong>（绿边上的向量点）。</p></li><li><p>通过计算样本与哪一类支持向量的内积更大来判断样本类别。</p></li><li><p>对于线性不可分的样本集,可以将其投射到高维空间中来分割。</p></li><li><p>SVM用核函数来方便计算高维空间中样本点之间的内积：核函数可以在原空间中计算高位空间中的内积。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM model.png" alt="SVM model" style="zoom:43%;"></p></li><li><p><strong>kernel function</strong> （在原维度计算kernel函数就可视为在高维做计算）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/kernel function.png" alt="kernel function" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Topic Models</strong> 主题模型</p><ul><li><p>LSA, pLSA, LDA</p></li><li><p>判断图片相似度：匹配像素</p></li><li><p>判断两篇文章相似度：匹配词汇，每篇文章词汇分布</p><ul><li><p>或者匹配词汇出现频率</p></li><li><p>下图有误，行不全是D1，是表示不同文章</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_match.png" alt="word_match" style="zoom:43%;"></p></li><li><p>词汇在文章出现，也相当于文章的上下文</p></li><li><p>避免选择类似冠词the出现频率过高，或者频率过低但又很重要的词汇</p></li><li><p>逆文档频率（term frequency-inverse document frequency, TF-IDF）</p><ul><li><p>TF-IDF 越高，则改词汇就很重要</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/TF-IDF.png" alt="TF-IDF" style="zoom:43%;"></p></li><li><p>但这个词汇向量只在乎了词语的出现频率（属于基于词袋），忽略了语义，词汇的信息（词汇序列才产生信息，与DNA碱基对序列同理）</p></li></ul></li><li><p>主题模型是基于词袋的模型</p></li><li><p>可用于检索，是否与某个‘词’相关，可以使用，但想知道与这个‘词’更细节的语义则不行</p></li><li><p><strong>潜在语义分析LSA（Latent Semantic Analysis）</strong></p><ul><li><p>主题就是一个潜在语义</p></li><li><p>n个文章 -&gt; k个主题 -&gt; m词汇</p></li><li><p>文章-主题矩阵（文章涉及主题分布），主题-词汇矩阵（主题涉及词汇分布）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSA.png" alt="LSA" style="zoom: 25%;"></p></li><li><p>拥有右边矩阵，想得到左边两个矩阵</p></li><li><p>SVD 奇异值分解（矩阵分解）（存在负值，负值不符合物理意义的解释）</p></li><li><p>LSA只是形式上拟合了文档-主题-词汇的关系,但并没有真正表达这种关系</p></li></ul></li></ul></li><li><p><strong>概率潜在语义分析 pLSA</strong></p><ul><li><p>在LSA基础上，输出概率值，才可赋予物理意义</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA.png" alt="pLSA" style="zoom:43%;"></p></li><li><p>箭头表示依赖关系</p></li><li><p>P(d) 文章被抽中的概率（属于先验概率）</p></li><li><p>P(z|d) 给定文章，主题的概率，P(w|z) 给定主题，词汇的概率（两个属于后验概率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA model.png" alt="pLSA model" style="zoom:43%;"></p></li><li><p>求出所有边的概率，则可以得到 文本-主题矩阵，主题-词汇矩阵</p></li><li><p><strong><em>只知道观察值，想知道内部的两个隐状态，则用 EM算法</em></strong></p></li></ul></li><li><p><strong>潜在狄利克雷分配 LDA</strong>（latent Dirichlet allocation）</p><ul><li><p>pLSA中单词和主题的先验分布都假设是均匀分布的，也就是假设我们对他们的先验分布一无所知。这种假设使得pLSA比较容易出现过拟合。</p></li><li><p>文档生成话题和话题生成单词的过程是典型的多项分布,在贝叶斯学习中，狄利克雷分布常作为多项分布的先验分布使用 LDA 将狄利克雷分布做为话题和单词生成的先验分布</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA.png" alt="LDA" style="zoom:43%;"></p></li><li><p>给文章根据狄利克雷分布随机分配个主题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA model.png" alt="LDA model" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Deep Learning Models</strong> 深度学习模型</p><ul><li><p>属于生成式模型</p></li><li><p>线性模型：所有特征加权求和，通过激活函数，则成为线性感知机</p><ul><li>logistic（缩小输出范围）</li><li>softmax（所有值变为概率分布，总值=1）</li><li>KL散度（交叉熵）评估输出概率分布和真实分布的差距</li></ul></li><li><p>人工神经网络</p><ul><li>神经元激活规则<ul><li>主要是指神经元输入到输出之间的映射关系,一般为非线性函数。</li></ul></li><li>网络的拓扑结构<ul><li>不同神经元之间的连接关系。</li></ul></li><li><p>学习算法</p><ul><li>通过训练数据来学习神经网络的参数。</li></ul></li><li><p>ANN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ANN.png" alt="ANN" style="zoom: 50%;"></p></li><li><p><strong>全连接前馈神经网络</strong></p><ul><li>在前馈神经网络中,各神经元分别属于不同的层。整个网络中无反馈，信号从输入层向输出层单向传 播,可用一个有向无环图表示。</li><li>全连接复杂度高，发挥全部能力</li><li>容易过拟合（使用dropout价格低复杂度，避免过拟合）</li><li>通用近似定理<ul><li>对于具有线性输出层和至少一个使用 “挤压” 性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够,它可以以任意精度来近似任何从一个定义在实数空间中的有界闭集函数</li><li>“挤压” 性质：将输入数值范围挤压到一定的输出数值范围 </li><li>不是“挤压”性质的就是线性的</li></ul></li><li>反向传播更新参数</li></ul></li></ul></li><li><p>深度学习三个步骤</p><ul><li>定义网络 -&gt; 损失函数 -&gt; 优化</li></ul></li><li><p>梯度爆炸</p><ul><li>若初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后,可能会导致求导的结果很大，形成梯度爆炸</li></ul></li><li><p><strong>梯度消失</strong></p><ul><li><p>若使用标准化初始w，那么各个层次的相乘都是0-1之间的小数,而激活函数f的导数也是0-1之间的数,其连乘后,结果会变的很小，导致梯度消失</p></li><li><p>Activation Function</p><ul><li>sigmoid是非0均值（相当于加了一个偏置），还计算指数（指数计算相对复杂）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/activation function.png" alt="activation function"></p></li><li><p><strong>CNN</strong></p><ul><li><p>让每个神经元不代表一个像素，而是代表一个区域，而且，区域更容易捕捉局部特征</p></li><li><p>生物学上局部感受野</p></li><li><p>结构特点：<strong>局部连接，权重共享</strong></p></li><li><p>同时使用多组卷积核,每个负责提取不同特征</p></li><li><p><strong>padding</strong> 在图片外面填充一圈0（在没有padding，则边缘像素被访问概率相对对较低）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/conv.png" alt="conv" style="zoom: 33%;"></p></li><li><p><strong>pooling 池化</strong>：卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CNN.png" alt="CNN" style="zoom:43%;"></p></li><li><p>优点：善于提取特征、适用于分类、可并行、效率高</p></li></ul></li></ul></li><li><p><strong>RNN</strong></p><ul><li><p>假设每次输入都是独立的，也就是说每次网络的输出只依赖于当前的输入</p></li><li><p>一个网络的输出做为另一个网络的输入</p><ul><li>y3是取决于前面y1, y2 (但重要程度是不一样的，越近越重要（但不是什么时候都符合这个，在序列较长时，存在远距离相关，则需要LSTM/GRU）)，但没有考虑到后者y4, y5（若需要考虑上下文，则需要双向RNN）</li><li>最后的y则包含所有信息</li><li>马尔科夫链每个状态只由前一个状态影响 而RNN每一个节点由前面所有节点影响</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN.png" alt="RNN" style="zoom:43%;"></p></li><li><p>LSTM （长短期记忆神经网络）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p>GRU （降低复杂度，能达到LSTM的效果）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GRU.png" alt="GRU" style="zoom:43%;"></p></li><li><p>各种类型RNN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN model.png" alt="RNN model" style="zoom:43%;"></p></li><li><p>层叠循环神经网络：可以捕捉更加抽象的内涵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deep RNN.png" alt="deep RNN" style="zoom:43%;"></p></li><li><p>双向循环神经网络：可以捕捉两侧的上下文信息</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/BRNN.png" alt="BRNN" style="zoom:43%;"></p></li><li><p>递归神经网络 Recursive Neural Network</p><ul><li><p>自然语言的句法结构</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Recursive NN.png" alt="Recursive NN" style="zoom:43%;"></p></li><li><p>递归神经网络实在一个有向图无循环图上共享一个组合函数</p></li><li><p>叶子节点为输入</p></li><li><p>对于有歧义的句子（句法的歧义，<strong>不知道（形容）词语指向哪个主语</strong>）或者图片（图片中<strong>物品属于哪个主人</strong>），（需要从属关系的时候）可以用递归神经网络，用树的结构区别句法的结构和物品所属</p></li><li><p>可以退化为循环神经网络（属于RNN的特例）</p></li></ul></li><li><p>优点：善于累积序列信息、适用于序列标注或编码、不可并行、效率低</p></li></ul></li><li><p><strong>Attention</strong></p><ul><li><p>基于RNN的机器翻译中的注意力现象：源语言词汇对每个目标语的依赖程度不同</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/autoenocder.png" alt="autoenocder" style="zoom:43%;"></p></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/attention.png" alt="attention" style="zoom:43%;"></p></li></ul></li><li><p>different network</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different network.png" alt="different network" style="zoom:43%;"></p></li><li><p>word embedding</p><ul><li>one-hot(独热)编码<ul><li>向量维度为数据库中总词汇数,每个词向量在其对应词处取值为1，其余处为0</li><li>存在的问题: 维度灾难，语义鸿沟</li></ul></li><li><p>分布式表示 Distributed Representation</p></li><li><p>假设一个单词的语义和这个单词的上下文是相关的，我们可以使用这个单词的上下文来表示这个单词的语义信息</p></li><li><p>延申：语义相似的单词也应该具有相似的上下文。</p><ul><li><p>上下文(context): 在附近出现的所有单词的集合。–&gt; 窗口window</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DR.png" alt="DR" style="zoom:43%;"></p></li><li><p>如何训练分布式</p><ul><li><p>共现矩阵 Co-occurrence Matrix</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/COM.png" alt="COM" style="zoom:43%;"></p></li><li><p>潜在语义分析 LSA (Latent Semantic Analysis)</p></li></ul></li><li><p>奇异值分解 Singular Value Decomposition</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVD.png" alt="SVD" style="zoom:43%;"></p></li><li><p>前馈神经网络语言模型 FNNLM</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM.png" alt="FNNLM" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM_2.png" alt="FNNLM_2" style="zoom:43%;"></p></li><li><p>Word2Vec</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word2vec.png" alt="word2vec" style="zoom:43%;"></p></li><li><p>CBOW (continuous bag-of-words)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CBOW.png" alt="CBOW" style="zoom:43%;"></p></li><li><p>Skip-Gram</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/skip-gram.png" alt="skip-gram" style="zoom:43%;"></p></li></ul></li></ul></li><li><p>Pre-training 预训练 </p><ul><li>先验知识 -&gt; 学习模型 &lt;- 经验数据<ul><li>模型选择</li><li>参数设定</li><li>领域知识</li></ul></li><li>预训练模型提供了先验知识，不需要知道后面的任务目标，获得embedding</li><li>越深层，特征越具体</li><li>小样本学习 = 预训练 + 微调</li><li>目标任务与云训练模型最好是同类型的，效果会更好</li><li>RNN:不能准确捕捉远距离依赖，不能并行<ul><li>解决并行问题:给每个词编码，然后在词编码上用NN输出一个向量，NN可以并行。可以并行的网络可以做的更深</li><li>解决远距离依赖问题：给每个词编码的时候用注意力机制</li></ul></li><li>Transformer是一个典型的Encoder-Decoder模型，最初用于机器翻译。其中间部分(Encoder的输出)，是一个句子的向量表示。因此,Transformer的Encoder部分可以用作句子向量的预训练模型。</li></ul></li><li><p><strong>GNN</strong></p><ul><li><p>卷积模型、序列模型</p></li><li><p>无论卷积还是序列模型，实际上都假定输入对象的结构是一个<strong>均匀</strong>的网络。换言之，就是基本元素(像素、词汇)之间的关系结构是处处相同的。（符合欧式距离）</p></li><li><p>但是，现实中元素之间的结构并不总是均匀的。而任意图才是元素结构的一般化表示,网格与序列都只是一般图的特例</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/non_eu_and eu.png" alt="non_eu_and eu" style="zoom:43%;"></p></li><li><p>具有一般图结构的对象十分广泛，都无法用普通的CNN和RNN有效处理</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_data.png" alt="graph_data" style="zoom:43%;"></p></li><li><p>若使用基于局部特征的方法来处理，一般图，如何定义卷积核的尺寸和方法? (<strong>CNN -&gt; GCN</strong>)</p></li><li><p>若使用序列的方法来处理一般图，如何给出序列的行走路线? (<strong>RNN -&gt; deepwalk</strong>)</p></li><li><p>通过NN获得<strong>embedding</strong>：包含 feature information + structure information</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_process.png" alt="graph_process" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/gnn_process.png" alt="gnn_process" style="zoom:43%;"></p></li><li><p><strong>SDNE (Structural deep network embedding)</strong></p><ul><li><p>同时优化一阶和二阶相似度</p></li><li><p>每个结点用一个自编码器来重建领域信息,从而建模二阶相似度</p></li><li><p>节点之间使用拉普拉斯特征映射(反映节点之间的距离)来惩罚使得相邻节点距离较远的编码结果</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SDNE.png" alt="SDNE" style="zoom: 50%;"></p></li></ul></li><li><p><strong>DeepWalk</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deepwalk.png" alt="deepwalk" style="zoom:43%;"></p></li><li><p><strong>Node2Vec</strong></p><ul><li><p>与DeepWalk的最大区别在于，node2vec采用有偏随机游走,在广度优先(bfs)和深度优先(dfs)图搜索之间进行权衡,从而产生比DeepWalk更高质量和更多信息量的嵌入</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/node2vec.png" alt="node2vec" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/bsf_dsf.png" alt="node2vec_" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/dsf_bsf.png" alt="dsf_bsf" style="zoom:43%;"></p></li></ul></li><li><p><strong>Metapath2vec</strong>: 异质性网络中的顶点表示</p><ul><li><p>随机路径必须符合预设的若干元路径(Metapath)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/metapath2vec.png" alt="metapath2vec" style="zoom:43%;"></p></li></ul></li><li><p><strong>LINE</strong>: explicitly preserves both first-order and second-order proximities.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LINE.png" alt="LINE" style="zoom:43%;"></p></li><li><p><strong>PTE</strong>: learn heterogeneous text network embedding via a semi-supervised manner.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/PTE.png" alt="{TE" style="zoom:43%;"></p></li><li><p><strong>GCN</strong></p><ul><li><p>以每个节点为核心，将其邻域设为卷积范围，卷积方法是汇聚邻居节点的信息做为核心节点的表示。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GCN.png" alt="GCN" style="zoom:43%;"></p></li></ul></li><li><p><strong>GAT</strong></p><ul><li><p>基本的GCN中邻居节点的权重是平均的。</p></li><li><p>GAT中邻居节点的权重是可以训练的参数。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GAT.png" alt="GAT" style="zoom:43%;"></p></li></ul></li><li><p>GCN和GAT 是Transductive learning: 训练语料包含待标注语料，标注在训练过程中完成。</p><ul><li>优点：质量高</li><li>缺点：扩展性差(标注新样本需要全局重新训练)</li><li>GCN和GAT的缺点：网络的任何变化都要重新进行全局训练 (类似word embedding)</li></ul></li><li><p><strong>GraphSage</strong> 是Inductive learning：训练语料不包含待标注语料，先训练获得模型,然后泛化到测试语料上。</p><ul><li><p>GraphSage学习一个由邻居节点形成中心节点表示的神经网络模型(聚合函数)</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage.png" alt="graphsage" style="zoom:43%;"></p></li><li><p>GraphSage是分层的，类似神经网络的层次。每一层的节点表示由前一层的邻居节点通过聚合函数获得。</p></li><li><p>随着层次的推进，每个结点实际上不仅可以获得邻居结点的信息，还可以获得更远距离的结点的信息。</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage2.png" alt="graphsage2" style="zoom:43%;"></p></li><li><p>GraphSage的参数学习需要设计一个损失函数。（有监督/无监督）</p></li><li><p>对于无监督学习，损失函数应该让临近的节点的拥有相似的表示。</p></li></ul></li><li><p>文本分类: <strong>Text-GCN 2019</strong></p><ul><li><p>以文档和词汇为结点构造异质性网络，训练获得文档的向量表示并分类到类别。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_gcn.png" alt="text_gcn" style="zoom:43%;"></p></li></ul></li><li><p><strong>关系抽取: 2018</strong></p><ul><li><p>以依存句法树做为GCN的输入图,得到词汇的表示,进而分类词汇是否为关系标记词</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/关系抽取.png" alt="关系抽取" style="zoom:43%;"></p></li></ul></li><li><p><strong>个性化推荐: 2018</strong></p><ul><li><p>建立用户-用户-物品关系图</p></li><li><p>在关系图上分别得到用户和物品的表示</p></li><li><p>基于用户和物品的表示建立Rating预测模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/个性化推荐.png" alt="个性化推荐" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;机器学习-machine-learning-from-TJU&quot;&gt;&lt;a href=&quot;#机器学习-machine-learning-from-TJU&quot; class=&quot;headerlink&quot; title=&quot;机器学习 machine learning from TJU&quot;&gt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>RNN_LSTM</title>
    <link href="http://yoursite.com/2020/05/06/RNN-LSTM/"/>
    <id>http://yoursite.com/2020/05/06/RNN-LSTM/</id>
    <published>2020-05-06T12:11:28.000Z</published>
    <updated>2020-05-06T12:18:07.061Z</updated>
    
    <content type="html"><![CDATA[<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><ul><li><p><strong>recurrent</strong> (it performs the same function for every input)</p></li><li><p>the output of the current input depends on the <strong>past one</strong> computation </p></li><li><p>RNN can use their <strong>internal state (memory)</strong> to process sequences of inputs</p></li><li><p>In RNN, all the inputs are <strong>related</strong> to each other (In other neural networks, all the inputs are independent of each other)</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/rnn.png" alt="rnn" style="zoom:50%;"></p></li><li><p>activation function is <strong>tanh()</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/tanh.png" alt="tanh" style="zoom:43%;"></p></li><li><p>output</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/yt.png" alt="yt" style="zoom:43%;"></p></li><li><p>Advantages</p><ul><li><strong>RNN</strong> can model sequence of data so that each sample can be assumed to be dependent on previous ones </li><li><strong>RNN</strong> are even used with convolutional layers to extend the effective pixel neighborhood. </li></ul></li><li><p>Disadvantages</p><ul><li>Gradient vanishing and exploding problems </li><li>Training an RNN is a very difficult task </li><li>It cannot process very long sequences if using <em>tanh</em> or <em>relu</em> as an activation function</li></ul></li></ul><h4 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM (Long Short-Term Memory)"></a>LSTM (Long Short-Term Memory)</h4><ul><li><p>LSTM is a modified version of RNN, which makes it easier to <strong>remember</strong> past data in memory </p></li><li><p>The <strong>vanishing gradient</strong> problem of RNN is resolved here </p></li><li><p>LSTM is well-suited to classify, process and predict <strong>time series given time lags of unknown duration</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p><strong>Input gate</strong> — discover which value from input should be used to modify the memory </p><ul><li><strong>Sigmoid</strong> function decides which values to let through <strong>0,1.</strong></li><li><strong>tanh</strong> function gives weightage to the values which are passed deciding their level of importance ranging from<strong>-1</strong> to <strong>1</strong> </li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/input_gate.png" alt="input_gate" style="zoom:43%;"></p></li><li><p><strong>Forget gate</strong> — discover what details to be discarded from the block</p><ul><li><strong>sigmoid</strong> function looks at the previous state(<strong>ht-1</strong>) and the content input(<strong>Xt</strong>) and outputs a number between <strong>0</strong> (<em>omit this</em>) and <strong>1</strong>(<em>keep this**</em>)<strong> for each number in the cell state </strong>Ct−1**. </li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/forget_gate.png" alt="forget_gate" style="zoom:43%;"></p></li><li><p><strong>Output gate</strong> — the input and the memory of the block is used to decide the output </p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/output_gate.png" alt="output_gate" style="zoom: 33%;"></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;recurrent&lt;/strong&gt; (it performs the same function
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Amino_acids_proteins</title>
    <link href="http://yoursite.com/2020/05/06/Amino-acids-proteins/"/>
    <id>http://yoursite.com/2020/05/06/Amino-acids-proteins/</id>
    <published>2020-05-06T11:17:00.000Z</published>
    <updated>2020-08-06T09:10:10.117Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Amino-acids-amp-proteins"><a href="#Amino-acids-amp-proteins" class="headerlink" title="Amino acids &amp; proteins"></a>Amino acids &amp; proteins</h4><ul><li><a href="https://www.bilibili.com/video/BV1aK41157GP?from=search&amp;seid=9348904521485452131" target="_blank" rel="noopener">what is protein</a></li><li>所有蛋白质由21种（基本单元）氨基酸构成</li><li><p>Amino acids 由 carbon (C), Oxygen (O), Hydrogen (H), Nitrogen(N), Sulfur (S) 构成</p><ul><li>硒代半胱氨酸是唯一的含有一个 Sel (硒原子) 的标准氨基酸</li></ul></li><li>Amino acids 由 Amino Group (氨基), Carboxyl Group (羧基),  Side Chain (侧链), Alpha Carbon (中央碳原子) 组成<ul><li>side chain 是不同氨基酸的唯一不同的部分，它决定了氨基酸的性质<ul><li><strong>Hydrophobic</strong> Amino acids 疏水性氨基酸具有丰富碳侧链，因此不能很好与水相互作用</li><li><strong>Hydrophilic</strong> Amino acids 亲水性（极性）氨基酸可以很好与水相互作用</li><li><strong>Charged</strong> Amino acids 带电荷的氨基酸与带相反电荷的氨基酸或其他分子相互作用</li></ul></li></ul></li><li><strong>primary structure（一级结构）</strong>是通过DNA编码的线性氨基酸序列，蛋白质中的氨基酸通过连接一个氨基酸氨基与另一个氨基酸羧基的肽键相连。每次肽键结合时都会释放一个水分子，相连的碳、氮、氧原子的序列构成了protein backbone（蛋白质的骨架）。</li><li><p>这些蛋白质链通常折叠成两种类型 <strong>secondary structure（二级结构）</strong></p><ul><li>alpha helix（螺旋）<ul><li>通过附近的氨基酸的氨基和羧基之间的 hydrogen bond (氢键) 稳定下来的右手螺旋线圈</li></ul></li><li>beta sheet（折叠）<ul><li>当两个或者多个相邻的链被氢键固定，形成 beta sheet</li></ul></li></ul></li><li><p><strong>tertiary structure（三级结构）</strong></p><ul><li>蛋白质链的三维形状</li><li>这种形状由构成链的氨基酸的性质决定</li><li>许多蛋白质形成球状，把疏水侧链包围在<strong>内部</strong>，远离周围的水</li><li>膜结合蛋白外面聚集着疏水残基，以便它们可以与膜中的脂质相互作用</li><li>带电荷的氨基酸允许蛋白质与具有互补电荷的分子相互作用</li><li>许多蛋白质的功能依赖于它们的三维形状<ul><li>血红蛋白形成一个袋状，以在中心保持血红素，一种含有铁原子的小分子，用来与氧气结合</li></ul></li></ul></li><li>quaternary structure（四级结构）<ul><li>两条或者更多条多肽链可以通过几个亚基结合在一起，形成一个功能分子</li><li>血红蛋白的四个亚基相互作用以便他们的符合物可以在肺部吸收更多氧气，并将其他释放到体内</li></ul></li><li>protein size<ul><li>大多数蛋白质小于光的波长</li><li>血红蛋白分子的尺寸约为6.5nm</li></ul></li><li><p>蛋白质的三维形状决定了他们的功能</p><ul><li><p>defense（防御）</p><ul><li>antibody 抗体灵活的手臂通过识别并与病原体结合以识别它们，作为免疫系统破坏的目标来保护我们远离疾病</li></ul></li><li><p>communication</p><ul><li>insulin 胰岛素是一种小而稳定的蛋白质可以在血液中旅行时保持形状，来调节血糖</li></ul></li><li><p>enzymes</p><ul><li>alpha 淀粉酶是一种在唾液中消化淀粉的酶</li></ul></li><li><p>transport</p><ul><li>钙泵由镁辅助并由ATP提供动力，在每次肌肉收缩后，将钙离子移动回肌浆网</li></ul></li><li><p>storage</p><ul><li>铁蛋白是一种带通道的球形蛋白质，根据有机体的需要，允许铁原子进入和退出，在铁蛋白内部形成一个空间，使铁原子附着在其内壁，铁蛋白以无毒形式储存铁。</li></ul></li><li><p>structure</p><ul><li>胶原蛋白形成强大的三重螺旋，用来在整个身体中支撑结构。胶原蛋白分子可以形成细长的原纤维，并聚集形成胶原纤维，这种类型胶原蛋白存在于皮肤和筋中</li></ul></li></ul></li></ul><ul><li><p>CDS (coding region 基因编码区，<a href="https://www.omicsclass.com/article/805" target="_blank" rel="noopener">Coding DNA Sequence</a>)</p><ul><li><p>完整一段基因，能翻译成蛋白质的区域是“间隔的、不连续的”（蛋白质编码序列和非蛋白质编码序列两部分组成）</p></li><li><p>编码序列（编码区（基因序列）中可翻译成蛋白质的序列）：exon 外显子</p></li><li><p>非编码序列（编码区（基因序列）中不可翻译成蛋白质的序列）：intron 内含子</p><p><img src="https://github.com/soloistben/images/raw/master/protein/exon_intron.jpeg" alt="exon_intron"></p><p><img src="https://github.com/soloistben/images/raw/master/protein/mRNA_protein.jpg" alt="Gene"></p></li><li><p>启动子（属于ORF的调控序列）是在DNA上，是作用于转录阶段，mRNA不包含启动子</p></li><li><font color="red">对于真核生物的大部分ORF在转录时，是包括外显子和内含子的。转录后，RNA在内含子处进行自我切割，只有外显子可以转录为成熟mRNA</font></li><li><p>mRNA上的CDS区域外显子的核苷酸–翻译–&gt;氨基酸</p></li><li><p><strong>CDS</strong>（mRNA）是<a href="https://weibo.com/ttarticle/p/show?id=2309404030921119537751" target="_blank" rel="noopener">mRNA上从起始密码子到终止密码子之间的RNA序列</a>（指编码一段蛋白产物的序列，是与蛋白质密码子一一对应的序列）</p></li><li><p><strong>ORF</strong>（DNA）是open reading frame的缩写，翻译成开放阅读框，基因的有意编码部分也就是开放阅读框（ORF）</p></li><li><p><strong>基因组DNA分为基因序列和非基因序列——基因序列就是一个完整的表达盒它包括ORF和ORF的调控序列——ORF转录后经加工，使得内含子被切除，外显子组成mRNA序列——mRNA包括了不能翻译的UTR序列和能翻译的CDS。</strong></p></li><li><p>CDS是ORF中不包含UTR的外显子部分</p><p><img src="https://github.com/soloistben/images/raw/master/protein/CDS.png" alt="CDS"></p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Amino-acids-amp-proteins&quot;&gt;&lt;a href=&quot;#Amino-acids-amp-proteins&quot; class=&quot;headerlink&quot; title=&quot;Amino acids &amp;amp; proteins&quot;&gt;&lt;/a&gt;Amino acids 
      
    
    </summary>
    
    
    
      <category term="basic protein" scheme="http://yoursite.com/tags/basic-protein/"/>
    
  </entry>
  
  <entry>
    <title>Master_Eng</title>
    <link href="http://yoursite.com/2019/12/02/Master-Eng/"/>
    <id>http://yoursite.com/2019/12/02/Master-Eng/</id>
    <published>2019-12-02T04:51:06.000Z</published>
    <updated>2019-12-04T07:52:55.118Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Master-Eng-硕士英语"><a href="#Master-Eng-硕士英语" class="headerlink" title="Master Eng 硕士英语"></a>Master Eng 硕士英语</h4><hr><h5 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h5><ul><li>Audience 面向大部分受众</li><li>Visual Aids (PPT)</li><li>Presentor <ul><li>eye contact</li><li>story teller</li><li>body language</li><li>jargon 避免过多专业术语，用最简单的话让他人明白，必须要用时，给出解释</li><li>修辞, 自黑</li></ul></li></ul><hr><h5 id="Dissertation-硕士论文"><a href="#Dissertation-硕士论文" class="headerlink" title="Dissertation 硕士论文"></a>Dissertation 硕士论文</h5><ul><li>1.5w，40-50页</li><li>process 写论文是个过程，需要不断修改、校对（初稿改大方向，后面改小方向）</li><li>content structure<ul><li>Title Page 封面</li><li>Declaration 声明</li><li>Acknowledgement 致谢<ul><li>The acknowledgement for thesis is the section where you thank all peoplewho helped you complete the project successfully.</li></ul></li><li>Contents 目录</li><li>Abstract 摘要（350字）<ul><li>正文<ul><li>Background 背景</li><li>Methodology 方法</li><li>Result 结果</li><li>Conclusion 总结 </li></ul></li><li>Keywords (3-5 words)</li></ul></li><li>Chapter 1: Introduction 引言 (4-6 pages)</li><li>Chapter 2: Literature Review 文献综述 <ul><li>阅读大量文献，了解前沿知识，<strong>find gap (why gap? why you?)</strong></li><li>写论文可以先写这部分</li><li>占比：20/50 页</li></ul></li><li>Chapter 3: Methodology 方法论（定量Quantitative）</li><li>Chapter 4: Result 结果</li><li>Chapter 5: Discussion 讨论 <ul><li>开始先回顾Result， 后续要与Literature Revie呼应，表明填补了哪些gap</li></ul></li><li>Chapter 6: Conclusion 总结（不足之处：人力、物力、财力）</li><li><strong>Reference (APA style)</strong><ul><li>快速定位文献</li><li><strong>避免学术抄袭</strong><ul><li>paraphrasing 改写（字数一致）</li><li>summarizing 总结（缩减至1/4）</li><li>synthesizing 综合前两种</li><li>in-text 文内引用（引用多句，中间无关紧要部分可用省略号表示; 自己修改部分需要中括号括起来）</li></ul></li></ul></li></ul></li></ul><hr><h5 id="Essay-小论文"><a href="#Essay-小论文" class="headerlink" title="Essay 小论文"></a>Essay 小论文</h5><ul><li>unity: 强调一段要完整，主题鲜明</li><li>coherence 段句之间要求连贯</li><li>1段：Topic + support + conclusion</li><li>essay VS project<ul><li>same topic</li><li>same length</li><li>same structure</li><li>essay: literature review, project: do experiment</li></ul></li><li>An essay has three main parts: an introduction, a body, and a conclusion. Introduction consists of two part: general statements and <strong>thesis statement</strong> which plays specific role in the role in the essay, stating the specific topic, listing subtopics of the main topic, and indicating the pattern of organization of the essay, and writer’s position or point of view.<br>&emsp;So a good thesis statement of introduction is first step of a well-written essay, at the same time, main body follow subtopics to expand supports, and conclusion responds the main topic of thesis statement.</li></ul><hr><h5 id="CV-amp-Resume"><a href="#CV-amp-Resume" class="headerlink" title="CV &amp; Resume"></a>CV &amp; Resume</h5><ul><li>CV 个人简历-研究（学术圈），多少页都可以，详细研究经历（who r u）</li><li>Resume 个人简历-普通工作，一页，以读者角度（快速了解），内容：objective 求职意向明确（精确到职位）、Education 精炼、experience 工作经验（研究或项目）、Contact Information 个人信息</li><li>cover letter 封面信（用于申请学校，放在申请材料最上面，解释说明都有什么材料；用于求职信，3段：招聘信息来源、个人信息、很想加入）</li></ul><hr><h5 id="Interview"><a href="#Interview" class="headerlink" title="Interview"></a>Interview</h5><ul><li><strong>First impressions</strong>: we need to be punctual and neat in our appearance, greeting each person with smile.</li><li><strong>Preparation</strong>: we need to know ourselves and the company, preparing some based questions for interviewers.</li><li><strong>Ending</strong>: Asking for a business card or ensuring the information about interviewer’s name, and e-mail address, we can send a <strong>thank-you note</strong>.</li></ul><hr><h5 id="Email"><a href="#Email" class="headerlink" title="Email"></a>Email</h5><ul><li>“密送”：一般用于HR发给候选人（特例：两人之间的邮件信息，在密送中的人可以看到全部内容。遇到想否定的回复直接就不回，然后找上级汇报）</li><li>“自动转发”：工作邮箱邮件转发到私人邮箱，可防止撤回; 在私人邮箱是已读状态，工作邮箱仍是未读，因为撤回是可知道是否已读。</li><li>邮件名称就用姓名拼即可</li></ul><hr><h5 id="Documentation-or-Reference"><a href="#Documentation-or-Reference" class="headerlink" title="Documentation or Reference"></a>Documentation or Reference</h5><ul><li>APA (American Psycholopicaly Association) 用于硕士论文</li><li>MLA (Modern Language Association) 文学 （姓，年份，页码）</li><li>in-text citation（文内引用，每段引用后加括号（姓，年份））</li><li>Reference （文后引用）</li><li>姓. 名（年份）：书名（斜体）. 出版社：xxx. 页码（双写小写p）</li><li>book eg: <strong>Mills. S (2019): <em>Language</em>. London: Oup. pp16-17.</strong></li><li>journals eg: <strong>Mills. S(2019): Language. <em>Journal Name</em>. London:Oup. pp16-17. </strong></li><li>Internet eg: <strong>Mills (or Unknowner) (2019): <em>Language</em>. website. [date] </strong></li></ul><hr><h5 id="Academic-plagiarism"><a href="#Academic-plagiarism" class="headerlink" title="Academic plagiarism"></a>Academic plagiarism</h5><p>&emsp;Academics try to add original contributions to human knowledge by finding gaps in research and by studying very specific topics in detail. When you use information from an outside source without acknowledging that source, you are guilty of plagiarism. Academic plagiarism is using someone else’s words or ideas as if they were your own, which  is a serious offense, and it can result in highly negative consequences such as paper retractions and loss of author credibility and reputation.<br>&emsp;<strong>It is essential for researchers to raise their awareness of academic  plagiarism.</strong> To avoid plagiarism, you should always put quotation marks around words that you copy exactly. You do not need to use <font color="#FF0000">quotation marks</font> if you change the words. <strong>However</strong>, whether you copy the words exactly or state an idea in your own words, you must <font color="#FF0000">cite the source</font>. To cite a source means to tell where you got the information. You can insert a short reference called in-text citation in parentheses at the end of each piece of borrowed information and list describing all your sources completely in the last page. <strong>Finally</strong>, two skills to avoid  plagiarism are <font color="#FF0000">paraphrasing</font> and<font color="#FF0000">summarizing</font>. A paraphrase is typically the same length as the original text but written in your own words and not using the words from the original source. A summary is a condensed version of the original text that highlights the main or key ideas in your own words. So if you were going to summarize a chapter, it might be a page. If you were going to summarize a paragraph, it might be a couple of lines.<br>&emsp;To sum up, <strong>academic dishonesty devalues everyone else’s hard work, at the same time, critical analysis plays a significant role in academic writing.</strong> Your assignments will require you to analyze ideas from multiple sources, draw connections between them, and come to your own conclusions. Therefore, if you read the original work carefully, try to understand the main idea, take good notes, and then express it in academic writing by your own words. Being honesty!</p><hr><p><em>In school, many learning things depend on IQ, but in work, EQ + Money + Relarionship.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Master-Eng-硕士英语&quot;&gt;&lt;a href=&quot;#Master-Eng-硕士英语&quot; class=&quot;headerlink&quot; title=&quot;Master Eng 硕士英语&quot;&gt;&lt;/a&gt;Master Eng 硕士英语&lt;/h4&gt;&lt;hr&gt;
&lt;h5 id=&quot;Presenta
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>algorithm</title>
    <link href="http://yoursite.com/2019/11/14/algorithm/"/>
    <id>http://yoursite.com/2019/11/14/algorithm/</id>
    <published>2019-11-14T12:44:27.000Z</published>
    <updated>2020-01-06T07:36:20.708Z</updated>
    
    <content type="html"><![CDATA[<h4 id="LCS-Longest-Common-Subsequence"><a href="#LCS-Longest-Common-Subsequence" class="headerlink" title="LCS Longest Common Subsequence"></a>LCS Longest Common Subsequence</h4><ul><li>运用动态规划方法查找给定两个序列的最大公共子序列（子序列之间的字符可以不连续，若子串substring的字符必须连续）</li><li>由序列尾部开始：<ul><li>若两个序列尾部字符相等：LCS[i][j] = LSC[i-1][j-1] +1 </li><li>若两个序列尾部字符不相等：LCS[i][j] = max(LSC[i-1][j], LSC[i][j-1])</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/lcs.png" alt="LCS"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'GXTXAYB'</span></span><br><span class="line">str2 = <span class="string">'AGGTAB'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lcs</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    L = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#print(L)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                L[i][j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                L[i][j] = L[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                L[i][j] = max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    print(L)</span><br><span class="line">    <span class="keyword">return</span> L[<span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(lcs(str1, str2))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 4.]]</span><br><span class="line">4.0</span><br></pre></td></tr></table></figure><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/dynamic_lcs.png" alt="dynamic_lcs"></p><h6 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: LCS(A, B)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">L[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then L[i][j]=L[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span> </span><br><span class="line">        <span class="keyword">else</span> L[i][j]=max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> L</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure><hr><h4 id="SA-Sequence-Alignment"><a href="#SA-Sequence-Alignment" class="headerlink" title="SA Sequence Alignment"></a>SA Sequence Alignment</h4><ul><li>运用动态规划方法对给定两个序列做对比（原用于基因DNA链碱基的序列对比）</li><li>对比结果有三种情况：<ul><li>match 相等</li><li>unmatch 不想等</li><li>gap 缺额（两个序列长度不一致）</li></ul></li><li>使用打分原则（match不扣分，unmatch和gap扣不同的分数）</li></ul><p>eg.<br>    A G C T<br>    A U T<br>[第一A和最后T属于match，第二或第三U属于unmatch，第三或第二属于gap]<br>[若match不扣分，unmatch扣3分，gap扣2分，则本次对比=5分，扣分（惩罚分）越低越好]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'AGGGCT'</span></span><br><span class="line">str2 = <span class="string">'SAGE'</span></span><br><span class="line">p_unmatch = <span class="number">3</span></span><br><span class="line">p_gap = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SA</span><span class="params">(str1, str2, p_unmatch, p_gap)</span>:</span></span><br><span class="line">    P = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">                P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                P[i][j] = P[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                P[i][j] = min(&#123;P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_unmatch, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap&#125;)</span><br><span class="line">            </span><br><span class="line">    print(P)</span><br><span class="line">    <span class="keyword">return</span> P[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(SA(str1, str2, p_unmatch, p_gap))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.  2.  4.  6.  8.]</span><br><span class="line"> [ 2.  3.  2.  4.  6.]</span><br><span class="line"> [ 4.  5.  4.  2.  4.]</span><br><span class="line"> [ 6.  7.  6.  4.  5.]</span><br><span class="line"> [ 8.  9.  8.  6.  7.]</span><br><span class="line"> [10. 11. 10.  8.  9.]</span><br><span class="line"> [12. 13. 12. 10. 11.]]</span><br><span class="line">11.0</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: SA(A, B, p_gap, p_xy)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">P[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">        P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then P[i][j]=P[i<span class="number">-1</span>][j<span class="number">-1</span>] </span><br><span class="line">        <span class="keyword">else</span> P[i][j]=min(P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_xy, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap)</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> P</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure><hr><h4 id="Matroid-拟阵"><a href="#Matroid-拟阵" class="headerlink" title="Matroid 拟阵"></a>Matroid 拟阵</h4><ul><li>拟阵理论不能完全覆盖所有的贪心算法（如赫夫曼编码问题），但它可以覆盖大多数具有实际意义的情况。首先介绍拟阵的概念。（有些文献也称作矩阵胚）<br><a href="https://blog.csdn.net/ChiXueZhiHun/article/details/54808939" target="_blank" rel="noopener">Matroid</a></li></ul><hr><h4 id="Set-Cover"><a href="#Set-Cover" class="headerlink" title="Set Cover"></a>Set Cover</h4><ul><li>给定一个集合B，和一些子集sets，在子集中找到最好的覆盖原集合的子集（贪心算法）</li><li>在t时刻，选择子集设为St，当前还剩集合元素个数为nt，以选子集的元素集合为P</li><li>则在子集中，每次选择与原集合<strong>相交</strong>最多元素的子集，且符合公式|St|&gt;= nt/|P|</li></ul><h5 id="S-0-1-2-3-4-5-6-7-8-9"><a href="#S-0-1-2-3-4-5-6-7-8-9" class="headerlink" title="S = {0,1,2,3,4,5,6,7,8,9}"></a>S = {0,1,2,3,4,5,6,7,8,9}</h5><h5 id="Sets-0-1-2-3-1-4-2-2-3-5-1-7-4-6-3-6-8-7-9"><a href="#Sets-0-1-2-3-1-4-2-2-3-5-1-7-4-6-3-6-8-7-9" class="headerlink" title="Sets = {0,1}, {2,3}, {1,4,2}, {2,3,5}, {1,7}, {4,6}, {3,6,8}, {7,9}"></a>Sets = {0,1}, {2,3}, {1,4,2}, {2,3,5}, {1,7}, {4,6}, {3,6,8}, {7,9}</h5><p>Let nt be the number of uncovered elements after step t, P be optimal selection<br>Choose max (sets ∩ S) in each step:</p><ul><li>Step 1: n1 = 10, S1 = {1,4,2}, P = S1 = {1,2,4}</li><li>Step 2: n2 = 7, S2 = {3,6,8}, P = S1US2 = {1,2,3,4,6,8}</li><li>Step 3: n3 = 4, S3 = {7,9}, P = S1US2US3 = {1,2,3,4,6,7,8,9}</li><li>Step 4: n4 = 2, S4 = {0,1}, P = S1US2US3Us4 = {0,1,2,3,4,6,7,8,9}</li><li>Step 5: n5 = 1, S5 = {2,3,5}, P = S1US2US3US4US5 = {0,1,2,3,4,5,6,7,8,9} = S</li></ul><h5 id="so-cover-1-4-2-U-3-6-8-U-7-9-U-0-1-U-2-3-5-S"><a href="#so-cover-1-4-2-U-3-6-8-U-7-9-U-0-1-U-2-3-5-S" class="headerlink" title="so cover {1,4,2}U{3,6,8}U{7,9}U{0,1}U{2,3,5} = S"></a>so cover {1,4,2}U{3,6,8}U{7,9}U{0,1}U{2,3,5} = S</h5><p>set cover problem need to obey  <strong>|St+1| ≥ nt/|P| </strong></p><ul><li>if choose l S1| = 2 in step 1, n1 = 10, and n1/|P| = 10/2 = 5, so we need to choose a set containing 5 elements in step 2. That is contradictory.</li><li>if choose l S1| = 3 in step 1, n1 =10, and n1/|P| = 10/3 = 3, so we need to choose a set containing 3 elements in step 2. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">B = set([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">sets = &#123;&#125;</span><br><span class="line">sets[<span class="number">1</span>] = set([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">sets[<span class="number">2</span>] = set([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">sets[<span class="number">3</span>] = set([<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">sets[<span class="number">4</span>] = set([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">sets[<span class="number">5</span>] = set([<span class="number">1</span>,<span class="number">7</span>])</span><br><span class="line">sets[<span class="number">6</span>] = set([<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">sets[<span class="number">7</span>] = set([<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>])</span><br><span class="line">sets[<span class="number">8</span>] = set([<span class="number">7</span>,<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">P = set()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> B:</span><br><span class="line">    best_sets = <span class="keyword">None</span></span><br><span class="line">    sets_covered = set()</span><br><span class="line">    <span class="keyword">for</span> sets_num, sets_value <span class="keyword">in</span> sets.items():</span><br><span class="line">        covered = B &amp; sets_value    <span class="comment"># 选择交集最大的set</span></span><br><span class="line">        <span class="keyword">if</span> len(covered) &gt; len(sets_covered):</span><br><span class="line">            sets_covered = covered</span><br><span class="line">            best_sets = sets_num </span><br><span class="line">    B -= sets_covered</span><br><span class="line">    print(best_sets,<span class="string">':'</span>,sets_covered)</span><br><span class="line">    P.add(best_sets)</span><br><span class="line">print(P)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3 : &#123;1, 2, 4&#125;</span><br><span class="line">7 : &#123;8, 3, 6&#125;</span><br><span class="line">8 : &#123;9, 7&#125;</span><br><span class="line">1 : &#123;0&#125;</span><br><span class="line">4 : &#123;5&#125;</span><br><span class="line">&#123;1, 3, 4, 7, 8&#125;</span><br></pre></td></tr></table></figure><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Dijkstra.png" alt="dijkstra_process"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bellman_ford.png" alt="bellman_ford"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bell.png" alt="bell"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal1.png" alt="kruskal_process"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal2.png" alt="kruskal_process"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal3.png" alt="kruskal_process"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow1.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow2.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow3.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow4.png" alt="Network_Flow"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching1.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching2.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching3.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching4.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching5.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching6.png" alt="bipartite_matching"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine1.png" alt="Turing_machine"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine2.png" alt="Turing_machine"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine3.png" alt="Turing_machine"></p><hr><h4 id="Halting-Problem"><a href="#Halting-Problem" class="headerlink" title="Halting Problem"></a>Halting Problem</h4><ul><li><p>问题：是否存在一个过程能做这件事：该过程以一个计算机程序以及该程序的一个输入作为输入，并判断该过程在给定输入运行时是否<strong>最终</strong>能停止。</p></li><li><p>假设：存在一个这样的过程<strong>H(P,I)</strong>，P为计算机程序，I为该程序的一个输入,H可以根据P和I返回true（该过程在给定输入运行时能停止)，或者false（该过程在给定输入运行时不能停止）</p></li><li><p>结论：因此不存在这样的一个过程能解决停机问题。</p></li><li><p>停机问题的“现实”意义是说明了程序不是无所不能的</p><p><a href="https://blog.csdn.net/MyLinChi/article/details/79044156" target="_blank" rel="noopener">HP</a></p></li></ul><hr><h4 id="P-vs-NP"><a href="#P-vs-NP" class="headerlink" title="P vs NP"></a>P vs NP</h4><ul><li>P 问题：现阶段可以用计算机在多项式时间内解决的问题(<strong>P</strong>olynomial Time)</li><li>NP问题：现阶段无法在多项式时间内解决，但可以在多项式时间内验证的问题(<strong>N</strong>on-deterministic <strong>P</strong>olynomial Time)</li><li>若P=NP，则任何问题均可用计算机破解，则没有加密可言，任意生物、医疗、经济、科技问题全部可以解决。</li><li>NP-complete：很多很难的NP问题，本质上是同一个问题（卡在了相同问题上），可以用简单的<strong>多项式时间</strong>转换（数学：<strong>这些NP问题中真正困难的部分</strong>），能解决NP-complete问题就可以解决NP问题（数独游戏、蛋白质折叠（治疗癌症问题））</li><li>证明P是否等于NP，就是一个NP问题</li><li>NP-hard = NP-complete + 各种指数级别问题（下棋的计算或验证）(NP-hrad问题 是假定有无限时间和空间)<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/NP.png" alt="NP"></li><li>P-SAPCE：在无限时间，只使用多项式数量的空间，就可以求解（包括NP-complete）</li><li>BPP: 多项式时间内有几率求解问题 （BQP = BPP的量子计算）（包括P）</li><li>比NP-hrad更难的问题 = “没有任何电脑在任意时间或空间可解出来的问题”（指数层叠，多项式层叠）</li><li>所有问题 -&gt; 在给定空间、时间下，什么可以计算出来（已经不在是探讨计算的特性，而是寻找时间和空间的本身性质）<br><a href="https://www.bilibili.com/video/av19085452?from=search&amp;seid=7087981409362725035" target="_blank" rel="noopener">b站</a></li></ul><hr><h4 id="CNF-amp-SAT"><a href="#CNF-amp-SAT" class="headerlink" title="CNF &amp; SAT"></a>CNF &amp; SAT</h4><ul><li><strong>Conjunctive Normal Form (CNF)合取范式</strong> 是命题公式的一种标准形。一个命题公式的合取范式可以通过<strong>真值(TRUE)</strong>表得到，也可以通过等价变换得到。（合取范式主要用于解决命题公式的逻辑判断，一个命题的合取范式不是唯一的）</li><li><strong>Satisfaction Problem (SAT)</strong>,  decide if there is an assignment that satisfies the given CNF. SAT is NP<ul><li>TRUE, if CNF is satisfied with the certification</li><li>FALSE, if CNF is not satisfied with the certification</li></ul></li><li>给定一个布尔变量集合（只取0和1）组成SAT和子句集合CNF，是否存在一个真值赋值（SAT结果），<strong>使得CNF为真，即每个子句为真</strong><br><a href="https://blog.csdn.net/yxl564710062/article/details/79882746" target="_blank" rel="noopener">3-SAT</a></li><li><strong>CIRCUIT-SAT</strong> is NPC，若将CIRCUIT-SAT问题划分三个SAT问题，是否能证明SAT也是NPC?</li><li><strong>3-CNF Satisfaction Problem (3-SAT)</strong>: decide if there is an assignment that satisfies the given 3-CNF, in which each clause contains exactly 3 literals. </li><li><strong>NP -&gt; NPC -&gt; 3-SAT -&gt; Clique -&gt; Vertex Cover</strong>  (Using Clique To Set Variables of 3-SAT)<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/SAT.png" alt="SAT"><ul><li>only Variables disconnect its not Variables (x1 disconnect not x1)</li><li>Variables disconnect others in same group (make no sence)</li><li>connected Variables is same value<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/SAT_Clique.png" alt="SAT_Clique"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Clique.png" alt="Clique"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Vertex_Cover.png" alt="Vertex_Cover"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Clique_VertexCover.png" alt="Clique_VertexCover"></li></ul></li></ul><hr><ol><li>算法中线性结构，数据元素之间存在一对一的线性关系。</li><li>算法符号，Θ：渐近紧确界，O：渐近上界，Ω：渐近下界；o：非渐近紧确的上界（O提供的渐近上界可能是也可能不是渐近紧确的），ω：非渐近紧确的下界</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;LCS-Longest-Common-Subsequence&quot;&gt;&lt;a href=&quot;#LCS-Longest-Common-Subsequence&quot; class=&quot;headerlink&quot; title=&quot;LCS Longest Common Subsequence&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>GNN</title>
    <link href="http://yoursite.com/2019/08/19/GNN/"/>
    <id>http://yoursite.com/2019/08/19/GNN/</id>
    <published>2019-08-19T05:39:28.000Z</published>
    <updated>2019-08-20T01:40:45.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications"><a href="#NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications" class="headerlink" title="NOTE of Graph Neural Networks: A Review of Methods and Applications"></a>NOTE of Graph Neural Networks: A Review of Methods and Applications</h4><ol><li>As a unique non-Euclidean data structure for machine learning, graph analysis focuses on node classification, link prediction, and clustering.</li><li>Why GNN?</li></ol><ul><li>Composed to CNN, GNN is highly expressive representations</li><li>shared weights reduce the computational cost compared with traditional spectral graph theory; multi-layer structure is the key to deal with hierarchical patterns, which captures the features of various sizes.</li><li>CNN can only operate on regular Euclidean data like images (2D grid) and text (1D sequence), GNN can.</li><li>graph embedding learns to represent graph nodes, edges or sub-graphs in low-dimensional vectors. </li><li>Deep-Walk, which is first graph embedding, is based on representation learning, word embedding and Skip-Gram model. Just like node2vec. However, <ul><li>no parameters are shared between nodes, so it means the number of parameters grows linearly with the number of nodes. </li><li>The direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs</li></ul></li></ul><ol start="3"><li>GNN model input and/or output consisting of elements and their dependency.</li><li>There isn’t a natural order of nodes in the graph. (CNN &amp; RNN can’t input node of no special order)</li><li>In the standard neural networks, the dependency information is just regarded as the feature of nodes, not edge which represents the information of dependency between two nodes in a graph!</li><li>GNN update the hidden state of nodes by a weighted sum of the states of their neighborhood.</li><li>Human brain is almost based on the graph which is extracted from daily experience. </li><li>GNN can learn the <em>reasoning</em> graph from large experimental data.</li><li><strong>Message Passing Neural Network (MPNN)</strong> could generalize several graph neural network and graph convolutional network approaches. </li><li><strong>Non-local Neural Network (NLNN)</strong> unifies several “self-attention”-style methods.</li><li>Both of MPNN&amp;NLNN focus on specific application domains and can’t provide a review over other graph attention models.</li><li><strong>Graph Network (GN) </strong>has strong capability to generalize other models. However, the graph network model is highly abstract and only gives a rough classification of the applications.</li><li>Graph neural networks suffer from over-smoothing and scaling problems. There are still no effective methods for dealing with dynamic graphs as well as modeling non-structural sensory data.</li><li>Original framework:</li></ol><ul><li>Notations<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/1.jpg" alt="chinese"><br><img src="https://github.com/soloistben/images/raw/master/gnn_image/2.png" alt="eng"></li><li>The target of GNN is to learn a state embedding <strong>h_v ∈ Rs </strong>which contains the information of neighborhood for each node.</li><li>Let <strong>f</strong> be a parametric function, called local transition function, that is shared among all nodes and updates the node state according to the input neighborhood.</li><li>Let <strong>g</strong> be the local output function that describes how the output is produced.</li><li><strong>h_v = f (x_v, x_co[v], h_ne[v], x_ne[v])  o_v = g (h_v, x_v)</strong>  (x is the features of v)</li><li><strong>Vectorize: H = F (H, X)  O = G (H, XN)</strong>  (F is global translation function, G is global output function)</li><li>The value of H is the fixed point of Eq.3 and is uniquely defined with the assumption that F is a contraction map.</li><li>GNN uses the following classic iterative scheme for computing the state.     <strong>Ht+1 = F(Ht, X)</strong></li><li>With the target information (t_v for a specific node) for the supervision. <strong>loss = ⅀ (t_i − o_i)</strong></li></ul><ol start="15"><li>Limitations:</li></ol><ul><li>GNN is inefficient to update the hidden states of nodes iteratively for the fixed point.</li><li>There are also some informative features on the edges which cannot be effectively modeled in the original GNN. </li><li>It’s unsuitable to use the fixed points if we focus on the representation of nodes instead of graphs because the distribution of representation in the fixed point will be much smooth in value and less informative for distinguishing each node.</li></ul><ol start="16"><li><strong>Variants of Graph Neural Networks</strong> to release the limitations:</li></ol><ul><li>different graph types:<ul><li>Directed Graphs. Directed edges can bring more information than undirected edges.</li><li>Heterogeneous Graphs. <ul><li>The simplest way to process heterogeneous graph is to convert the type of each node to a one-hot feature vector which is concatenated with the original feature.</li><li>For each neighbor group, GraphInception treats it as a sub-graph in a homogeneous graph to do propagation and concatenates the propagation results from different homogeneous graphs to do a collective node representation. (heterogeneous graph attention network, <strong>HAN</strong>)</li><li>[<a href="https://zhuanlan.zhihu.com/p/47040007]" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47040007]</a></li></ul></li><li>Graphs with Edge Information.<ul><li>Converting the graph to a bipartite graph where the original edges also become nodes and one original edge is split into two new edges which means there are two new edges between the edge node and begin/end nodes. (The encoder of <strong>G2S</strong> uses the following aggregation function for neighbors).</li><li>Adapting different weight matrices for the propagation on different kinds of edges. When the number of relations is very large, r-GCN introduces two kinds of regularization to reduce the number of parameters for modeling amounts of relations: basis and block diagonal-decomposition</li></ul></li><li>Dynamic Graphs. It has static graph structure and dynamic input signals.</li></ul></li><li>several modifications:<ul><li><strong>Graph Convolutional Network (GCN)</strong>: convolutions to the graph domain<ul><li>spectral approaches,使用谱分解的方法，应用图的拉普拉斯矩阵分解进行节点的信息收集     </li><li>non-spectral (spatial) approaches,直接使用图的拓扑结构，根据图的邻居信息进行信息收集</li></ul></li><li><strong>Gated graph neural network (GGNN)</strong>: using the gate mechanism like GRU or LSTM in the propagation step to diminish the restrictions in the former GNN models and improve the long-term propagation of information across the graph structure. Tree LSTM、Graph LSTM and Sentence LSTM</li><li><strong>Graph Attention Network (GAT)</strong> : incorporates the attention mechanism into the propagation step. <ul><li>GAT computes the hidden states of each node by attending over its neighbors, following a self-attention strategy.     </li><li>Gated Attention Network (GAAN) also uses the multi-head attention mechanism. However, it uses a self-attention mechanism to gather information from different heads to replace the average operation of GAT.</li></ul></li><li><strong>Residual connection</strong>: aiming to achieve better results as more layers make each node aggregate more information from neighbors, because more layers could also propagate the noisy information from an exponentially increasing number of expanded neighborhood members.<ul><li>Highway GCN</li><li>Jump Knowledge Network, selects from all of the intermediate representations (which ”jump” to the last layer) for each node at the last layer, which makes the model adapt the effective neighborhood size for each node as needed. uses three approaches of concatenation, max-pooling and LSTM-attention in the experiments to aggregate information. (The Jump Knowledge Network performs well on the experiments in social, bioinformatics and citation networks. It could also be combined with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks to improve their performance.)</li></ul></li><li><strong>Hierarchical Pooling</strong>, Complicated and large-scale graphs usually carry rich hierarchical structures which are of great importance for node-level and graph-level classification tasks</li></ul></li></ul><ol start="17"><li>Training Method:</li></ol><ul><li>Sampling<ul><li>GraphSAGE replaced full graph Laplacian in GCN with learnable aggregation functions, which are key to perform message passing and generalize to unseen nodes.  With learned aggregation and propagation functions, GraphSAGE could generate embeddings for unseen nodes.</li><li>PinSage, By simulating random walks starting from target nodes, this approach chooses the top T nodes with the highest normalized visit counts.</li><li>FastGCN, Instead of sampling neighbors for each node, FastGCN directly samples the receptive field for each layer.  </li><li>adaptive sampler could find optimal sampling importance and reduce variance simultaneously</li></ul></li><li>Receptive Field Control, a control-variate based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate. </li><li>Data Augmentation, To solve the limitations, the authors proposed Co-Training GCN and Self-Training GCN to enlarge the training dataset. </li><li>Unsupervised Training, Graph auto-encoders (GAE) aim at representing nodes into low-dimensional vectors by an unsupervised training manner.</li></ul><ol start="18"><li>Frameworks:</li></ol><ul><li><strong>Message Passing Neural Networks (MPNN)</strong> unified GNN &amp; GCN. It abstracts the commonalities between several of the most popular models for graph-structured data.<ul><li>message passing </li><li>Readout computes a feature vector for the whole graph</li></ul></li><li><strong>Non-local Neural Networks (NLNN)</strong> unified several self-attention for capturing long-range dependencies with deep neural networks. It computes the response at a position as a weighted sum of the features at all positions. List the choices for f function:<ul><li>Gaussian (natural choice) </li><li>Embedded Gaussian </li><li>Dot product </li><li>Concatenation.</li></ul></li><li><span style="border-bottom:2px dashed red;"><strong>Graph Networks (GN) </strong>unified MPNN &amp; NLNN and so on.</span><ul><li>Graph definition</li><li>GN block contains<ul><li>three “update” functions, φ_e, φ_h &amp; φ_u, </li><li>three “aggregation” functions, ρ ( The ρ functions must be invariant to permutations of their inputs and should take variable numbers of arguments)<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/3.png" alt="func"></li></ul></li><li>Computation steps<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/4.png" alt="steps"></li><li>Design Principles<ul><li><strong>Flexible representations</strong><ul><li>One can simply tailor the output of a GN block according to specific demands of tasks </li><li>be applied to both structural scenarios where the graph structure is explicit and non structural scenarios where the relational structure should be inferred or assumed.</li></ul></li><li><strong>Configurable within-block structure</strong>. Based on different structure and functions settings, a variety of models (such as MPNN, NLNN and other variants) could be expressed by the GN framework.</li><li><strong>Composable multi-block architectures.</strong><ul><li>Arbitrary numbers of GN blocks could be composed in sequence with shared or unshared parameters.</li><li>utilizes GN blocks to construct an encode process decode architecture and a recurrent GN-based architecture.</li><li>Other techniques for building GN based architectures could also be useful, such as skip connections, LSTM- or GRU-style gating schemes and so on.</li></ul></li></ul></li></ul></li></ul><ol start="19"><li>Applications of GNN:</li></ol><ul><li>supervised, semi-supervised, unsupervised and reinforcement learning</li><li>Structural Scenarios<ul><li>Physics</li><li>Chemistry and Biology</li><li>Knowledge graph</li></ul></li><li>Non-Structural Scenarios<ul><li>Image<ul><li>Visual Reasoning</li><li>Semantic Segmentation</li></ul></li><li>Text<ul><li>Text classification</li><li>Sequence labeling</li><li>Neural machine translation</li><li>Relation extraction</li><li>Event extraction</li><li>Other applications</li></ul></li></ul></li><li>Other Scenarios<ul><li>Generative Models</li><li>Combinatorial Optimization</li></ul></li></ul><ol start="20"><li>Problems</li></ol><ul><li>Shallow Structure</li><li>Dynamic Graphs</li><li>Non-Structural Scenarios</li><li>Scalability </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications&quot;&gt;&lt;a href=&quot;#NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>DL_RNN</title>
    <link href="http://yoursite.com/2019/07/29/DL-RNN/"/>
    <id>http://yoursite.com/2019/07/29/DL-RNN/</id>
    <published>2019-07-29T01:22:01.000Z</published>
    <updated>2019-07-29T06:48:00.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Sequence-Model-序列模型（包括RNN）："><a href="#Sequence-Model-序列模型（包括RNN）：" class="headerlink" title="Sequence Model 序列模型（包括RNN）："></a>Sequence Model 序列模型（包括RNN）：</h4><p>1、<strong>Speech Recognition 语音识别</strong>：输入一段语音，输出英文句子（输入输出都属于Sequence Data序列数据）<br>2、<strong>Music generation 音乐生成</strong>：输入可以是空集（可以不输入，可以是数字、音乐风格），输出是音符（属于序列）<br>3、<strong>Sentiment classification 情感分类</strong>：输入是评语/一段话（属于序列），输出是衡量情感的标记<br>4、<strong>DNA Squence analysis DNA序列分析</strong>：输入给定的DNA序列，输出在DNA上标记匹配某种蛋白质的<br>5、<strong>Machine Transaction 机器翻译</strong>：输入英语一句话，输出中文的翻译<br>6、<strong>Video Activity Recognition 视频动作识别</strong>：输入以帧单位的视频，输出描述语句<br>7、<strong>Name Entity Recognition 命名实体识别</strong>：输入一句话，输出在句子中识别到的名字（常用于搜索引擎识别特殊名词）<br>8、在序列模型训练中，有输入输出都是序列数据（可能等长，可能类型不同），也有仅输入或输出才是序列模型，<br>9、命名实体识别在输入句子中，以每个单词为单位，<span style="border-bottom:2px dashed red;">输出一维向量，匹配位置，是名字的标记1，否则标记0（用X&lt;t&gt;来表示单词所在时序序列，Tx表示输入的序列长度，Ty表示输出的序列长度）</span>（这个例子也属于<strong>NLP（Natural Language Processing自然语言处理）</strong>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/17.png" alt="image"><br>10、NLP中，首要解决的是怎样表示序列里单独的单词，然后制作一个单词表/字典（在商用中字典单词经常会有3w到5w，甚至10w、100w+）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/18.png" alt="image"><br>11、在输入句子序列中，<span style="border-bottom:2px dashed red;">字典个数=每个向量的维度</span>（用one-hot的方式标记向量，用1标出在字典的位置，其他为0，一维向量）<span style="border-bottom:2px dashed red;">（若是遇到不再字典的单词，就创建一个叫做Unknown Word的伪造单词，来表示不再字典的单词）</span><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/19.png" alt="image"></p><h4 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network RNN:"></a>Recurrent Neural Network RNN:</h4><p>1、若是用传统的神经网络，输入x则全是one-hot的向量，而且维度会很大，输出也相同；缺点是在不同例子中可以有不同长度，Tx与Ty不一定相等，并且<span style="border-bottom:2px dashed red;">从文本位置上学到的特征并没有共享使用（若是一个人名首次被识别人名之后，在第二处，直接就可以识别成人名</span>，这才是想要的结果（计算机视觉，就是学习了小部分特征，立马推广到图片的其他地方））<br>2、<span style="border-bottom:2px dashed red;">RNN：由左到右，每次执行都添加如激活值a[i]（第一个a[0]一般初始化为0，作为伪激活），每一层都会结合上一层所得到的激活值（代表前面所有层的信息）一起预测出y，则每一层都有了联系，参数得到共享</span>（缺点是只得到前面层的信息，得不到后面层的信息，因为有时仅靠前面信息无法推定该词回事人名的单词，会有<strong>Bidirectional Recurrent Neural Network双向循环神经网络（BRNN）</strong>解决该问题）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/20.png" alt="image"><br>3、 <strong>a&lt;t&gt; = g(Waa*a&lt;t-1&gt;+Wax*X&lt;t&gt;+ba); y&lt;t&gt; = g(Wya*a&lt;t&gt;+by)</strong>（W下标，谁在前面就是求谁，在后就要乘以谁）（常用激活函数是tanh/ReLU，通常是<span style="border-bottom:2px dashed red;">tanh</span>，有其他方式可以避免梯度消失问题）（输出部分的激活函数，若是二分问题，则用sigmoid函数）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/21.png" alt="image"><br>4、 设Wa=[Waa Wax]，若Waa维度是(100,100)，Wax是(100,10000)，则Wa是(100,10100)，简化<strong>a&lt;t&gt; = g(Wa[a&lt;t-1&gt;, X&lt;t&gt;]+ba)</strong>，[a&lt;t-1&gt;, X&lt;t&gt;]维度是(10100,100)是a在上部分，X在下部分，结果a&lt;t&gt;维度是(100,100)；简化<strong>y&lt;t&gt; = g(Wy*a&lt;t&gt;+by)</strong>仅有一个下标表示输出什么类型的量<br>5、<strong>RNN反向传播backpropagation through time</strong>：基本与RNN的方向相反（前向传播顺着时间走，反向传播逆着时间走）；<span style="border-bottom:2px dashed red;">某个单词预测的损失函数使用交叉熵损失函数，整个损失值单个单词损失值之和</span><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/22.png" alt="image"></p><h4 id="不同基本模块的RNN："><a href="#不同基本模块的RNN：" class="headerlink" title="不同基本模块的RNN："></a>不同基本模块的RNN：</h4><p>1、Tx=Ty的RNN架构属于多对多类型<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/23.png" alt="image"><br>2、在情感分析的RNN中，输入是一句评语，输出判定正/负面评价（0/1二分类问题），多对一类型则在设计神经网络时，不必像多比多那样每次循环都要输出，仅在最后在输出0/1即可<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/24.png" alt="image"><br>3、在音乐生成的RNN中，输入是空集也可以，输出是一段音乐（序列），一对多类型在设计神经网络时，仅在第一次循环输入，后续循环的输入是上次循环的结果y<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/25.png" alt="image"><br>4、在多对多类型中还有Tx≠Ty的情况，机器翻译就属于其一（每种语言文字单词不全是一对一），则在设计神经网络时，(encode)就先循环输入x，(decode)输入完毕在循环输出y<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/26.png" alt="image"><br>5、一对一结构则是一种标准类型的神经网络，不需要RNN也可以</p><h4 id="Language-Model-语言模型："><a href="#Language-Model-语言模型：" class="headerlink" title="Language Model 语言模型："></a>Language Model 语言模型：</h4><p>1、<span style="border-bottom:2px dashed red;">语言模型：输入句子（文本序列 y），进行估计句子中各个单词出现的可能性/概率</span>（学习文字描述风格，在运用风格写文章）（给出部分单词，预测下一个单词）<br>2、训练集要包含 large cropus of English text 很大的英文文本语料库（cropus语料库是自然语言处理中的专业名词，意思是很多很长、用英文句子、组成的文本）<br>3、整个流程：</p><ul><li>预处理：输入一句话序列(cats average 15 hours of sleep a day)；<strong>tokenize标记化</strong>，将句子中单词转成one-hot向量（字典中的索引），<span style="border-bottom:2px dashed red;">增加一个&lt;EOS&gt;作为句子的结尾</span>，被标记为末尾y&lt;Ty&gt;（若将句号看出标志，其他符号也需要看成标志）；在出现不在字典中的单词时，将其标记问<strong>UNK(Unknown Word)</strong></li><li>设计RNN序列概率模型，<span style="border-bottom:2px dashed red;">初始化激活值a&lt;0&gt;和输入值x&lt;1&gt;为零向量，计算出a&lt;1&gt;，再softmax预测出y&lt;1&gt;概率，得出第一个单词的概率；再第二个循环中x&lt;2&gt;=y&lt;1&gt;，重复操作</span>（<strong>通过训练，预测出下一个单词，也就是学习了造句风格，在测试中可以遇到训练过的单词，会更多概率选中与风格相匹配的单词</strong>）；第一层算出概率P(y&lt;1&gt;)，第二层在第一层基础上算出概率P(y&lt;2&gt;|y&lt;1&gt;)，第三层则是P(y&lt;3&gt;|y&lt;1&gt;,y&lt;2&gt;)…最后<strong>P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;)=P(y&lt;1&gt;)P(y&lt;2&gt;|y&lt;1&gt;)P(y&lt;3&gt;|y&lt;1&gt;,y&lt;2&gt;)</strong><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/27.png" alt="image"></li><li><strong>Sampling novel sequences对新序列进行采样</strong>：在训练序列模型之后，一个非正式的方法（对序列采样）可以得知这个模型学习了什么；<span style="border-bottom:2px dashed red;">对训练得到的概率分布P(y&lt;1&gt;,y&lt;2&gt;,…,y&lt;Tx&gt;)进行采样，来生成一个新的单词序列；首先初始化激活值a&lt;0&gt;和输入值x&lt;1&gt;为零向量，第一层softmax预测到的时所有单词作为第一个单词概率分布，然后进行随机采样（用np.random.choice），将第一层得到的y\<1>输出到第二层，若第一个单词选择了“the”，则第二层计算出在“the”情况下，下一个单词概率，然后再次随机采样，一直循环、采样到结束</1></span>（有可能会出现UNK，所以在采用时拒绝出现未录入字典的单词，直到不是UNK的单词）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/28.png" alt="image"></li></ul><p>4、<strong>Character-level language model 基于字符的语言模型</strong>：将每个单词字母全部拆分训练，优点是不会出现UNK，缺点是会得到太多太长的序列（一般一个句子就10~20个单词）不利于<strong>捕捉句子间的依赖关系</strong>（句子较前部分预测句子较后部分），并且计算成本高<br>5、NLP趋势是使用<strong>word level language model基于词汇的语言模型</strong>，（在计算能力好的情况下，处理大量未知文本或者专业词汇，使用基于字符的语言模型会更好）</p><h4 id="梯度消失："><a href="#梯度消失：" class="headerlink" title="梯度消失："></a>梯度消失：</h4><p>1、RNN常用tanh激活函数，因而存在有梯度消失问题，影响反向传播<br>2、基础的RNN仍是<span style="border-bottom:2px dashed red;">不擅长于捕捉长期依赖效应</span>（主语与谓语相隔较远时，时态和单双数的变化，类似的长期依赖），这需要在RNN对主语的<strong>长期记忆</strong>，才能在后面对谓语有联系<br>3、基础的RNN模型会有很多局部影响，<span style="border-bottom:2px dashed red;">比如y&lt;3&gt;就会受到其附近的值影响，在后面的值很难受到前面的影响，这个区域都很难反向传播到序列前面部分，因此神经网络很难调整前面的计算（缺点）</span><br>4、RNN也会出现梯度爆炸，相对容易发现，会直接让神经网络崩溃，参数数值会很大，甚至NaN，溢出；解决方法：<strong>gradient clipping梯度修剪</strong>（观察梯度向量，若其大于某个threshold阈值，缩放梯度向量，保证它不会太大，这就是最大值修剪）</p><h4 id="Gated-Recurrent-Unit-GRU门控循环单元："><a href="#Gated-Recurrent-Unit-GRU门控循环单元：" class="headerlink" title="Gated Recurrent Unit GRU门控循环单元："></a>Gated Recurrent Unit GRU门控循环单元：</h4><p>1、GRU能让RNN在深层网络中捕捉依赖关系，解决梯度消失问题<br>2、RNN中隐藏单元的可视化<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/29.png" alt="image"><br>3、GRU加入新变量c表示memory cell细胞，记忆细胞（在后面动词看到c时会知道主语是单数复数）：在时间t处，c&lt;t&gt;=a&lt;t&gt;（但在LSTMs他俩不相等）<strong>用c~&lt;t&gt; = tanh(Wc[c&lt;t-1&gt;,x&lt;t&gt;+bc)代替了c&lt;t&gt;</strong><br>4、GRU中心思想时，有个门<strong>Γu = sigmoid((Wu[c&lt;t-1&gt;,x&lt;t&gt;+bu)</strong>（Γ 范围是0到1，u代表更新gate，sigmoid的输出范围是[0,1]）<br>5、用c~更新c的等式，然后<span style="border-bottom:2px dashed red;">Γ 决定是否要更新它</span>（假设c&lt;t&gt;=1时表示单数，0表示复数，经过中间隔着很久，到达动词，c&lt;t&gt;仍是等于1，则动词显示单数；Γu 就决定什么时候去更新c&lt;t&gt;这个值）<strong>c&lt;t&gt; = Γu*c~&lt;t&gt; + (1-Γu)*c&lt;t&gt;</strong>（元素对应乘积）（<span style="border-bottom:2px dashed red;">当Γu=1时，c&lt;t&gt;=1，对于主谓语之间的语句 Γu=0，也就是c&lt;t&gt;=c&lt;t-1&gt;，不用进行更新</span>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/30.png" alt="image"><br>6、因为sigmoid的值，Γu很容易取到0，则c&lt;t&gt;一直维持值不变，因为Γ很接近0，可能是0.000001，c&lt;t&gt;不变，a&lt;t&gt;值也维持不变，这就不会出现梯度消失了<br>7、c&lt;t&gt;可以是个向量，若有100个隐含依赖，则可以是100维，Γ也是相同维度，就是100bit的向量（里面几乎全是0或者1）（Γ不会确切的等于0或者1，会是0到1的中间值，为了理解可以当成0和1）<br>8、完整的GRU公式（会多一个Γr相关门）（GRU和LSTM都十分常用）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/31.png" alt="image"></p><h4 id="Long-Short-Term-Memory-LSTM长短期记忆网络："><a href="#Long-Short-Term-Memory-LSTM长短期记忆网络：" class="headerlink" title="Long Short-Term Memory LSTM长短期记忆网络："></a>Long Short-Term Memory LSTM长短期记忆网络：</h4><p>1、比GRU更加有效，是比GRU更通用更强<br>2、在LSTM中得公式，</p><ul><li>c~&lt;t&gt; = tanh(Wc[a&lt;t-1&gt;,x&lt;t&gt;+bc)</li><li>Γu = sigmoid((Wu[a&lt;t-1&gt;,x&lt;t&gt;+bu)    update gate</li><li>Γf = sigmoid((Wf[a&lt;t-1&gt;,x&lt;t&gt;+bf)    forget gate</li><li>Γo = sigmoid((Wo[a&lt;t-1&gt;,x&lt;t&gt;+bo)     output gate</li><li>c&lt;t&gt; = Γu*c~&lt;t&gt; + Γf*c&lt;t-1&gt;（Γf代替1-Γu）</li><li>a&lt;t&gt; = Γo*tanh(c&lt;t&gt;)<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/32.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/33.png" alt="image"></li></ul><p>3、设置了forget gate和update gate，则LSTM很容易把c&lt;0&gt;一直传递下去<br>4、在三种gate的计算下，只需要a&lt;t-1&gt;和x&lt;t&gt;，也可以加入c&lt;t-1&gt;，这称为<strong>peephole connection窥视孔连接</strong>（则gate的值也由记忆细胞的值一同决定）<br>5、GRU实在LSTM中做出的简化，GRU设计更简单，只有两个gate，容易创建更大规模更深的网络</p><h4 id="Bidirectionnal-RNN-双向RNN："><a href="#Bidirectionnal-RNN-双向RNN：" class="headerlink" title="Bidirectionnal RNN 双向RNN："></a>Bidirectionnal RNN 双向RNN：</h4><p>1、因为单向的 RNN中的单个单词只被其前面的单词所影响，但很多情况影响其的意义来源于后面的内容，因此需要双向RNN<br>2、BRNN，也是一个Acyclic graph无环图，正反向独立运行，（正反向前向传播均属于是前向传播）（可以由前后内容一同判断一个单词）缺点是，<span style="border-bottom:2px dashed red;">需要完整的序列才能预测任意位置，例如做语音识别，需要等这个人把话说完才能进行识别</span><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/34.png" alt="image"><br>3、<strong>y&lt;t&gt; = g(Wy[a→&lt;t&gt;,a←&lt;t&gt;]+by)</strong></p><h4 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs:"></a>Deep RNNs:</h4><p>1、在标准的RNN上，垂直方向多加三趟循环，加上水平时间线上输入x<tx>，已经是很深层了（与卷积100层不一样），会有很多隐含层，计算量十分大<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/35.png" alt="image"><br>2、a[2]&lt;3&gt; = g(Wa[2][a[2]&lt;2&gt;, a[1]&lt;3&gt;]+ba[2])<br>3、第1层有Wa[1]、ba[1]；第2层有Wa[2]、ba[2]；第3层有Wa[3]、ba[3]</tx></p><h4 id="Word-Representation-词汇表征："><a href="#Word-Representation-词汇表征：" class="headerlink" title="Word Representation 词汇表征："></a>Word Representation 词汇表征：</h4><p>1、<strong>Embedding 词嵌入</strong>：语言表示的一种方式，<span style="border-bottom:2px dashed red;">可以让算法自动的理解一些类似的词</span>（比如 男人女人、国王王后）（意思是将众多的单词按特征归类）<br>2、通过词嵌入可以构建NLP应用，即使模型的训练集相对小也可以（需要通过一些方法消除词嵌入的偏差）<br>3、用one-hot向量表示单词在字典的位置，会把单词都独立起来，这样泛化能力不强（<span style="border-bottom:2px dashed red;">可以通过300个特征来联系这些单词，但是实际上联系效果还是不够</span>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/36.png" alt="image"><br>4、<strong>特征化</strong>：可以将300维的特征向量嵌入到二维空间，即可可视化了（常用可视化算法是t-SNE算法，非线性）；可以更直观发现特征相似的词都聚集在一起<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/37.png" alt="image"><br>5、<strong>命名实体识别词嵌入的迁移学习</strong>：在大量无标签文本中学习大量词嵌入（也可以下载已训练好的模型，则可以将自己少量的训练集迁移学习，除非训练集很大，否则不需要再次调整词嵌入，直接使用就好）<br>6、词嵌入的运用十分广泛（<strong>named entity recognition命名实体识别</strong>、<strong>text summarization文本摘要</strong>、<strong>co-reference resolution 文本解析</strong>、<strong>parsing指代消解</strong>）；在语言模型之类的训练，则较少用到（因为会有大量数据）<br>7、词嵌入特征是能帮助实现<strong>analogy reasoning类比推理</strong>（虽然没有在NLP中着重运用，但能让人看到词嵌入干了什么，能干什么）；当男人能推出女人时，国王也能推出女王，得出他们差别都在性别（<strong>Eman-Ewomen ≈ Eking-Equeen</strong>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/38.png" alt="image"><br>8、求<strong>Ew</strong>(Equeen)：<strong>argmax sim(Ew, Eking-Eman+Ewomen)</strong> 相似最大化；通过方程找导最理想结果（准确率一般只有30%~70%）（常用的similarity function相似度函数时<strong>cosine similarity余弦相似度 sim(u,v)=(u^T * v)/(||u|| ||v||)，就是求u、v的俩向量的夹角Φ余弦值</strong>（cos函数中，Φ=0时，cos值是1；Φ=2PI 时，cos值是-1；就是两者越是相似，角度Φ越小，越接近1，越大则不相似则会接近-1））（<span style="border-bottom:2px dashed red;">相同关系的推断：线性，同特征的推断会使向量趋向平行</span>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/39.png" alt="image"><br>9、还可以用<strong>平方距离/欧式距离||u-v||^2</strong>求差异表示相似度<br>10、学习词嵌入就是学习一个嵌入矩阵，嵌入矩阵E，维度(300,1000)，与one-hot向量o，维度(1000,1)；<strong>E·o=e</strong>（o向量在E中的<strong>嵌入向量e</strong>(300,1)）（但不常用矩阵乘积的方法找e，计算量太大，矩阵乘积慢，会有单独函数方法来直接找到E中的哪一列）<br>11、词嵌入的NLP过程，求得每个嵌入向量e，最终汇聚于全连接层和softmax输出，做出多个单词的概率计算，最高概率则是预测单词（也可以仅看前4个单词即可推出，适当减少参数；或者仅提供其前面的/附近的一个单词也可）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/40.png" alt="image"><br>12、<strong>Word2Vec算法的skip-gram模型</strong>：（用于学习词嵌入更高效）抽取上下文单词于目标词匹配，来构造一个监督学习问题（Content c “orange” -&gt; Target t “juice”）（<strong>o_c -&gt; E -&gt; e_c -&gt; softmax -&gt; y^</strong>）（softmax：<strong>p(t|c)=(e^(θt^T * e_c))/(Σj e^(θj^T·e_c)</strong>)（θt是一个输出t的参数））（softmax损失函数：<strong>L(y^,y)=-Σi yi log y^</strong>）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/41.png" alt="image"><br>13、关键的p(t|c)中分母运算太大使得操作变慢；解决方法：使用hierarchical分级的softmax分类器（类似折中查找一样，每次二分为一查找，提高速度（也类似哈夫曼树，使用频率高的词靠近root））<br>14、<strong>Negative Sample 负采样</strong>：（与skip-gram模型做的相类似，但效率会更好）构造新的监督学习，给定一对单词，去预测是否是一对content-target的组合（<strong>context：orange-juice 输出target 1属于正样本；context：orange-king 输出target 0属于负样本</strong>）（k次随机在字典中寻找词与orange匹配得出负样本，提供作为训练集）（数据集小的话k一般取5~20；数据集大k选2~5即可）；可以使用<strong>logistic回归模型</strong>，p(y=1|c,t)=sigmoid(θt^T·e_c)；在整个训练中，训练一个正样本、随机k个负样本，就不需要每次在softmax中分母运算高纬度求和<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/42.png" alt="image"><br>15、在负采样中负样本的word如何选？推出该公式用于选取负样本的word（对词频的3/4次方，再求在总和的比例）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/43.png" alt="image"><br>16、（难）<strong>GloVe word vector 词向量算法</strong>：用词表示全局变量，明确context-target的关系Xij（i、jb表示两个词，Xij=Xji（若i、j相邻，则不符合这个对称等式）可将i、j定义为两个位置相近的单词，假设左右各10词的距离）；Xij可获取单词i、j出现位置相近时或者彼此接近时的频率的计数器<br>17、（难）<strong>GloVe’s Model</strong>：最小化它们的差值 <strong>minimize=ΣiΣj f(Xij)((θi^T·ej+bi+bj-logXij)^2</strong>；表示两个单词关系多紧密（防止Xij=0，导致无穷大，需添加一个weighting term加权项f(Xij)项，当Xij=0，f(Xij)=0）；<span style="border-bottom:2px dashed red;">不给频繁词过大的权重，不给少用词太小的权重</span>；θ和e是对称的，将其颠倒或排序，都可以输出最佳结果；训练方法是将他们一致地初始化，然后梯度下降来最小化输出，当每个词处理完之后，取平均值（e(find)=(e_w+θ_w)/2）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/44.png" alt="image"><br>18、<strong>Sentiment classification情感分析</strong>：分细一段评语，判断是否喜欢讨论的东西；这个任务的标记训练集可能没有那么多，可用词嵌入；可用于收集消费者的评价<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/45.png" alt="image"><br>19、训练嵌入矩阵E会很大维度（若在很大训练集上训练E，则会学到很多知识）；在特征向量求得需要平均化在送入softmax；<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/46.png" alt="image"><br>20、只把词的特征向量，加起来，可能就理解不了“反话”，则需要RNN<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/47.png" alt="image"><br>21、<strong>debiasing word embedding 词嵌入消除偏见</strong>：在使用词嵌入时需要减少或者消除词嵌入（一个完成词嵌入可能会输出Man:Computer_Programmer as Woman:Homemake / father:doctor as mother:nurse这种性别歧视的话语）；<span style="border-bottom:2px dashed red;">根据训练模型使用的文本，词嵌入能反映出性别、种族、年龄、性取向等的这些偏见</span>（这些偏见都跟社会经济状态相关，机器学习作出决策时不应该存在偏见）<br>22、步骤：1）辨别出想要减少的或者消除的特定偏见趋势（将(e_he - e_she)和(e_male - e_female)这类的男女性别的差值求和再简单取平均值）；2）neutralize step中和步，对那些定义不确切的词，可以将其处理下，避免偏见；3）Equalize pairs 均衡步，类似girl和boy能够有一致的相似度（距离中立词语能有相等距离）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/48.png" alt="image"></p><h4 id="Basic-Models-基础模型："><a href="#Basic-Models-基础模型：" class="headerlink" title="Basic Models 基础模型："></a>Basic Models 基础模型：</h4><p>1、<strong>seq2seq的机器翻译（法语转英语）</strong>：将法语句子序列转成输入向量x，英语句子为y；<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/49.png" alt="image"><br>输入需要先经过一个<strong>encoder network编码网络</strong>（属于RNN，隐含单元可以是GRU/LSTM）（<span style="border-bottom:2px dashed red;">输入法语，在输出一个能代替法语的 向量，更好进行训练</span>）；在后面可以建立一个<strong>decoder network解码网络</strong>（输出英语，直到输出结束标志）（这一部分RNN与语言模型的RNN基本一致）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/50.png" alt="image"><br>2、<strong>Image captioning图像描述</strong>（用一句话描述图片）：在卷积神经网络（AlexNet）中先识别图片（可以算作图像的编码网络（得到一个4096的一维向量）），然后用RNN生成图片的描述（作为解码网络）<br>3、可以将机器翻译称为有条件的语言模型<strong>P(y&lt;1&gt;,y&lt;2&gt;,…,y&lt;Ty&gt;|x)</strong>；<span style="border-bottom:2px dashed red;">在机器翻译可能会翻译出多条句子，虽然没错但不是最好的翻译</span>（最好的翻译结果句子的P(y|x)是最高的），则需要一种算法将找到合适的y值，再将其最大化（常用算法为<strong>Beam Search束搜索</strong>）（Greedy search贪心搜索算法不可用，最常用的搭配不见得是最好的翻译结果，并且每个词都需要遍历所有去寻找最好的，计算量是字典总数的指数级别）<br>4、<strong>Beam Search集束搜索</strong>：</p><ul><li>首先在解码网络中，需要在字典10000的单词中，挑选出输出的第一个单词P(y&lt;1&gt;|x)，会考虑选多个的单词（<span style="border-bottom:2px dashed red;">参数B，称为beam width集束宽，B=3则一次会考虑三个单词</span>）将所挑选的单词概率存入计算机内存里；</li><li><p>在所选的三个单词，分别再其基础上选择第二个单词P(y&lt;2&gt;|x,y&lt;1&gt;)，<span style="border-bottom:2px dashed red;">但最主要是找到最大概率<strong>P(y&lt;1&gt;,y&lt;2&gt;|x) = P(y&lt;1&gt;|x) * P(y&lt;2&gt;|x,y&lt;1&gt;)</strong></span>；第二步就有30000种可能了，然后在这30000可能中找出3个最高的概率的y&lt;1&gt;,y&lt;2&gt;继续下个30000的概率选择；主要乘积概率公式如下：<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/51.png" alt="image"></p></li><li><p>重复类似第二步的操作，每一步增加一个单词，直至识别结束符号；当B=1时，就变成了贪婪搜索</p></li></ul><p>5、<strong>Length normalization 长度归一化</strong>：对集束搜索的稍微调整；因为通过beam search公式所求概率通常小于1，多次小于1的概率乘积，会变得很小很小，导致数值下溢，计算机精度不够无法精准存储；<span style="border-bottom:2px dashed red;">改造公式为对数的求和，更加稳定，不容易出现四舍五入的误差避，免数值下溢（对数函数单调递增，所以最大化logP(y|x)与最大化P(y|x)是一回事）</span>；<br>6、（因为每次都会乘以小于1的概率，所以也会偏向更少数量的输出）<span style="border-bottom:2px dashed red;">因为最短也不一定是最好的输出，所以不再最大化概率了，改为归一化，求概率对数的平均值</span>；在取平均数时，如何选择分母才会更好，在分母设置参数α，Ty^a作为分母会更柔和一点，α可以设置为0.7；α=0则等于没有归一化，α=1则等于完全用长度来归一化；用调整后的α会得到更好的结果<br>7、对于B的选择，一般越多还是越好，但过多计算成本就会变高；B很大的情况：会得到更好的结果，计算会慢一些，占用内存大；B很小的情况：结果不会很好，但计算快，占用内存少；<span style="border-bottom:2px dashed red;">一般在实际使用会设置B=10</span>；设置为100就太大了，但在科研需要更好的效果则会设置成1000或者3000<br>8、运用广度优先搜索或者深度优先搜索可能会比集束搜索速度更快，但不能保证一定找到最大化的精准的最大值<br>9、误差分析可以节省时间，用更多时间投入更有用的工作<br>10、集束搜索算法是一种<strong>approximate search近似搜索算法/heuristic search启发式搜索算法</strong>，不直接输出最好的句子，而是记录多个好句子，再输出最好的<br>11、人工翻译的句子记为y*、RNN输出结果记为y^；人工翻译比RNN输出要好的前提下，对比P(y*|x)和P(y^|x)（忽略归一化复杂情况下）</p><ul><li>case1：P(y*|x)&gt;P(y^|x)，则beam search出问题了（加大B），没有找到更大的y^</li><li>case2：P(y*|x)&lt;=P(y^|x)，则是RNN出问题了（加深网络），y*翻译试比y^好的，但RNN算出P(y*|x)&lt;=P(y^|x)</li></ul><p>12、持续多次句子翻译的对比P(y*|x)和P(y^|x)，出现最多错的就是最主要误差问题</p><h4 id="Bleu-score："><a href="#Bleu-score：" class="headerlink" title="Bleu score："></a>Bleu score：</h4><p>1、用于代替人类去评估机器翻译结果，衡量准确性（给定一个翻译结果，计算出一个分数，接近人类的翻译就高分）；衡量机器翻译质量的方法之一是<span style="border-bottom:2px dashed red;">观察输出结果的每一个词，看其是否出现在人工翻译参考当中</span>（精确度）（但是出现个别情况，输出所有单词一样，并且其都在人工翻译中出现，会出现精度很好，但翻译出的句子不行）；改良后的方法：<span style="border-bottom:2px dashed red;">记录每一个词在出现在人工翻译中的次数，在多个人工翻译句子里面出现最多次数视为上限，上限为分子，在机器翻译的结果中出现的次数为分母</span>；<strong>体现出翻译结果与人工翻译的重叠层度</strong>（若是与人工翻译一致，则P=1.0）（可以下载已有的模型）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/52.png" alt="image"><br>2、<strong>Bleu score on brigrams（二元组）（相邻两个单词）</strong>：（unigrams一元组）（也会有trigrams三元组）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/53.png" alt="image"><br>3、一元组精度公式：<strong>P1=Σ</strong>unigram∈y^ <em> <strong>Count</strong>clip<strong>(unigram) / Σ</strong>unigram∈y^ </em> <strong>Count(unigram)</strong><br>4、n元组精度公式：<strong>Pn=Σ</strong>n-gram∈y^ <em> <strong>Count</strong>clip<strong>(n-gram) / Σ</strong>n-gram∈y^ </em> <strong>Count(n-gram)</strong><br>5、结合Bleu score，采用BP（<strong>brevity penalty简短惩罚</strong>）的惩罚因子作为调整因子，<strong>BPexp(1/4 Σ4 Pn)</strong>（对P1、P2、P3、P4取均值）；<span style="border-bottom:2px dashed red;">若是输出一个很短的翻译则会得到很高的精确度，但简短的输出也不一定是最好</span>；机器翻译长度大于人工翻译，BP=1<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/54.png" alt="image"></p><h4 id="（难）Attention-Model-注意力模型："><a href="#（难）Attention-Model-注意力模型：" class="headerlink" title="（难）Attention Model 注意力模型："></a>（难）Attention Model 注意力模型：</h4><p>1、在翻译一段文本中，人类大部分是翻译一部分，再翻译下一部分，而不是看完全部，再靠记忆去由零开始翻译<br>2、Bleu score在短文翻译和长达30字以上的文本翻译都会比较难，所以分数较低<br>3、注意力模型就会让其变得更像人类一样翻译文本，在长文本中表现良好<br>4、<span style="border-bottom:2px dashed red;">利用BRNN，但每层的输出并不会直接输出翻译结果，而是输出<strong>α&lt;t,t’&gt;“注意力权重”</strong>用于接入新的RNN的<strong>S&lt;t&gt;“注意力权重集”</strong>（注意力权重则是一个注意力权重集对BRNN这层的所需要的注意力），S&lt;t&gt;的输入会是对应单词和其上下文单词（多个输入），再逐个输出翻译的单词</span><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/55.png" alt="image"><br>5、上下文的定义是<span style="border-bottom:2px dashed red;">被注意力权重除权的不同步时间中的特征值</span>；BRNN多个输出α&lt;t,t’&gt;相加得出<strong>c&lt;t&gt;</strong>，c&lt;t&gt;在统一输入注意力权重集（<strong>c&lt;t&gt;=Σα&lt;t,t’&gt;*a&lt;t’&gt;</strong>）（<strong>Σα&lt;t,t’&gt;=1</strong>）（用softmax来确保这些权重加起来等于1）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/56.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/57.png" alt="image"><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/58.png" alt="image"><br>6、如果想要决定花多少注意力在t’的激活值上，很大程度取决于上一个时间步的s&lt;t-1&gt;的隐藏状态的激活值<br>7、缺点是算法复杂度是O(n^3)，总参数个数会是Tx*Ty，但是一部分一部分翻译的话，这个消耗还是很可接受<br>8、注意力模型还被应用于将任何形式的时间表达方法转化成标准时间显示方式<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/59.png" alt="image"><br>9、注意力权重的可视化，需要更多注意力的色块会更亮<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/60.png" alt="image"></p><h4 id="Speech-Recognition-语音识别："><a href="#Speech-Recognition-语音识别：" class="headerlink" title="Speech Recognition 语音识别："></a>Speech Recognition 语音识别：</h4><p>1、seq2seq在音频上的应用（输入音频，输出文本）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/61.png" alt="image"><br>2、音频数据的常见预处理步骤：运行这个原始的音频片段，然后生成一个<strong>spectrogram声谱图</strong>（不同颜色表示声波能量的大小）；<strong>false blank output伪空白输出</strong>（模仿人耳）也常应用于预处理<br>3、曾经也有人工设计的<strong>phonemes音位</strong>作为基本单元（就是听发音判别单词）<br>4、采用<strong>CTC损失函数（connectionist temporal classification）</strong>来做语音识别；例如”the quick brown fox”一句话十秒，每秒100hz，则有1000层输入（模型较为简单，但很深）；虽然输入有1000，但输出是没有1000，则需要用到CTC，整合输出”ttt__h_eee____ __qq__…”类似的以赫兹输出音频（CTC：将空格之间的重复字符折叠起来成为一个单词）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/62.png" alt="image"></p><h4 id="Trigger-word-detect触发检字："><a href="#Trigger-word-detect触发检字：" class="headerlink" title="Trigger word detect触发检字："></a>Trigger word detect触发检字：</h4><p>1、类似”ok，google！”一样的唤醒语音系统，然后发出一条语音指令，再识别后执行某些事<br>2、设备一直检测着附近的音频，输入音频，提取出音频中的特征，将特征输入RNN，再没有识别到关键字的时候，输出0，检测到关键字时输出1；缺点是0太多了，导致训练集很不平衡；<span style="border-bottom:2px dashed red;">一个暴力解决方法（在输出1后，固定一定时间保持输出1，达到一定平衡）</span>，起码是训练集更加平衡些，更好于训练<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/63.png" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Sequence-Model-序列模型（包括RNN）：&quot;&gt;&lt;a href=&quot;#Sequence-Model-序列模型（包括RNN）：&quot; class=&quot;headerlink&quot; title=&quot;Sequence Model 序列模型（包括RNN）：&quot;&gt;&lt;/a&gt;Seque
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Blockchain</title>
    <link href="http://yoursite.com/2019/07/19/Blockchain/"/>
    <id>http://yoursite.com/2019/07/19/Blockchain/</id>
    <published>2019-07-19T00:51:03.000Z</published>
    <updated>2019-11-14T12:46:46.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>比特币和以太坊均是区块链技术下一种<strong>crypto-currency加密货币</strong></li><li><strong>比特币：</strong>密码学基础、比特币的数据结构、共识协议和系统实现、挖矿算法和难度调整、比特币脚本、软分叉和硬分叉、匿名和隐私保护</li><li><strong>以太坊：</strong>基于账户的分布式账本、数据结构（状态树、交易树、收据树）、GHOST协议、挖矿（memory-hard mining puzzle）、挖矿难度调整、权益证明（Casper the Friendly Finality Gadget(FFG)）、智能合约</li></ul><h4 id="密码学基础："><a href="#密码学基础：" class="headerlink" title="密码学基础："></a>密码学基础：</h4><p>1、加密货币是不加密的 ，所有信息是公开的，交易金额与时间都是公开的<br>2、比特币中用到密码学的两大功能：<strong>哈希</strong>、<strong>签名</strong><br>3、<strong>cryptographic hash function 哈希</strong>：有两大性质（<strong>collision resistance哈希碰撞</strong>）（<strong>hiding计算过程是单向的、不可逆的</strong>）<br>4、哈希碰撞：当有两个互不相等的输入x和y，而H(x)=H(y)），则称为哈希碰撞；其是不可避免地，但是客观存在；作用是在H(m)计算出来，很难几乎找不到有另外m’的H(m’)=H(m)，则是<span style="border-bottom:2px dashed red;">没有办法再对m作出修改后，不被检测出来（找不到人为的哈希碰撞）</span><br>5、hiding：计算过程无法逆推，<span style="border-bottom:2px dashed red;">则是哈希结果并不会泄露原本信息</span>（前提是输入也是足够大，输入分布要比较均匀，取值概率差不多，否则可以蛮力破解）<br>6、<strong>collision resistance + hiding = digital commitment = digital equivalent of a sealed envelope</strong>    （sealed envelope表示现实生活中的公信机制）<br>7、在输入数量数值不够大时，<span style="border-bottom:2px dashed red;">需要随机数nonce</span>，则H(x)，变为H(x||nonce)，才不会被蛮力破解<br>8、除了密码学基础两大性质之后，还需要一个性质<strong>puzzle friendly</strong>（<span style="border-bottom:2px dashed red;">指就算得到哈希结果，事先是不知道哪个输入会得到该结果</span>）<br>9、挖矿就是寻找nonce，nonce与块头的其他信息组合，然后需要H(block header)&lt;=target（区块链时链表，每块有个块头，块头有很多域设置信息，其中一个就是设置nonce）（<span style="border-bottom:2px dashed red;">挖矿就是不断试不同nonce，然后放入块头进行哈希运算，得出结果落在target space指定范围内</span>）（只有不断试nonce才行，没有其他方法，这也代表了工作量）（一旦找到nonce发布出去，计算一次哈希就可以验证是否正确，称为difficult to solve解决很难，easy to verify验证很简单）<br>10、比特币中所用的哈希函数：<strong>SHA-256</strong>（Secure Hash Algorithm）<br>11、比特币账户，去中心化，在本地创立公钥与私钥（public key, private key）即可，就是一个账户，运用asymmetric encryption algorithm非对称加密算法（<strong>加密用公钥，解密用私钥</strong>）（公钥是公开的（相当于账号），私钥是保留本地的（相当于账户密码），<span style="border-bottom:2px dashed red;">发送方用接收方的公钥加密信息，接收方用其私钥解密</span>）<br>12、公钥 私钥适用于签名，每次交易，<span style="border-bottom:2px dashed red;">发送方需要在交易信息发送前用私钥进行签名</span>（才能确保是发送方发出，而不是冒名顶替，可以用公钥验证是否本人）<br>13、两个人公钥私钥相同的概率可忽略不计（前提是生成公钥私钥的随机源要好，否则就会出现该情况）</p><h4 id="比特币的数据结构："><a href="#比特币的数据结构：" class="headerlink" title="比特币的数据结构："></a>比特币的数据结构：</h4><p>1、hash pointer哈希指针：需要存地址和哈希值<br>2、区块链与普通链表区别是：用了哈希指针代替了普通指针（第一个块是系统产生的，称为genesis block创世纪块），这个结构可以实现tamper-evident log防篡改日志，<span style="border-bottom:2px dashed red;">一旦篡改区块的哈希信息，后面区块哈希信息不匹配</span>，就连接不上了（有了这个机制，可以仅保留目前几个区块就可以了，要是需要前面的区块，可以向系统索取）<br>3、Merkle Tree（结构之一）：哈希指针的二叉树（叶子节点都是data block数据块（交易信息），非叶子节点都是hash point哈希指针）（知道一个root的哈希值，只要有一个地方发生改变，均可知道，只要一改变，哈希值就对不上了）（树比链表结构更优，遍历速度快）<br>4、每个区块分成block header块头和block body块身，交易信息在块身<br>5、<strong>merkle tree</strong>提供merkle proof证明<br>6、比特币节点分成两类：<span style="border-bottom:2px dashed red;">全节点（有块头块身，有交易信息，类似比特币客户端）清节点（仅保留块头，块头仅有root哈希值，用于个人用户，则无法知道交易是否被提交到区块链，需要merkle proof验证）</span><br>7、merkle proof就是可以查询交易的叶子节点，一直往根节点查询，整个路径；<span style="border-bottom:2px dashed red;">需要向全节点请求提供图中红色节点哈希值，即可计算出root哈希值，再进行对比</span>即可知道是否存在该交易（但仅仅能验证该分支的正确性，其他分支无法验证），merkle proof过程称为proof of memebership / proofof inclusion，其复杂度为O(log(n))<br><img src="https://github.com/soloistben/images/raw/master/block_chain_image/merkle_tree.jpg" alt="image"><br>8、要证明一个交易节点不在merkle tree中，需要得到整个树，得到所有叶子节点，在排除，复杂度是O(n)；若是先将叶子节点的哈希值进行排序，则可以根据哈希值找到两个叶子节点（即范围），在验证一下俩节点哈希值往上到root哈希是否正确，若正确，则该交易点不在merkle树中<br>9、只要无环数据结构，都可以用上哈希指针，有环就不可以</p><h4 id="共识协议："><a href="#共识协议：" class="headerlink" title="共识协议："></a>共识协议：</h4><p>pass</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;比特币和以太坊均是区块链技术下一种&lt;strong&gt;crypto-currency加密货币&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;比特币：&lt;/strong&gt;密码学基础、比特币的数据结构、共识协议和系统实现、挖矿算法和难度调整、比特币脚本、软分叉和硬分
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>DL_CNN</title>
    <link href="http://yoursite.com/2019/07/03/DL-CNN/"/>
    <id>http://yoursite.com/2019/07/03/DL-CNN/</id>
    <published>2019-07-03T02:29:01.000Z</published>
    <updated>2020-05-20T02:46:40.036Z</updated>
    
    <content type="html"><![CDATA[<h4 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h4><p>1、sigmoid常用于二元分类在输出时做激活函数，其他地方tanh更优于sigmoid<br>2、tanh和sigmoid在过大过小时都会出现梯度为零（斜率为0，与x轴平行），则更多选择使用relu（rectified linear unit修正线性单元）（在x=0处不可导，但实际情况很少会出现靠近0的数，则可以忽略）（不确定用什么激活函数，就用relu）（<span style="border-bottom:2px dashed red;">relu不会出现梯度弥散</span>，则会更快）<br>3、 leaky relu（g(z)=max(0.01z,z)）在x&lt;0时有梯度 ，relu在x&lt;0时没有梯度<br>4、选择什么激活函数：在多事几个激活函数训练下，选择使参数更优的激活函数<br>5、若激活函数全是线性函数，则隐含层没有意义，整个神经网络都仅仅只是线性运算<br>6、线性函数作为激活函数一般是在输出时（全连接层）<br>7、实际上大多数现象呈现关系都是正相关关系，并非线性关系，因此用非线性函数作为激活函数是更适合表达正相关关系</p><h4 id="激活函数的导数："><a href="#激活函数的导数：" class="headerlink" title="激活函数的导数："></a>激活函数的导数：</h4><p>1、sigmoid的导数 =&gt; g’(z) =g(z)(1-g(z))，z=0时g’(0)=1/4<br>2、tanh的导数 =&gt; g’(z) = 1-(g(z))^2，z=0时g’(0)=1<br>3、relu的导数 =&gt; g’=0(z&lt;0),g’=1(z&gt;0),z=0时，g’=0或1都可以（不重要，z=0概率太小）<br>4、leaky relu的导数 =&gt; g’=0.01(z&lt;0),g’=1(z&gt;0)</p><h4 id="梯度下降："><a href="#梯度下降：" class="headerlink" title="梯度下降："></a>梯度下降：</h4><p>1、不可以将所有参数初始化为0，否则梯度下降会失效（偏置初始化为0可以，权重初始化为0则会出现问题）<br>2、权重初始化一般都比较小，过大会在tanh和sigmoid中梯度太小甚至出现梯度弥散，使得梯度下降过慢</p><h4 id="前向传播反向传播："><a href="#前向传播反向传播：" class="headerlink" title="前向传播反向传播："></a>前向传播反向传播：</h4><p>1、为什么要更深的神经网络？<br>​    在相同节点情况下，深比浅更快（树的类型）<br>2、每次执行一个神经元节点运算（激活函数g(wx+b)），都会缓存z=wx+b的值，作为激活函数的输入和反向传播的输入</p><h4 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h4><p>1、反向传播输出每一层的dw和db，则会更新下一轮的w=w-dw，b=b-db（<span style="border-bottom:2px dashed red;">参数越来越优，越接近真实值，dw、db就会越来越小</span>）<br>2、参数：w、b  超参数：学习率，激活函数，隐含层层数等（均影响w和b的值）<br>3、深度学习，机器学习是迭代的过程，不可能一开始就直接把所有超参数全部设置好</p><h4 id="数据划分："><a href="#数据划分：" class="headerlink" title="数据划分："></a>数据划分：</h4><p>1、数据集一般分为train训练集、dev验证集（验证方差、偏差）、test测试集（泛化性）<br>2、对于小数据，训练:测试=&gt;7:3，训练:验证:测试=6:2:2<br>3、对于大数据，验证集与测试集只需要小比例数据即可，验证集有1w(1%)数据即可用于在多种算法选出最优的几个算法，测试集有1w(1%)就可以评估模型性能，尽可能多数据用于训练<br>4、<span style="border-bottom:2px dashed red;">直接用网络图片，会导致训练集与测试集和验证集不是来自于同一分布</span></p><h4 id="方差与偏差："><a href="#方差与偏差：" class="headerlink" title="方差与偏差："></a>方差与偏差：</h4><p>1、训练集误差小，验证集误差大，可能会是过拟合，属于高方差<br>2、训练集与验证集误差相差不大，但其他分类有明显差距，可能是欠拟合，属于高偏差<br>3、训练集误差大，验证集误差更大，则属于高偏差与高方差<br>4、训练集验证集的误差都小，则属于低偏差与低方差<br>5、一般最优误差是贝叶斯误差，几乎接近0误差<br>6、若高偏差，则需要更深的神经网络or更长时间训练or更好的优化算法<br>7、若偏差不高，则判断是否高方差<br>8、若高方差，则需要更多数据or正则化<br>9、优化目标要做到低方差、低偏差</p><h4 id="正则化："><a href="#正则化：" class="headerlink" title="正则化："></a>正则化：</h4><p>1、用L1正则化，会使w变稀疏，但未改变所占内存大小，并不便于减小权重<br>2、常用L2正则化，但计算量会增大<br>3、使用正则项，可避免数据权值矩阵过大，正则系数过大，使w过小，会减少过拟合，接近高偏差状态<br>4、w越小，z=wx+b也越小，靠近0处的范围，则在tanh和sigmoid激活函数中，激活函数大致呈线性状态，使梯度（斜率）保持较大状态<br>5、在代价函数J加入正则化项，是为了预防权重过大<br>6、代价函数对于梯度下降的每个调幅都单调递减</p><h4 id="Dropout："><a href="#Dropout：" class="headerlink" title="Dropout："></a>Dropout：</h4><p>1、dropout中的keep-prop概率表示每个隐藏单元的不被消除概率<br>2、使用dropout会减小权重，类似正则化，并且完成了一些预防过拟合的外层正则化<br>3、<span style="border-bottom:2px dashed red;">在过拟合严重处可降低keep-prop，不担心过拟合处可提高keep-prop</span><br>4、一般不在输入层使用dropout<br>5、除非出现过拟合状态，否则不用dropout，<span style="border-bottom:2px dashed red;">在计算机视觉常用dropout，在别的领域少用</span><br>6、dropout缺点是代价函数J不再被明确定义<br>7、先执行代码，确保代价函数是单调递减，再使用dropout</p><h4 id="其他减少过拟合方法："><a href="#其他减少过拟合方法：" class="headerlink" title="其他减少过拟合方法："></a>其他减少过拟合方法：</h4><p>1、data augment数据增强，在少图片数据时增多图片数据，有一定减少拟合作用<br>2、early stopping提早结束，在出现过拟合前（验证集误差提高前）获得中等大小的w范数，提早结束训练。缺点时停止了代价函数的降低和停止了优化</p><h4 id="正则化输入："><a href="#正则化输入：" class="headerlink" title="正则化输入："></a>正则化输入：</h4><p>1、归一化特征值输入可以加速神经网络的训练，将代价函数的图像不均匀显示（输入参数之间取值范围差别过大），则将图像显示更加均匀（重建坐标系将输入参数之间范围缩至相差不多）则可以使梯度下降更快<br>2、z=wx+b （w比1大）过大引起梯度爆炸，（w比1小）过小引起梯度弥散<br>3、高斯随机变量初始化权重w，会有一定程度减少发生梯度爆炸和梯度弥散的情况（relu激活函数使用2/n的方差，tanh使用1/n的方差）</p><h4 id="梯度检测："><a href="#梯度检测：" class="headerlink" title="梯度检测："></a>梯度检测：</h4><p>1、梯度检测不要用在训练，用于调试（dθapprox[i]计算需要很长时间，dθ要反向传播计算，所以在调试计算比较好）<br>2、梯度检测结果不好，要检查所有项，找出bug<br>3、若使用了正则化，代价函数不能漏掉正则项<br>4、<span style="border-bottom:2px dashed red;">梯度检测不能使用dropout，使用dropout无法确认代价函数</span></p><h4 id="mini-batch："><a href="#mini-batch：" class="headerlink" title="mini batch："></a>mini batch：</h4><p>1、batch训练，每次迭代都需要遍历整个训练集。理论上，代价函数是单调递减，若是出现增加，则必然有bug或者是一次运行的batch太大（运算时间很长）<br>2、mini batch训练，并不是每一次的迭代都是下降的，整体是单调递减，但会有很多噪声，出现噪音的原因是每个mini batch运算难度不一<br>3、当mini batch=1时，叫随机梯度下降法，每个样本都时独立的mini batch，每次运算都是局部最优，不一定朝着最小值方向，其代价函数永远不收敛，而是在最小值附件波动，缺点是失去所有向量化的运算加速、每次只计算一个会效率低。<br>4、通过减少学习率，噪声会有所减少<br>5、选择折中的mini batch大小，会有更快学习速度，既有向量化的运算加速，且不需要等待整个训练集被处理完就可以开始后续工作<br>6、若是训练集少于2000，直接使用batch会更好<br>7、一般mini batch大小：64、128、256、512，选择2的次方会更快</p><h4 id="指数加权平均（加权移动平均值-滑动平均）："><a href="#指数加权平均（加权移动平均值-滑动平均）：" class="headerlink" title="指数加权平均（加权移动平均值/滑动平均）："></a>指数加权平均（加权移动平均值/滑动平均）：</h4><p>1、Vt=βVt-1 + (1-β)θt     Vt==1/(1-β)<br>2、β=0.9，Vt==10，β=0.98，Vt==50，β=0.5，Vt==2<br>3、用于调整参数，选择中间值会得到更好的效果<br>4、指数加权平均运算占用内存少<br>5、指数加权平均往往比直接求平均数得到更好的估测，缺点是需要保存数据多</p><h4 id="bias-correction偏差修正："><a href="#bias-correction偏差修正：" class="headerlink" title="bias correction偏差修正："></a>bias correction偏差修正：</h4><p>1、偏差修正让平均数运算更加准确<br>2、β约靠近1，在估测初期，值会偏低，则不使用Vt，而使用Vt/(1-β^t)，可修复初期偏低情况</p><h4 id="Momentum梯度下降："><a href="#Momentum梯度下降：" class="headerlink" title="Momentum梯度下降："></a>Momentum梯度下降：</h4><p>1、学习率过大，会摆动过大<br>2、momentum使用指数加权平均数在每次梯度运算更新dw、db的值，会减少噪音（摆动）、更快更直接的到达最小值<br>3、Vdw = βVdw + (1-β)dw<br>​     Vdb = βVdb + (1-β)db<br>​     w = w - αVdw  b = b - αVdb<br>​     超参数：α(学习率)、β(指数加权平均数，常用0.9)<br>4、第二版：Vdw = βVdw + dw去掉了(1-β)，β仍然是0.9，α要相应改变，效果差不多，但更偏向上者</p><h4 id="RMSprop（root-mean-square-prop）均方根："><a href="#RMSprop（root-mean-square-prop）均方根：" class="headerlink" title="RMSprop（root mean square prop）均方根："></a>RMSprop（root mean square prop）均方根：</h4><p>1、与momentum类似，可以加速梯度下降<br>2、w决定水平方向（前进方向），b决定垂直方向（噪音、摆动方向），需要让减缓摆动，希望dw小一点使w更新后跟小，db大一点使b更新后变化不大，从而减少噪音。但需要一个较大的学习率α来加快学习<br>3、Sdw = βSdw + (1-β)(dw)^2<br>​     Sdb = βSdb + (1-β)(db)^2<br>​     w = w - αdw/(√Sdw)  b = b - αdb/(√Sdb)</p><h4 id="Adam优化算法-Adaptive-Moment-Estimation-Momentum-RMSprop"><a href="#Adam优化算法-Adaptive-Moment-Estimation-Momentum-RMSprop" class="headerlink" title="Adam优化算法(Adaptive Moment Estimation)=Momentum+RMSprop:"></a>Adam优化算法(Adaptive Moment Estimation)=Momentum+RMSprop:</h4><p>1、初始化Vdw=0，Sdw=0，Vdb=0，Sdb=0<br>​     t in loop(using mini batch)：<br>​            (momentum)<br>​        Vdw = β1Vdw + (1-β1)dw<br>​        Vdb = β1Vdb + (1-β1)db<br>​            (RMSprop)<br>​        Sdw = β2Sdw + (1-β2)(dw)^2<br>​        Sdb = β2Sdb + (1-β2)(db)^2<br>​           (bias correction)<br>​        Vdw = Vdw/(1-β1^t)    Vdb = Vdb/(1-β1^t)<br>​        Sdw = Sdw/(1-β2^t)    Sdb = Sdb/(1-β2^t)<br>​            (updata w&amp;b)<br>​        w = w - αVdw/(√(Sdw+ε))     b = b - αVdb/(√(Sdb+ε))   （debug α value，β1=0.9，β2=0.999，ε=10^-8(可不设置)）<br>2、结合momentum和RMSprop，适合更加广泛的神经网络</p><h4 id="Learning-rate-decay衰减学习率："><a href="#Learning-rate-decay衰减学习率：" class="headerlink" title="Learning rate decay衰减学习率："></a>Learning rate decay衰减学习率：</h4><p>1、随着训练递进，逐步慢慢减少学习率，从而加快学习速度（学习率过大无法收敛）<br>2、batch分成多个mini batch后，第一次遍历叫epoch1，epoch-num也表示遍历到第几个mini batch<br>3、α = α0/(1+decay-rate*epoch-num)<br>4、当α0=0.2，decay-rate=1时<br>​    epoch  α<br>​        1      0.1<br>​        2      0.67<br>​        3      0.5<br>​        4      0.4<br>5、学习率递减<br>6、指数衰减、离散下降学习率=每次下降减半、手动衰减</p><h4 id="局部最优："><a href="#局部最优：" class="headerlink" title="局部最优："></a>局部最优：</h4><p>1、通常梯度为0的点不是最优点<br>2、代价函数中的梯度为0的点叫鞍点<br>3、在高维度神经网络中，一般不会被困于较差的局部最优<br>4、图形平稳的的地方，会让学习十分缓慢（需要优化算法加速）</p><h4 id="调试处理："><a href="#调试处理：" class="headerlink" title="调试处理："></a>调试处理：</h4><p>1、调试先后重要程度：学习率α、momentum的β、mini batch 大小、隐含节点数、层数、衰减学习率<br>2、调参数之前不知到那个参数更重要，当选择两个参数时，若选择学习率alpha和adam的ε，α会相对更重要，ε取值变化不大，<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/1.PNG" alt="image"><br>3、若是在较优点附近的点效果也不错，则需要扩大这几个点范围，再次密集随机取点<br>4、在数轴上取值，随机均匀取样比较合理<br>5、学习率α范围在[0.0001,1]，选择在[10^a,10^b]内取值<br>6、β范围在[0.9,0.999]不好取值（0.9表示10个元素之间的平均，0.999表示1000个元素之间的平均），1-β范围在[0.001,0.1]容易取值，越靠近1取值变化会很大，若β在0.9到0.9005变化不大，而0.999到0.9995就相差很大</p><h4 id="batch归一化："><a href="#batch归一化：" class="headerlink" title="batch归一化："></a>batch归一化：</h4><p>1、归一化输入，可加快学习过程<br>2、在隐含节点中归一化的是z而不是激活函数后的a<br>3、batch归一化与mini batch一起用<br>4、batch归一化类似dropout带来一些噪音，有轻微正则化效果</p><h4 id="softmax分类："><a href="#softmax分类：" class="headerlink" title="softmax分类："></a>softmax分类：</h4><p>1、特点是将输出归一化，输入向量，输出向量</p><h4 id="ML策略："><a href="#ML策略：" class="headerlink" title="ML策略："></a>ML策略：</h4><p>当识别率为90%时，仍远远不够，需要<br>1）收集更多图片数据<br>2）收集更多样的训练数据集<br>3）使用梯度下降算法优化时，训练更长时间<br>4）使用Adam优化算法<br>5）设计一个更大或更小的神经网络<br>6）用dropout或L2正则化<br>7）修改网络框架（更换激活函数、改变隐含节点）</p><h4 id="正交化："><a href="#正交化：" class="headerlink" title="正交化："></a>正交化：</h4><p>1、可以调整的参数设置在不同的正交的维度上，调整其中一个参数，不会或几乎不会影响其他维度上的参数变化，更容易更快速地将参数调整到一个比较好的数值<br>2、若是在训练集表现不好，用更宽更深神经网络、用Adam优化算法<br>3、若是在验证集表现不好，可调节正则化、用更多训练数据<br>4、若是在测试集表现不好，用更多验证数据<br>5、若是在投入真实使用表现不好，需要修改代价函数J、验证集数据<br>6、测试集表现好坏与真实使用表现无关</p><h4 id="单一指数评估指标："><a href="#单一指数评估指标：" class="headerlink" title="单一指数评估指标："></a>单一指数评估指标：</h4><p>1、查准率（表示分类结果的百分比）查全率（表示分类结果对的所占百分比）两者之间往往需要折中<br>2、在查准率表现好的未必在查全率表现好<br>3、不能仅依靠查准率和查全率来选择训练好的分类器，F1 score是查准率P和查全率R的调和平均，更好去选择分类器<br>4、F1 score = 2/(1/P + 1/R)<br>5、当需要顾及多个指标，在观察多个成本大小时，选出最好的那个<br>6、测试集和验证集必须来自于同一分布，逼近同一目标<br>7、大数据时代，大量数据作为训练，少了量分给验证与测试<br>8、为了规避识别图片将情色图片识别进去目标图片范畴，给损失函数加个权重，限制情色图片（图片处理需要将情色图片具体标出来）原：J = 1/mΣL(y-yi)，修改后：J = 1/Σwi * ΣwiL(y-yi)<br>9、定义指标是为了选择出更好的分类器（1、如何定义一个指标衡量想做事情的表现；2、分开考虑如改善系统在指标上的表现）<br>10、当在测试集和验证集中表现好，在实际应用却不好，则需要修改指标，或者改变测试集</p><h4 id="机器学习与人对比："><a href="#机器学习与人对比：" class="headerlink" title="机器学习与人对比："></a>机器学习与人对比：</h4><p>1、通过大量数据训练，机器学习表现会出现超过人类，然后平稳，<span style="border-bottom:2px dashed red;">但始终会有一个性能无法超越的理论上限，叫贝叶斯最优误差Bayes optimal</span><br>2、将贝叶斯误差估计和训练误差之间的误差称为可避免偏差avoidable bias，训练误差与验证误差之间属于方差，哪个误差大就调整哪个<br>3、若训练集误差比贝叶斯误差还好，则是过拟合了<br>4、human-level performance人类水平表现，人类水平误差作为贝叶斯误差的代表<br>5、贝叶斯误差一般为小于0.5%<br>6、可避免偏差：更大模型、更优算法、训练更久、超参数搜索（找到更好神经网络架构）<br>7、方差：更多数据、正则化、dropout、数据增强、超参数搜索</p><h4 id="误差分析："><a href="#误差分析：" class="headerlink" title="误差分析："></a>误差分析：</h4><p>1、收集更多狗图片数据，喂入猫分类器，学习更多，以至于更好区别猫和狗<br>2、在错误分类中，找出问题较为严重的给予解决，抓住问题根本，例如照片模糊影响分类器比例比将狗识别为猫比例更大，应着手解决模糊问题<br>3、监督学习，在大量训练数据下，允许少量标记错误<br>4、验证数据做了修正，测试数据也需要做出同样的修正<br>5、同时检验算法判断正确和判断错误的例子，这才公平，否则对算法的偏差估计可能会变大</p><h4 id="语音识别："><a href="#语音识别：" class="headerlink" title="语音识别："></a>语音识别：</h4><p>1、背景噪音处理<br>2、口音处理<br>3、麦克风过远处理<br>4、儿童语音识别<br>5、口吃、感叹词处理</p><h4 id="神经网络创建准备："><a href="#神经网络创建准备：" class="headerlink" title="神经网络创建准备："></a>神经网络创建准备：</h4><p>1、设立验证集和测试集，还有指标<br>2、搭好机器学习系统原型，用训练集训练一下查看效果，理解算法的表现<br>3、<span style="border-bottom:2px dashed red;">用偏差和方差分析，决定优化方向</span><br>4、想出所有能走的方向，选择实际上最有希望的方向<br>5、若是搭载已成熟的方向，有大量论文理论支撑，可以直接搭载复杂的神经网络，例如人脸识别<br>6、若是新的方向，先由简单神经网络开始<br>7、主要是造出能用的神经网络模型，而不是<strong>发明全新的机器学习算法</strong></p><h4 id="训练集与测试集不是来自同一分布："><a href="#训练集与测试集不是来自同一分布：" class="headerlink" title="训练集与测试集不是来自同一分布："></a>训练集与测试集不是来自同一分布：</h4><p>1、option1：将来自不同分布的图片合并在一起（不推荐）<br>2、option2：若分布1是较多的，训练集=分布1+0.5×分布2，验证集=0.25×分部2，测试集=0.25×分部2<br>3、训练一个语音新方向的神经网络，使用已有语音识别的数据（音频剪辑、听写记录）和部分新方向的数据作为训练集，但验证集和测试集是新方向的数据</p><h4 id="不同分布数据集的偏差与方差："><a href="#不同分布数据集的偏差与方差：" class="headerlink" title="不同分布数据集的偏差与方差："></a>不同分布数据集的偏差与方差：</h4><p>1、训练集和验证集来自不同分布，会出现训练集误差很低，验证集误差相对较高<br>2、新成立数据集：training-dev set 训练验证集（来自于训练集，但不用于训练）<br>3、四部分数据集：训练集|训练验证集|验证集|测试集<br>​    贝叶斯误差：0%<br>​    训练误差：1%<br>​    训练验证误差：9%<br>​    验证误差：10%<br><strong>结论：方差问题！！！</strong><br>​    贝叶斯误差：0%<br>​    训练误差：1%<br>​    训练验证误差：1.5%<br>​    验证误差：10%<br><strong>结论：数据不匹配！！！</strong><br>​    贝叶斯误差：0%<br>​    训练误差：10%<br>​    训练验证误差：11%<br>​    验证误差：12%<br><strong>结论：偏差问题！！！</strong><br>​    贝叶斯误差：0%<br>​    训练误差：10%<br>​    训练验证误差：11%<br>​    验证误差：20%<br><strong>结论：可避免偏差相当高，数据不匹配！！！</strong><br>4、可避免偏差：贝叶斯误差与训练误差之间<br>​     方差：训练误差与训练验证误差<br>​     数据匹配程度：训练验证误差与验证误差<br>​     拟合程度：验证误差与测试误差<br>5、个别情况：验证误差和测试误差均小于训练误差和训练验证误差，是因为验证集数据更容易处理（两者不是同一分布）</p><h4 id="数据不匹配的优化尝试："><a href="#数据不匹配的优化尝试：" class="headerlink" title="数据不匹配的优化尝试："></a>数据不匹配的优化尝试：</h4><p>1、了解不同分布的数据集具体到差异，做误差分析，为避免过拟合，应该人工查看验证集而不是测试集<br>2、若在验证集中噪声可能更大，识别数字准确度不够，则需要收集更多类似验证集的数据去训练（降低数据不匹配，尽量来自于同一分布），或者去<span style="border-bottom:2px dashed red;">模拟噪声、模拟数字发音（将清晰的语音与噪声合成模拟现场的语音），在训练集做数据增强操作！（缺点：噪声也需要多样，否则容易过拟合）</span><br>3、”The quick brown fox jumps over the lazy dog”常在语音识别中出现，包含了26个字母<br>4、无人汽车中的汽车识别，运用人工合成的图片在人眼看来可能很正常，在只是合成了小情况的图片（无法将所有情况考虑进去），容易过拟合。（在渲染得十分逼真的游戏中，截图汽车的图片，但是游戏中汽车款式远少于现实生活中的汽车款式）</p><h4 id="迁移学习："><a href="#迁移学习：" class="headerlink" title="迁移学习："></a>迁移学习：</h4><p>1、将分类猫的神经网络所学习的知识，去学习关于x光图片识别的神经网络。将类似的神经网络学习称为迁移学习<br>2、神经网络框架基本不变，在最后一层输出需要改变成新的输出层<br>3、若是训练数据不多，则仅训练最后一层或两三层即可（节省时间，少数据重新训练可能会过拟合），其他隐含层就可以不训练，使用旧神经网络的参数<br>4、若是数据足够多，可以训练所有层参数<br>5、<span style="border-bottom:2px dashed red;">旧数据的初期训练阶段称为pre-training预训练，新图片数据训练阶段称为fine tuning微调</span><br>6、为什么旧的神经网络学习的东西能迁移到新的神经网络？因为<span style="border-bottom:2px dashed red;">旧神经网络已经学会低层次特征（边缘检测、曲线检测，阳性对象检测），已经学会了结构信息、图像形状信息，学会的点、线等等会帮助新神经网络的高层次特征学习</span><br>7、迁移学习意义是迁移来源问题（旧神经网络）有很多数据，但迁移目标问题（新神经网络）仅有少量数据（过少不足以拟合神经网络）<br>8、数据增强可用少数的图片数据变得更多，其中可以改变RGB通道的数值，改变颜色，使其失真；<br>9、若是仅对一种颜色进行改变，这叫PAC增强（Principle Components Analysis）</p><h4 id="multi-task-learning多任务学习："><a href="#multi-task-learning多任务学习：" class="headerlink" title="multi-task learning多任务学习："></a>multi-task learning多任务学习：</h4><p>1、迁移学习是类似神经网络的串行，多任务学习是类是类似神经网络的并行<br>2、<span style="border-bottom:2px dashed red;">无人汽车的同时执行多个物体的位置检测属于多任务学习</span>，单个神经网络系统中识别多个物体<br>3、单个神经网络识别多个物体比多个独立的神经网络分别识别物体性能更好<br>4、在训练数据中，一张图没有全部标记所需的物体，问题不大，输出结果是问号（不是0不是1），在求和中不会计算问号<br>5、意义：当识别多个物体能共用低层次特征；每个任务的数据量很接近（若有对称性，其他任务提供低层次特征，相对少量的数据也可以学习）；可以训练一个足够大的网络做好所有任务（不足够大的话，性能就比不上单独训练的神经网络）</p><h4 id="end-to-end-learning端到端学习："><a href="#end-to-end-learning端到端学习：" class="headerlink" title="end-to-end learning端到端学习："></a>end-to-end learning端到端学习：</h4><p>1、speech recognition语音识别：MFCC是用于在音频中提取一组特定的人工设计的特征，提取出低层次特征，组织成单词后串成transcript文本<br>2、端到端学习：只需要吧训练集拿过来直接学习到了x和y之间的函数映射，直接绕过其中很多的步骤，简化整个框架（需要大量数据才可以）（训练一个巨大神经网络，进输入音频即可得出文本，不从低层次特征学起）<br>3、face recognition人脸识别：若是用端到端学习，识别整个人一步到位输出身份信息，其实效果并不好（数据量足够的话，其实效率会更好）；若先识别整体人的脸部，裁剪使人脸居中再进行人脸识别会更好（分两部分解决问题都简单，而且数据量都多）<br>4、machine translation机器翻译：英法互译，在有很多数据能将单词一对一对应起来，端到端学习效率是很好<br>5、estimating child’s age估计孩子年龄：用x射线图估计孩子年龄，判断孩子发育是否正常（或者用于破案）。将照片中分割出每一块骨头，分别识别每块骨头应该属于哪里，查看长度，比对正常发育的长度比例，然后估计出孩子年龄（这种情况用端到端学习，效果就不好，没有足够多数据）<br>6、优点：1）只是让数据说话（足够多的x、y映射）（在传统对语音或图片的分割存在有人类的偏见，分割什么，怎么分，如何定义，都是人类创造的）（音频要分音位，图片要分像素，再用低层次特征组合高层次特征）2）省去很多神经网络中间手工组件的设计<br>7、缺点：1）需要大量x-y映射数据 2)省去中间的手工组件设计，也会排除掉一些有用的手工组件设计（机器学习工程师很鄙视手工设计，所以就无法从很小的训练集数据中获得洞察力）（学习知识来源是数据和手工设计）<br>8、<span style="border-bottom:2px dashed red;">用不用端到端学习，取决于是否有大量映射数据</span><br>9、无人驾驶：（深度学习）图像识别多个物体-（控制算法）规划路线，转盘方向-加速/减速指令</p><h4 id="卷积神经网络："><a href="#卷积神经网络：" class="headerlink" title="卷积神经网络："></a>卷积神经网络：</h4><p>1、image classification图像分类、object detection物体检测、neural style transfer 风格迁移<br>2、edge detection边缘检测，先将低层次特征的线条检测出来，再组合成高层次特征，最后组合成完整图像<br>3、conv kernel卷积核=filter过滤器，卷积convolve：元素相乘再求和<br>4、（垂直过滤器）卷积之后，检测到中间有线（6x6图片小，所以得出结果中间亮处线很粗）（若是由黑到亮的原图，结果线是黑色）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/2.png" alt="image"><br>5、sobel filter 增加了中间一行元素的权重，则处在图像中央的像素点，使结果的robust更高。<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/3.png" alt="image"><br>6、scharr filter也常用<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/4.png" alt="image"><br>7、也可以将filter所有元素设置为参数，用反向传播更新，得到更好filter，对复杂图像更好<br>8、输入图片nxn，过滤器fxf，结果为(n-f+1)x(n-f+1)<br>9、在没有padding情况下，角落的像素与中间的像素相比，中间像素被重复运算的几率更大，意味着丢掉了图像边缘位置信息<br>10、padding的意义：不是每次经过卷积层都需要缩小图片，否则在深层的神经网络中最后图片变得很小；保留边缘信息。<br>11、输入图片nxn，加入p圈的padding变成(n+p)x(n+p)，过滤器fxf，结果为(n+2p-f+1)x(n+2p-f+1)<br>12、padding加入多少圈有两个选择：valid卷积（no padding）、same卷积（输入输出一致，p=(f-1)/2(f一般为奇数，便于指出过滤器位置)）<br>13、输入图片nxn，padding为p，过滤器fxf，步长为s(s&gt;1)，结果为((n+2p-f+1)/s + 1) x ((n+2p-f+1)/s + 1)，不是整数，向下取整<br>14、在数学上，卷积运算前需要将过滤器沿着副对角线做镜像翻转，但在深度学习上没有翻转，仍称为卷积（convolution）；在机器学习上没有翻转，称为互相关（cross-correlation）<br>15、在rgb三通道的图片中，过滤器也需要是深度为3，同样元素相乘，三维的元素之和相加，最后结果是二维<br>16、当同时通过<strong>多种过滤器</strong>之后，所有<span style="border-bottom:2px dashed red;">二维结果堆叠在一起，形成三维</span><br>17、过滤器参数个数，fxfxnc（nc表示通道数，1或3），偏置1个（所有元素加相同一个数），n个过滤器，因此，个数=(fxfxnc+1)xn<br>18、过滤器参数个数不变，无论输入是多大的数据，输出都是固定的，有效避免过拟合<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/5.png" alt="image"><br>19、pooling池化用于缩小模型大小，提高计算速度，提高特征的robust （池化超参数是固定的，不需要梯度下降，计算公式与卷积计算公式一致，信道/深度不变）<br>20、常用最大池化，少用平均池化，也有在很深的神经网络时，会使用平均池化<br>21、常用池化大小为2x2，步长为2，等于原来高度宽度缩减一半；也常用3x3,步长为2的（其他大小就要具体看使用什么池化了，也可以加padding，但一般不加）<br>22、池化属于神经网络中的静态属性，是固定的<br>23、将卷积操作和池化操作一起称为卷积，属于一卷积层<br>24、尽量不要自己设置超级参数，查看文献采用超级参数数值<br>25、卷积层参数少于全连接层参数<br>26、激活值（参数数量）减少太快会影响神经网络性能<br>27、与全连接层相比，卷积层有两大优势：parameter sharing参数共享（在卷积操作时，卷积计算可以在不同图片区域使用不同的参数；一个过滤器适合一个图片某一块，则也会适合另一块）、sparsity connections稀疏连接（输入少），有着两个优势也可以预防过拟合<br>28、卷积神经网络善于捕捉trainslation invariance平移不变，即平移图片后，仍能识别成一个东西<br>29、损失函数等于神经网络对整个训练集的预测的损失总和，用梯度下降去减少误差<br>30、Alexnet比Lenet优势：使用了ReLU（Lenet使用的是sigmoid和tanh）、使用了两个GPU、使用LRN层（局部响应归一化）（后来被证实没啥用）<br>31、vgg16，没有太多参数，仅是一种只需要专注于构建卷积层的简单网络，优点是简化了神经网络结构（卷积核是3x3，步长1，same；过滤器2x2，步长2），卷积没有缩小图像（使用padding），仅在池化缩小图像。虽然参数非常巨大，神经网络很深，但是结构不复杂，比较规整<br>32、很深的神经网络很难训练，因为存在vanish梯度消失和exploding梯度爆炸的问题<br>33、使用1x1x1的过滤器，用在6x6x1图片上，效果不好，但用在6x6x32上，效果就很好（将深度上的元素做智能乘积运算）（被称为Network in Network）（用池化仅压缩高度和宽度，则1x1可以压缩/增加深度）（也可以不压缩深度，这就仅仅添加了非线性函数）<br>34、数据量多，则使用更少手工；数据量少（想低调，怕黑客攻击），则使用更多手工（可以获得更好表现 ），使用迁移学习更好<br>35、拥有大量数据，应花更多时间在设计神经网络框架中，手动工程则是十分困难；对于计算机视觉图片识别，计算机视觉文学依赖于大量手工工程<br>36、在基准做得好得技巧（在比赛用的多，在生产较少使用）：<br>​        Ensembling集成（独立训练几个神经网络（3-15个网络，耗时），再平均化输出，会再基准提升个1%、2%）<br>​        Multi-crop at test time（在图片分类中，从同一图片的多种不同视角进行识别（10-crop），平均输出）</p><h4 id="ResNet-残差网络（Residual-Network）："><a href="#ResNet-残差网络（Residual-Network）：" class="headerlink" title="ResNet 残差网络（Residual Network）："></a>ResNet 残差网络（Residual Network）：</h4><p>1、skip connection跳远链接，可从某一网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层（有助于解决梯度消失和梯度爆炸问题）<br>2、Residual block残差块，ResNet是由残差块构建的<br>3、plain network 普通的神经网络，随着深度的增加，误差先是降低，然后升高；而ResNet深度增加，误差持续越少（几百层都可以）<br>4、<span style="border-bottom:2px dashed red;">在原有普通神经网络中，中间或末尾加入残缺块，性能不差于普通神经网络，更多时候能传递更多信息，性能更好（a[l+2]=g(z[l+2]+a[l])）</span><br>5、残缺块输入a[l]、输出a[l+2]的维度相同，若不同维度，则多加一个矩阵在a[l]前（a[l+2]=g(z[l+2]+w*a[l])）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/6.png" alt="image"></p><h4 id="Google-Inception-Net："><a href="#Google-Inception-Net：" class="headerlink" title="Google Inception Net："></a>Google Inception Net：</h4><p>1、Inception层的作用是代替人工来确定卷积层中的过滤器类型or是否要创建卷积层或者池化层（为了不让人做选择，则全部情况考虑进去）（缺点就是计算成本大）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/7.png" alt="image"><br>2、直接用5x5卷积运算，运算成本太大，每个元素计算1.2亿遍；<span style="border-bottom:2px dashed red;">在5x5卷积运算中间，加入一个1x1，将卷积运算分成两部卷积（先缩小再扩大）则可以降低计算成本，减少到原里1/10，1240万，1x1层被称为bottleneck layer瓶颈层</span><br>3、大幅缩小表示层规模，只要合理构建瓶颈层，就不会降低网络性能<br>4、单个inception模块<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/8.png" alt="image"><br>5、整体Inception Net<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/9.png" alt="image"><br>6、每个inception模块设计功能可以不一样，还可再Inception层分支出来做FC和softmax</p><h4 id="Object-Detection-对象检测："><a href="#Object-Detection-对象检测：" class="headerlink" title="Object Detection 对象检测："></a>Object Detection 对象检测：</h4><p>1、Image classification 图像分类，Classification with localization 分类定位，用图像分类的思路可以帮助学习分类定位，分类定位的思路又有助于学习对象检测<br>2、无人驾驶中需要检测的对象：pedestrian行人，car汽车、motorcycle摩托车、background背景<br>3、用bounding box边界框定位出目标，需要在神经网络多输出几个单元，输出边界框（就是让神经网络多数出四个数字bx、by、bh、bw用于定位）（<strong>图片左上角为(0,0)，右下角为(1,1)</strong>）<br>4、要想确定边界框的具体位置，需要指定红色方框的中心点(bx,by)，高度为bh，宽度为bw<br>5、<span style="border-bottom:2px dashed red;">在图片标签中，除了分类标签，还需要标签表示边框的四个数字（学习分类时，同时学习了边框）</span><br>6、一张图片的标签，pc表示分两类，pc=1时表示行人、汽车、摩托车（有对象类），pc=0时表示背景（没有对象），则y=[pc bx by  bh bw c1 c2 c3]（c1表示行人，c2表示汽车，c3表示摩托车）（暂时假定一张图至多有一个对象）（若一张图有汽车，则y=[1 bx by bh bw 0 1 0]，若一张背景图，则y=[0 ? ? ? ? ? ? ?]，?表示无意义的参数）（<strong>标签训练数据最终决定了训练结果</strong>）<br>7、损失值则等于一维矩阵中每个元素相应差值的平方和；若pc=1，损失值就是剩下元素的差值平方和；若pc=0，损失值是第一个元素差值平方和（后面7个元素无意义）<br>8、在实际运用上，<span style="border-bottom:2px dashed red;">pc应用logistic regression loss逻辑回归函数（squared error or predict平方预测误差也可以），边界框坐标应用平方误差（或其他类似方法），可以不对c1 c2 c3和softmax激活函数应用对数损失函数</span><br>9、<strong>Landmark detection 特征点检测</strong>，定义好多少个特征点，边界框四个，人脸64个，人体结构32个，均需要手工标注<br>10、对象检测采用的是基于<strong>sliding windows滑动窗口</strong>的目标检测算法（训练数据集，需要将汽车图片截图并标签）（先设定一个边界框，在图片开始滑动截图，以固定步幅滑动窗口，遍历图像的每个区域，每次都将截图喂入卷积神经网络，判断是否有对象）（计算成本高）（需要极小步幅滑动，才能准确定位图片中的对象）<br>11、在卷积神经网络上应用滑动窗口：首先将神经网络<span style="border-bottom:2px dashed red;">全连接层转化为卷积层</span><br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/10.png" alt="image"><br>设定边界框为14x14，在16x16图中进行滑动窗口卷积，得出4种结果，一次性输出在最终结果（而不是截取成4张图片再独立威入神经网络），在28x28图中滑动窗口卷积可以直接得出64种的所有结果（大大节省了计算成本，但仍然无法确定精准的边界）（与Fast R-CNN类似）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/11.png" alt="image"><br>12、<strong>YOLO(you only look once)算法</strong>：在100x100图片中分成3x3（实际操作可能会是19x19），<span style="border-bottom:2px dashed red;">九宫格中每个格子都要指定标签y=[pc bx by  bh bw c1 c2 c3]</span>，发现哪个格子有对象，则取对象的中点，然后将这个对象分配给包含对象中点的格子<span style="border-bottom:2px dashed red;">（如果有两个格子包含同一个对象，对象仅仅属于对象中点在的格子，对象仅仅属于一个格子，不能同时属于一个格子，对象中点是标签的(bx,by)）</span>，仅需要一次卷积就可输出3x3x8，九种情况，8个属性值。优点在于能输出精确的边界框，前提是每个格子不超过1个对象（将格子再细分）；速度快，可以达到实时识别<br>13、每个格子左上角都是(0,0)右下角都是(1,1)，bx、by的范围值在1之内，bh、bw可能会超过1<br>14、<strong>IoU(Intersection over union)交并比函数</strong>：用于判断对象检测算法运作是否良好。原理：计算预测的边界框和标签的边界框的交集和并集之比，最好情况是比值为1（范围是[0,1]）（约定只要IoU&gt;=0.5都认为正确（不过是人为定的，可以定0.6 0.7））<br>15、<strong>non-max supperession 非最大值抑制</strong>：在对象检测中，针对同一个对象都会做出多次检测，会得到多个预测边界框，非最大值抑制就是处理这些多个预测边界框（卷积神经网络后的结果，给出预测边界框的概率，选中最大概率的那一个，将其他的边界框与其做交并比，高度重叠、交并比很高的其他边界框全部被抑制显示）<br>16、对象检测整体流程：（yolo）将原图分成19x19个格子，通过卷积神经网络输出每个格子的情况，舍弃pc&lt;=0.6的格子，（非最大值抑制）剩下的边界框，仅保留最大pc值得边界框（若是检测对个对象，则独立进行多次非最大值抑制）<br>17、<strong>Anchor box</strong>：在一个格子中可以检测多个对象；定义多个不同形状anchor box，预测结果和这些anchor box关联起来（有时会达到5个anchor box甚至更多），对象会被分配有其中点的（格子，anchor box），原本卷积神经网络输出结果y只代表一个边界框，<span style="border-bottom:2px dashed red;">改为y中增加多个结果</span>（同时标签工作也需要将正确结果y标注清楚），<span style="border-bottom:2px dashed red;">anchor box最后就是非最大抑制后的边界框（缺点：要是仅定义两个anchor box，出现三个对象就不行了；出现两个一个类型的对象，也不行了</span>（但是用了19x19较细格子就很少出现该情况，两个对象中点出现在同一个格子概率很低））（手工选择anchor box必须考虑周全形状才可以）（通过机器学习的方法k-平均算法，可以自动选择一组anchor box，可以适合十几种对象）<br>18、当使用两个anchor box时，每个格子输出都会是有两个边界框，除去pc较低的边界框，在独立运行两次非最大值抑制即可得出最终的边界框<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/12.png" alt="image"><br>19、<strong>Region proposals（候选区域）R-CNN（带区域的卷积神经网络）</strong>：在滑动窗口卷积时，进选择一部分有意义的窗口进行卷积操作，减少卷积时间（但还是很慢）（候选区的方法时运行Segmentation algorithm 图像分割算法）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/13.png" alt="image"><br>20、<strong>Fast R-CNN</strong>：将全连接层转化为卷积层，一次卷积就可以得到所有滑动窗口的结果；<strong>Faster R-CNN</strong>：用卷积的方法选择候选区（比Fast R-CNN快，但没有YOLO快）</p><h4 id="Face-recognition-人脸识别："><a href="#Face-recognition-人脸识别：" class="headerlink" title="Face recognition 人脸识别："></a>Face recognition 人脸识别：</h4><p>1、<strong>Face Verification 人类验证</strong>：输入名字和相片，判断是否本人（1对1问题），而人脸识别是识别问题（1对多问题）<br>2、<strong>One-Shot 一步学习</strong>：只通过一张人脸图片，就能识别这个人；设计Similarity函数：直接对比两张图片，输出差异值（若是同一个人差异值很小，若不是同一个人，则差异值很大）<br>3、<strong>Siamese Network</strong>：在神经网络实现Similarity方法，训练一个Siamese网络，将图片喂入，得出结果是一个128维向量或者编码，用于代表这张图片，然后再d函数中实现对比两张图片向量或者编码的范数<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/14.png" alt="image"><br>4、<strong>Triple loss三元组损失</strong>：通过神经网络得到较优的人脸图片编码，其中一个方法是设计三元组损失函数，然后应用梯度下降（<span style="border-bottom:2px dashed red;">将原图定义为anchor图片，将是同一个人的图片称为positive，不是同一个人称为negative，同时对比三张图片，这就是三元(A,P,N)</span>）希望得到的结果是d(A,P)&lt;=d(A,N)，即是<strong>||f(A)-f(P)||2&lt;=||f(A)-f(N)||^2</strong>为了防止全部f(xi)输出0值，设置为<strong>||f(A)-f(P)||2-||f(A)-f(N)||2+α&lt;=0</strong>（α称为margin间隔）<br>5、基于三张图片来定义损失函数<strong>L(A,P,N) = Max(||f(A)-f(P)||2-||f(A)-f(N)||2+α, 0)</strong>（loss希望输出0，所以运用max函数）代价函数J  = ΣL(Ai, Pi, Ni)（需要将训练集分为三元组进行训练）（若是随机选择图片组合成三元组，很容易就满足<strong>d(A,P)+α&lt;=d(A,N)</strong>这个条件，尽量选择<strong>d(A,P)≈d(A,N)</strong>这种三元组训练才会让神经网络使d(A,P)更小、d(A,N)更大，否则梯度效果不好，识别也不好，神经网络就没学习到什么）<br>6、可以将人类识别转换为一个二分类问题，是同一个人输出1，不是则输出0（<strong>y = sigmoid(Σ wi*|f(xi)k-f(xj)k|+b)</strong>，用于预测1和0）</p><h4 id="Style-Transfer-风格迁移："><a href="#Style-Transfer-风格迁移：" class="headerlink" title="Style Transfer 风格迁移："></a>Style Transfer 风格迁移：</h4><p>1、神经网络学习已有大师的作品风格（Style）（S），将风格赋予给其他图片（Content）（C），形成同风格的新作品（Generated image）（G）<br>2、图像识别的神经网络：第一层都会去寻找低层次的图片特征（线条、颜色、阴影等）第二层识别的图片的质地，深层则会识别负责的图像<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/15.png" alt="image"><br>3、实现风格迁移，关键设计关于G的代价函数J(G)，衡量生成图片的好坏；分成两部分，第一部分是content cost内容代价J(C,G)（衡量原图C与生成图片G的相似程度），第二部分是style cost风格代价J(S,G)（衡量原风格S与生成图片G风格相似程度）<strong>J(G) = α Jcontent(C,G)+β Jstyle(S,G)</strong>（需要梯度下降不断更新G）<br><img src="https://github.com/soloistben/images/raw/master/deeplearning_image/16.png" alt="image"><br>4、<strong>Jcontent(C,G)[l] </strong>，用隐含层去计算内容代价，但不可以选择太浅网络（太浅的话生成图片仅是像素十分接近原图，深的神经网络会问是否有一只狗，然后生产图片肯定有只狗）（因此不能选太浅和太深）；用一个预训练的神经网络（可以是VGG）；分别给C、G设置激活数，a[i](C)和a[i](G)（若两激活数相似，则图片相似）<strong>Jcontent(C,G) = 1/2 ||a[i](C)-a[i](G)||^2</strong><br>5、<strong>Jstyle(S,G)</strong>，选择一层为图片风格定义一个深度测量，将图片风格定义为该层中各个通道之间激活项的相关系数（如何计算每个通道间的相关系数呢？）（<span style="border-bottom:2px dashed red;">若一通道识别纹理，一通道识别橙色，若是两通道相关关系大，则证明在出现纹理的地方颜色有很大概率是橙色</span>）（相关系数定义则是，两通道特征同时/不同时出现的概率）（对比原图S和生成图G的通道间相关系数之间的差距，就能判断风格是否相似）<br>6、设置激活项a[l](i,j,k)（i表示高度，j表示宽度，k表示通道数），定义一个Style Matrix（或者称为gram matrix）风格矩阵G[l](S)（大小为nc x nc，nc是该层通道数）（将k和k’通道相对应的激活项相乘求和，非标准的互相关函数（因为没有减去平均数）），<strong>G[l](S)kk’ = ΣiΣj a[l](i,j,k)a[l](i,j,k’)</strong>（俩激活项若是相关程度大，G值也大），原图S和生成图G都计算风格矩阵G[l](S)、G[l](G)，<strong>Jstyle(S,G)[l] = 1/(2*nh*nw*nc)2||G[l](S)kk’ - G[l](G)kk’||^2 = 1/(2*nh*nw*nc)^2ΣkΣk’(G[l](S)kk’ - G[l](G)kk’)^2</strong>，则<strong>Jstyle(S,G) = Σlλ[l]Jstyle(S,G)[l]</strong>（将每一层的相关系数求和）</p><h4 id="一维与三维图片的卷积"><a href="#一维与三维图片的卷积" class="headerlink" title="一维与三维图片的卷积:"></a>一维与三维图片的卷积:</h4><p>1、心电图或者信号图这种属于一维图片，ct片、x光片的3d立体扫描属于三维图片（电影也可属于三维，长、宽、时间轴，可以应用于检测动作和人物行为）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;激活函数：&quot;&gt;&lt;a href=&quot;#激活函数：&quot; class=&quot;headerlink&quot; title=&quot;激活函数：&quot;&gt;&lt;/a&gt;激活函数：&lt;/h4&gt;&lt;p&gt;1、sigmoid常用于二元分类在输出时做激活函数，其他地方tanh更优于sigmoid&lt;br&gt;2、tanh和sig
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>pandas_property</title>
    <link href="http://yoursite.com/2019/01/06/pandas-property/"/>
    <id>http://yoursite.com/2019/01/06/pandas-property/</id>
    <published>2019-01-06T13:45:48.000Z</published>
    <updated>2019-07-19T02:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="pandas属性"><a href="#pandas属性" class="headerlink" title="pandas属性"></a>pandas属性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment">#自带下标</span></span><br><span class="line">s = pd.Series([<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,np.nan,<span class="number">44</span>,<span class="number">1</span>])</span><br><span class="line">print(s)</span><br><span class="line"><span class="comment">#时间数组</span></span><br><span class="line">dates = pd.date_range(<span class="string">'20180106'</span>,periods=<span class="number">6</span>)</span><br><span class="line">print(dates)</span><br><span class="line"><span class="comment">#index是行索引，columns是列索引</span></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">4</span>),index=dates,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">print(df) </span><br><span class="line"><span class="comment">#默认索引</span></span><br><span class="line">df1 = pd.DataFrame(np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>)))</span><br><span class="line">print(df1)</span><br><span class="line"><span class="comment">#字典dict</span></span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'A'</span>:<span class="number">1.</span>,</span><br><span class="line"><span class="string">'B'</span>:pd.Timestamp(<span class="string">'20180105'</span>),</span><br><span class="line"><span class="string">'C'</span>:np.array([<span class="number">3</span>]*<span class="number">4</span>,dtype=<span class="string">'int32'</span>),</span><br><span class="line"><span class="string">'D'</span>:pd.Series(<span class="number">1</span>,index=list(range(<span class="number">4</span>)),dtype=<span class="string">"float32"</span>),</span><br><span class="line"><span class="string">'E'</span>:pd.Categorical([<span class="string">"test"</span>,<span class="string">"train"</span>,<span class="string">"solo"</span>,<span class="string">"lol"</span>]),</span><br><span class="line"><span class="string">"F"</span>:<span class="string">'cpp'</span>&#125;)</span><br><span class="line">print(df2)</span><br><span class="line">print(df2.dtypes)<span class="comment">#查看每列属性</span></span><br><span class="line">print(df2.index)<span class="comment">#查看行索引</span></span><br><span class="line">print(df2.columns)<span class="comment">#查看列索引</span></span><br><span class="line">print(df2.values)<span class="comment">#查看所有值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#描述(仅显示数字部分，数值、平均值、等等)</span></span><br><span class="line">print(df2.describe())</span><br><span class="line"><span class="comment">#转置</span></span><br><span class="line">print(df2.T)</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">print(df2.sort_index(axis=<span class="number">1</span>,ascending=<span class="keyword">False</span>))<span class="comment">#列倒序排序</span></span><br><span class="line">print(df2.sort_index(axis=<span class="number">0</span>,ascending=<span class="keyword">False</span>))<span class="comment">#行倒序排序</span></span><br><span class="line"></span><br><span class="line">print(df2.sort_values(by=<span class="string">'E'</span>))<span class="comment">#值排序，根据'E'列排序(不写ascending的值，默认是true)</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">0     1.0</span><br><span class="line">1     3.0</span><br><span class="line">2     6.0</span><br><span class="line">3     NaN</span><br><span class="line">4    44.0</span><br><span class="line">5     1.0</span><br><span class="line">dtype: float64</span><br><span class="line">DatetimeIndex([<span class="string">'2018-01-06'</span>, <span class="string">'2018-01-07'</span>, <span class="string">'2018-01-08'</span>, <span class="string">'2018-01-09'</span>,</span><br><span class="line">               <span class="string">'2018-01-10'</span>, <span class="string">'2018-01-11'</span>],</span><br><span class="line">              dtype=<span class="string">'datetime64[ns]'</span>, freq=<span class="string">'D'</span>)</span><br><span class="line">                   a         b         c         d</span><br><span class="line">2018-01-06 -1.193238  1.729652  0.213075 -0.333416</span><br><span class="line">2018-01-07 -0.775480 -0.960333  0.349506  0.246139</span><br><span class="line">2018-01-08 -0.315293 -1.656652  0.504151  0.892104</span><br><span class="line">2018-01-09  1.111614 -0.210477  0.530345 -0.056502</span><br><span class="line">2018-01-10 -0.563181  0.870935  1.053737 -0.208736</span><br><span class="line">2018-01-11 -0.078093  0.722973 -0.939166  0.253624</span><br><span class="line"></span><br><span class="line">   0  1   2   3</span><br><span class="line">0  0  1   2   3</span><br><span class="line">1  4  5   6   7</span><br><span class="line">2  8  9  10  11</span><br><span class="line"></span><br><span class="line">     A          B  C    D      E    F</span><br><span class="line">0  1.0 2018-01-05  3  1.0   <span class="built_in">test</span>  cpp</span><br><span class="line">1  1.0 2018-01-05  3  1.0  train  cpp</span><br><span class="line">2  1.0 2018-01-05  3  1.0   solo  cpp</span><br><span class="line">3  1.0 2018-01-05  3  1.0    lol  cpp</span><br><span class="line"></span><br><span class="line">A           float64</span><br><span class="line">B    datetime64[ns]</span><br><span class="line">C             int32</span><br><span class="line">D           float32</span><br><span class="line">E          category</span><br><span class="line">F            object</span><br><span class="line"></span><br><span class="line">dtype: object</span><br><span class="line">Int64Index([0, 1, 2, 3], dtype=<span class="string">'int64'</span>)</span><br><span class="line">Index([<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>], dtype=<span class="string">'object'</span>)</span><br><span class="line"></span><br><span class="line">[[1.0 Timestamp(<span class="string">'2018-01-05 00:00:00'</span>) 3 1.0 <span class="string">'test'</span> <span class="string">'cpp'</span>]</span><br><span class="line"> [1.0 Timestamp(<span class="string">'2018-01-05 00:00:00'</span>) 3 1.0 <span class="string">'train'</span> <span class="string">'cpp'</span>]</span><br><span class="line"> [1.0 Timestamp(<span class="string">'2018-01-05 00:00:00'</span>) 3 1.0 <span class="string">'solo'</span> <span class="string">'cpp'</span>]</span><br><span class="line"> [1.0 Timestamp(<span class="string">'2018-01-05 00:00:00'</span>) 3 1.0 <span class="string">'lol'</span> <span class="string">'cpp'</span>]]</span><br><span class="line">         A    C    D</span><br><span class="line">count  4.0  4.0  4.0</span><br><span class="line">mean   1.0  3.0  1.0</span><br><span class="line">std    0.0  0.0  0.0</span><br><span class="line">min    1.0  3.0  1.0</span><br><span class="line">25%    1.0  3.0  1.0</span><br><span class="line">50%    1.0  3.0  1.0</span><br><span class="line">75%    1.0  3.0  1.0</span><br><span class="line">max    1.0  3.0  1.0</span><br><span class="line"></span><br><span class="line">                     0         ...                             3</span><br><span class="line">A                    1         ...                             1</span><br><span class="line">B  2018-01-05 00:00:00         ...           2018-01-05 00:00:00</span><br><span class="line">C                    3         ...                             3</span><br><span class="line">D                    1         ...                             1</span><br><span class="line">E                 <span class="built_in">test</span>         ...                           lol</span><br><span class="line">F                  cpp         ...                           cpp</span><br><span class="line">[6 rows x 4 columns]</span><br><span class="line"></span><br><span class="line">     F      E    D  C          B    A</span><br><span class="line">0  cpp   <span class="built_in">test</span>  1.0  3 2018-01-05  1.0</span><br><span class="line">1  cpp  train  1.0  3 2018-01-05  1.0</span><br><span class="line">2  cpp   solo  1.0  3 2018-01-05  1.0</span><br><span class="line">3  cpp    lol  1.0  3 2018-01-05  1.0</span><br><span class="line">     A          B  C    D      E    F</span><br><span class="line">3  1.0 2018-01-05  3  1.0    lol  cpp</span><br><span class="line">2  1.0 2018-01-05  3  1.0   solo  cpp</span><br><span class="line">1  1.0 2018-01-05  3  1.0  train  cpp</span><br><span class="line">0  1.0 2018-01-05  3  1.0   <span class="built_in">test</span>  cpp</span><br><span class="line">     A          B  C    D      E    F</span><br><span class="line">3  1.0 2018-01-05  3  1.0    lol  cpp</span><br><span class="line">2  1.0 2018-01-05  3  1.0   solo  cpp</span><br><span class="line">0  1.0 2018-01-05  3  1.0   <span class="built_in">test</span>  cpp</span><br><span class="line">1  1.0 2018-01-05  3  1.0  train  cpp</span><br></pre></td></tr></table></figure><hr><h4 id="选择数据"><a href="#选择数据" class="headerlink" title="选择数据"></a>选择数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20180106'</span>,periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates,columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line">print(df)</span><br><span class="line">print(df[<span class="string">'A'</span>])</span><br><span class="line">print(df.A)<span class="comment">#显示第A列</span></span><br><span class="line">print(df[<span class="number">0</span>:<span class="number">2</span>])<span class="comment">#前两行</span></span><br><span class="line">print(df[<span class="string">'20180107'</span>:<span class="string">'20180109'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#select by label:loc</span></span><br><span class="line">print(df.loc[<span class="string">'20180108'</span>])</span><br><span class="line">print(df.loc[:,[<span class="string">'A'</span>,<span class="string">'B'</span>]])<span class="comment">#A,B列的所有行</span></span><br><span class="line">print(df.loc[<span class="string">'20180108'</span>,[<span class="string">'A'</span>,<span class="string">'B'</span>]])<span class="comment">#A,B列的20180108行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#select by position:iloc</span></span><br><span class="line">print(df.iloc[<span class="number">3</span>])<span class="comment">#第4行数据</span></span><br><span class="line">print(df.iloc[<span class="number">3</span>,<span class="number">1</span>])<span class="comment">#第4行第2个元素</span></span><br><span class="line">print(df.iloc[<span class="number">3</span>:<span class="number">5</span>,<span class="number">2</span>:<span class="number">4</span>])<span class="comment">#第4行-第5行的第3列和第4列数据</span></span><br><span class="line">print(df.iloc[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],<span class="number">2</span>:<span class="number">4</span>])<span class="comment">#第2行--第4行-第6行的第3列和第5列数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#mixed selection:in 混合标签和索引筛选</span></span><br><span class="line">print(df.ix[:<span class="number">3</span>,[<span class="string">'A'</span>,<span class="string">'C'</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#boolean indexing</span></span><br><span class="line">print(df[df.A&gt;<span class="number">8</span>])<span class="comment">#A列大于8的所有行</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">             A   B   C   D</span><br><span class="line">2018-01-06   0   1   2   3</span><br><span class="line">2018-01-07   4   5   6   7</span><br><span class="line">2018-01-08   8   9  10  11</span><br><span class="line">2018-01-09  12  13  14  15</span><br><span class="line">2018-01-10  16  17  18  19</span><br><span class="line">2018-01-11  20  21  22  23</span><br><span class="line">2018-01-06     0</span><br><span class="line">2018-01-07     4</span><br><span class="line">2018-01-08     8</span><br><span class="line">2018-01-09    12</span><br><span class="line">2018-01-10    16</span><br><span class="line">2018-01-11    20</span><br><span class="line">Freq: D, Name: A, dtype: int32</span><br><span class="line">2018-01-06     0</span><br><span class="line">2018-01-07     4</span><br><span class="line">2018-01-08     8</span><br><span class="line">2018-01-09    12</span><br><span class="line">2018-01-10    16</span><br><span class="line">2018-01-11    20</span><br><span class="line">Freq: D, Name: A, dtype: int32</span><br><span class="line">            A  B  C  D</span><br><span class="line">2018-01-06  0  1  2  3</span><br><span class="line">2018-01-07  4  5  6  7</span><br><span class="line">             A   B   C   D</span><br><span class="line">2018-01-07   4   5   6   7</span><br><span class="line">2018-01-08   8   9  10  11</span><br><span class="line">2018-01-09  12  13  14  15</span><br><span class="line">A     8</span><br><span class="line">B     9</span><br><span class="line">C    10</span><br><span class="line">D    11</span><br><span class="line">Name: 2018-01-08 00:00:00, dtype: int32</span><br><span class="line">             A   B</span><br><span class="line">2018-01-06   0   1</span><br><span class="line">2018-01-07   4   5</span><br><span class="line">2018-01-08   8   9</span><br><span class="line">2018-01-09  12  13</span><br><span class="line">2018-01-10  16  17</span><br><span class="line">2018-01-11  20  21</span><br><span class="line">A    8</span><br><span class="line">B    9</span><br><span class="line">Name: 2018-01-08 00:00:00, dtype: int32</span><br><span class="line">A    12</span><br><span class="line">B    13</span><br><span class="line">C    14</span><br><span class="line">D    15</span><br><span class="line">Name: 2018-01-09 00:00:00, dtype: int32</span><br><span class="line">13</span><br><span class="line">             C   D</span><br><span class="line">2018-01-09  14  15</span><br><span class="line">2018-01-10  18  19</span><br><span class="line">             C   D</span><br><span class="line">2018-01-07   6   7</span><br><span class="line">2018-01-09  14  15</span><br><span class="line">2018-01-11  22  23</span><br><span class="line">            A   C</span><br><span class="line">2018-01-06  0   2</span><br><span class="line">2018-01-07  4   6</span><br><span class="line">2018-01-08  8  10</span><br><span class="line">             A   B   C   D</span><br><span class="line">2018-01-09  12  13  14  15</span><br><span class="line">2018-01-10  16  17  18  19</span><br><span class="line">2018-01-11  20  21  22  23</span><br></pre></td></tr></table></figure><hr><h4 id="插入操作"><a href="#插入操作" class="headerlink" title="插入操作"></a>插入操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates,columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line">df.iloc[<span class="number">2</span>,<span class="number">2</span>] = <span class="number">111</span></span><br><span class="line">df.loc[<span class="string">'20130101'</span>,<span class="string">'B'</span>] = <span class="number">222</span></span><br><span class="line">print(df.A&gt;<span class="number">12</span>)</span><br><span class="line">df[df.A&gt;<span class="number">12</span>] = <span class="number">0</span><span class="comment">#df.A大于12的每一行全置0</span></span><br><span class="line">print(df)</span><br><span class="line">df.A[df.A&lt;<span class="number">12</span>] = <span class="number">1</span><span class="comment">#df.A大于12的每一行的A列全置1</span></span><br><span class="line">print(df)</span><br><span class="line">df[<span class="string">'F'</span>] = np.nan<span class="comment">#添加多1列</span></span><br><span class="line">print(df)</span><br><span class="line">df[<span class="string">'E'</span>] = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],index=pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>))</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">             A   B   C   D</span><br><span class="line">2013-01-01   0   1   2   3</span><br><span class="line">2013-01-02   4   5   6   7</span><br><span class="line">2013-01-03   8   9  10  11</span><br><span class="line">2013-01-04  12  13  14  15</span><br><span class="line">2013-01-05  16  17  18  19</span><br><span class="line">2013-01-06  20  21  22  23</span><br><span class="line"></span><br><span class="line">2013-01-01    False</span><br><span class="line">2013-01-02    False</span><br><span class="line">2013-01-03    False</span><br><span class="line">2013-01-04    False</span><br><span class="line">2013-01-05     True</span><br><span class="line">2013-01-06     True</span><br><span class="line">Freq: D, Name: A, dtype: bool</span><br><span class="line"></span><br><span class="line">             A    B    C   D</span><br><span class="line">2013-01-01   0  222    2   3</span><br><span class="line">2013-01-02   4    5    6   7</span><br><span class="line">2013-01-03   8    9  111  11</span><br><span class="line">2013-01-04  12   13   14  15</span><br><span class="line">2013-01-05   0    0    0   0</span><br><span class="line">2013-01-06   0    0    0   0</span><br><span class="line"></span><br><span class="line">             A    B    C   D</span><br><span class="line">2013-01-01   1  222    2   3</span><br><span class="line">2013-01-02   1    5    6   7</span><br><span class="line">2013-01-03   1    9  111  11</span><br><span class="line">2013-01-04  12   13   14  15</span><br><span class="line">2013-01-05   1    0    0   0</span><br><span class="line">2013-01-06   1    0    0   0</span><br><span class="line"></span><br><span class="line">             A    B    C   D   F</span><br><span class="line">2013-01-01   1  222    2   3 NaN</span><br><span class="line">2013-01-02   1    5    6   7 NaN</span><br><span class="line">2013-01-03   1    9  111  11 NaN</span><br><span class="line">2013-01-04  12   13   14  15 NaN</span><br><span class="line">2013-01-05   1    0    0   0 NaN</span><br><span class="line">2013-01-06   1    0    0   0 NaN</span><br><span class="line"></span><br><span class="line">             A    B    C   D   F  E</span><br><span class="line">2013-01-01   1  222    2   3 NaN  1</span><br><span class="line">2013-01-02   1    5    6   7 NaN  2</span><br><span class="line">2013-01-03   1    9  111  11 NaN  3</span><br><span class="line">2013-01-04  12   13   14  15 NaN  4</span><br><span class="line">2013-01-05   1    0    0   0 NaN  5</span><br><span class="line">2013-01-06   1    0    0   0 NaN  6</span><br></pre></td></tr></table></figure><hr><h4 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment">#处理丢失数据</span></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>)</span><br><span class="line">df = pd.DataFrame(np.arange(<span class="number">24</span>).reshape((<span class="number">6</span>,<span class="number">4</span>)),index=dates,columns=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])</span><br><span class="line"></span><br><span class="line">df.iloc[<span class="number">0</span>,<span class="number">1</span>] = np.nan</span><br><span class="line">df.iloc[<span class="number">1</span>,<span class="number">2</span>] = np.nan</span><br><span class="line">print(df)</span><br><span class="line"><span class="comment">#how&#123;'any','all'&#125;,any表示只要出现NaN数据就丢弃,all表示全部是NaN才丢弃</span></span><br><span class="line"><span class="comment">#axis 0表示行，1表示列</span></span><br><span class="line">print(df.dropna(axis=<span class="number">1</span>,how=<span class="string">'any'</span>))</span><br><span class="line"><span class="comment">#填补NaN</span></span><br><span class="line">print(df.fillna(value=<span class="number">0</span>))<span class="comment">#填补完后还是缺失状态</span></span><br><span class="line"><span class="comment">#是否有NaN</span></span><br><span class="line">print(df.isnull())<span class="comment">#返回矩阵</span></span><br><span class="line">print(np.any(df.isnull())==<span class="keyword">True</span>)<span class="comment">#返回是否有缺失</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">             A     B     C   D</span><br><span class="line">2013-01-01   0   NaN   2.0   3</span><br><span class="line">2013-01-02   4   5.0   NaN   7</span><br><span class="line">2013-01-03   8   9.0  10.0  11</span><br><span class="line">2013-01-04  12  13.0  14.0  15</span><br><span class="line">2013-01-05  16  17.0  18.0  19</span><br><span class="line">2013-01-06  20  21.0  22.0  23</span><br><span class="line">             A   D</span><br><span class="line">2013-01-01   0   3</span><br><span class="line">2013-01-02   4   7</span><br><span class="line">2013-01-03   8  11</span><br><span class="line">2013-01-04  12  15</span><br><span class="line">2013-01-05  16  19</span><br><span class="line">2013-01-06  20  23</span><br><span class="line">             A     B     C   D</span><br><span class="line">2013-01-01   0   0.0   2.0   3</span><br><span class="line">2013-01-02   4   5.0   0.0   7</span><br><span class="line">2013-01-03   8   9.0  10.0  11</span><br><span class="line">2013-01-04  12  13.0  14.0  15</span><br><span class="line">2013-01-05  16  17.0  18.0  19</span><br><span class="line">2013-01-06  20  21.0  22.0  23</span><br><span class="line">                A      B      C      D</span><br><span class="line">2013-01-01  False   True  False  False</span><br><span class="line">2013-01-02  False  False   True  False</span><br><span class="line">2013-01-03  False  False  False  False</span><br><span class="line">2013-01-04  False  False  False  False</span><br><span class="line">2013-01-05  False  False  False  False</span><br><span class="line">2013-01-06  False  False  False  False</span><br><span class="line"></span><br><span class="line">True</span><br></pre></td></tr></table></figure><hr><h4 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment">#读取存取 数据</span></span><br><span class="line"><span class="comment">#read_csv(excel最基础格式,也可以读取txt，推荐)#to_csv</span></span><br><span class="line"><span class="comment">#read_excel#to_excel</span></span><br><span class="line"><span class="comment">#read_hdf#to_hdf</span></span><br><span class="line"><span class="comment">#read_sql#to_sql</span></span><br><span class="line"><span class="comment">#read_json#to_json</span></span><br><span class="line"><span class="comment">#read_msgpack#to_msgpack</span></span><br><span class="line"><span class="comment">#read_html#to_html</span></span><br><span class="line"><span class="comment">#read_gdp#to_gdp</span></span><br><span class="line"><span class="comment">#read_stata#to_stata</span></span><br><span class="line"><span class="comment">#read_sas#to_sas</span></span><br><span class="line"><span class="comment">#read_clipboard#to_clipboard</span></span><br><span class="line"><span class="comment">#read_pickle(python)#to_pickle(python)</span></span><br><span class="line">data = pd.read_csv(<span class="string">'stu.csv'</span>)</span><br><span class="line">print(data)</span><br><span class="line">data.to_pickle(<span class="string">'stu.pickle'</span>)</span><br><span class="line">data.to_json(<span class="string">'stu.json'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    Student ID   name  age  gender</span><br><span class="line">0         1100  Kelly   22  Female</span><br><span class="line">1         1101    Clo   21  Female</span><br><span class="line">2         1102  Tilly   22  Female</span><br><span class="line">3         1103   Tony   24    Male</span><br><span class="line">4         1104  David   20    Male</span><br><span class="line">5         1105  Catty   22  Female</span><br><span class="line">6         1106      M    3  Female</span><br><span class="line">7         1107      N   43    Male</span><br><span class="line">8         1108      A   13    Male</span><br><span class="line">9         1109      S   12    Male</span><br><span class="line">10        1110  David   33    Male</span><br><span class="line">11        1111     Dw    3  Female</span><br><span class="line">12        1112      Q   23    Male</span><br><span class="line">13        1113      W   21  Female</span><br></pre></td></tr></table></figure><hr><h4 id="矩阵合并-简单-concat"><a href="#矩阵合并-简单-concat" class="headerlink" title="矩阵合并 (简单)concat"></a>矩阵合并 (简单)concat</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment">#concatenating  合并dataframe</span></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df3 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">print(df1)</span><br><span class="line">print(df2)</span><br><span class="line">print(df3)</span><br><span class="line">res = pd.concat([df1,df2,df3],axis=<span class="number">0</span>,ignore_index=<span class="keyword">True</span>)<span class="comment">#竖向合并,ignore_index 重新排序</span></span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">#join,['inner','outer']</span></span><br><span class="line">df4 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>],index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">df5 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>,columns=[<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>],index=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(df4)</span><br><span class="line">print(df5)</span><br><span class="line">res1 = pd.concat([df4,df5],join=<span class="string">'inner'</span>,ignore_index=<span class="keyword">True</span>)<span class="comment">#将不匹配的去掉</span></span><br><span class="line">print(res1)</span><br><span class="line">res2 = pd.concat([df4,df5],join=<span class="string">'outer'</span>,ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#join_axes</span></span><br><span class="line">res3 = pd.concat([df4,df5],axis=<span class="number">1</span>,join_axes=[df4.index])<span class="comment">#引用df4的index，df5没有的用NaN代替，多余的去除</span></span><br><span class="line">print(res3)</span><br><span class="line"><span class="comment">#append</span></span><br><span class="line">df6 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">0</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df7 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">1</span>,columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">df8 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">4</span>))*<span class="number">2</span>,columns=[<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>],index=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">res4 = df6.append(df7,ignore_index=<span class="keyword">True</span>)</span><br><span class="line">res5 = df6.append([df7,df8],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res4)</span><br><span class="line">print(res5)</span><br><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])<span class="comment">#仅添加加一行</span></span><br><span class="line">res6 = df6.append(s1,ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(res6)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#concat</span></span><br><span class="line">     a    b    c    d</span><br><span class="line">0  0.0  0.0  0.0  0.0</span><br><span class="line">1  0.0  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0  0.0</span><br><span class="line">     a    b    c    d</span><br><span class="line">0  1.0  1.0  1.0  1.0</span><br><span class="line">1  1.0  1.0  1.0  1.0</span><br><span class="line">2  1.0  1.0  1.0  1.0</span><br><span class="line">     a    b    c    d</span><br><span class="line">0  2.0  2.0  2.0  2.0</span><br><span class="line">1  2.0  2.0  2.0  2.0</span><br><span class="line">2  2.0  2.0  2.0  2.0</span><br><span class="line">     a    b    c    d</span><br><span class="line">0  0.0  0.0  0.0  0.0</span><br><span class="line">1  0.0  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0  0.0</span><br><span class="line">3  1.0  1.0  1.0  1.0</span><br><span class="line">4  1.0  1.0  1.0  1.0</span><br><span class="line">5  1.0  1.0  1.0  1.0</span><br><span class="line">6  2.0  2.0  2.0  2.0</span><br><span class="line">7  2.0  2.0  2.0  2.0</span><br><span class="line">8  2.0  2.0  2.0  2.0</span><br><span class="line"><span class="comment">#join</span></span><br><span class="line">     a    b    c    d</span><br><span class="line">1  0.0  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0  0.0</span><br><span class="line">3  0.0  0.0  0.0  0.0</span><br><span class="line">     b    c    d    e</span><br><span class="line">2  1.0  1.0  1.0  1.0</span><br><span class="line">3  1.0  1.0  1.0  1.0</span><br><span class="line">4  1.0  1.0  1.0  1.0</span><br><span class="line">     b    c    d</span><br><span class="line">0  0.0  0.0  0.0</span><br><span class="line">1  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0</span><br><span class="line">3  1.0  1.0  1.0</span><br><span class="line">4  1.0  1.0  1.0</span><br><span class="line">5  1.0  1.0  1.0</span><br><span class="line">     a    b    c    d    e</span><br><span class="line">0  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">1  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">2  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">3  NaN  1.0  1.0  1.0  1.0</span><br><span class="line">4  NaN  1.0  1.0  1.0  1.0</span><br><span class="line">5  NaN  1.0  1.0  1.0  1.0</span><br><span class="line"><span class="comment">#join_axes</span></span><br><span class="line">     a    b    c    d    b    c    d    e</span><br><span class="line">1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN</span><br><span class="line">2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0</span><br><span class="line">3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0</span><br><span class="line"><span class="comment">#append</span></span><br><span class="line">     a    b    c    d</span><br><span class="line">0  0.0  0.0  0.0  0.0</span><br><span class="line">1  0.0  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0  0.0</span><br><span class="line">3  1.0  1.0  1.0  1.0</span><br><span class="line">4  1.0  1.0  1.0  1.0</span><br><span class="line">5  1.0  1.0  1.0  1.0</span><br><span class="line">     a    b    c    d    e</span><br><span class="line">0  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">1  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">2  0.0  0.0  0.0  0.0  NaN</span><br><span class="line">3  1.0  1.0  1.0  1.0  NaN</span><br><span class="line">4  1.0  1.0  1.0  1.0  NaN</span><br><span class="line">5  1.0  1.0  1.0  1.0  NaN</span><br><span class="line">6  NaN  2.0  2.0  2.0  2.0</span><br><span class="line">7  NaN  2.0  2.0  2.0  2.0</span><br><span class="line">8  NaN  2.0  2.0  2.0  2.0</span><br><span class="line">     a    b    c    d</span><br><span class="line">0  0.0  0.0  0.0  0.0</span><br><span class="line">1  0.0  0.0  0.0  0.0</span><br><span class="line">2  0.0  0.0  0.0  0.0</span><br><span class="line">3  1.0  2.0  3.0  4.0</span><br></pre></td></tr></table></figure><hr><h4 id="矩阵合并-merge"><a href="#矩阵合并-merge" class="headerlink" title="矩阵合并 merge"></a>矩阵合并 merge</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment">#Merge</span></span><br><span class="line"><span class="comment">#merging two df by key/keys.(may be used in database)</span></span><br><span class="line">left = pd.DataFrame(&#123;<span class="string">'key'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>,<span class="string">'K3'</span>],</span><br><span class="line"><span class="string">'A'</span>:[<span class="string">'A0'</span>,<span class="string">'A1'</span>,<span class="string">'A2'</span>,<span class="string">'A3'</span>],</span><br><span class="line"><span class="string">'B'</span>:[<span class="string">'B0'</span>,<span class="string">'B1'</span>,<span class="string">'B2'</span>,<span class="string">'B3'</span>]&#125;)</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">'key'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>,<span class="string">'K3'</span>],</span><br><span class="line"><span class="string">'C'</span>:[<span class="string">'C0'</span>,<span class="string">'C1'</span>,<span class="string">'C2'</span>,<span class="string">'C3'</span>],</span><br><span class="line"><span class="string">'D'</span>:[<span class="string">'D0'</span>,<span class="string">'D1'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>]&#125;)</span><br><span class="line">print(left)</span><br><span class="line">print(right)</span><br><span class="line">res = pd.merge(left,right,on=<span class="string">'key'</span>)<span class="comment">#基于key合并</span></span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2列key</span></span><br><span class="line">left1 = pd.DataFrame(&#123;<span class="string">'key1'</span>:[<span class="string">'K0'</span>,<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>],</span><br><span class="line"><span class="string">'key2'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K0'</span>,<span class="string">'K1'</span>],</span><br><span class="line"><span class="string">'A'</span>:[<span class="string">'A0'</span>,<span class="string">'A1'</span>,<span class="string">'A2'</span>,<span class="string">'A3'</span>],</span><br><span class="line"><span class="string">'B'</span>:[<span class="string">'B0'</span>,<span class="string">'B1'</span>,<span class="string">'B2'</span>,<span class="string">'B3'</span>]&#125;)</span><br><span class="line">right1 = pd.DataFrame(&#123;<span class="string">'key1'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>],</span><br><span class="line"><span class="string">'key2'</span>:[<span class="string">'K0'</span>,<span class="string">'K0'</span>,<span class="string">'K0'</span>,<span class="string">'K0'</span>],</span><br><span class="line"><span class="string">'C'</span>:[<span class="string">'C0'</span>,<span class="string">'C1'</span>,<span class="string">'C2'</span>,<span class="string">'C3'</span>],</span><br><span class="line"><span class="string">'D'</span>:[<span class="string">'D0'</span>,<span class="string">'D1'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>]&#125;)</span><br><span class="line">print(left1)</span><br><span class="line">print(right1)</span><br><span class="line">res = pd.merge(left1,right1,on=[<span class="string">'key1'</span>,<span class="string">'key2'</span>])<span class="comment">#默认inner</span></span><br><span class="line">print(res)</span><br><span class="line"><span class="comment">#how = ['left','right','inner','outer']</span></span><br><span class="line">res = pd.merge(left1,right1,on=[<span class="string">'key1'</span>,<span class="string">'key2'</span>],how=<span class="string">'outer'</span>)</span><br><span class="line">print(res)</span><br><span class="line">res = pd.merge(left1,right1,on=[<span class="string">'key1'</span>,<span class="string">'key2'</span>],how=<span class="string">'left'</span>)<span class="comment">#基于left填充</span></span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># indicator显示提示合并方式</span></span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">'col1'</span>:[<span class="number">0</span>,<span class="number">1</span>],<span class="string">'col_left'</span>:[<span class="string">'a'</span>,<span class="string">'b'</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'col1'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],<span class="string">'col_left'</span>:[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]&#125;)</span><br><span class="line">print(df1)</span><br><span class="line">print(df2)</span><br><span class="line">res = pd.merge(df1,df2,on=<span class="string">'col1'</span>,how=<span class="string">'outer'</span>,indicator=<span class="keyword">True</span>)</span><br><span class="line">print(res)</span><br><span class="line">res = pd.merge(df1,df2,on=<span class="string">'col1'</span>,how=<span class="string">'outer'</span>,indicator=<span class="string">'in_col'</span>)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">#index</span></span><br><span class="line">left = pd.DataFrame(&#123;<span class="string">'A'</span>:[<span class="string">'A0'</span>,<span class="string">'A1'</span>,<span class="string">'A2'</span>],</span><br><span class="line"><span class="string">'B'</span>:[<span class="string">'B0'</span>,<span class="string">'B1'</span>,<span class="string">'B2'</span>]&#125;,</span><br><span class="line">index=[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>])</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">'C'</span>:[<span class="string">'C0'</span>,<span class="string">'C2'</span>,<span class="string">'C3'</span>],</span><br><span class="line"><span class="string">'D'</span>:[<span class="string">'D0'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>]&#125;,</span><br><span class="line">index=[<span class="string">'K0'</span>,<span class="string">'K2'</span>,<span class="string">'K3'</span>])</span><br><span class="line">print(left)</span><br><span class="line">print(right)</span><br><span class="line">res = pd.merge(left,right,left_index=<span class="keyword">True</span>,right_index=<span class="keyword">True</span>,how=<span class="string">'outer'</span>)</span><br><span class="line">print(res)</span><br><span class="line">res = pd.merge(left,right,left_index=<span class="keyword">True</span>,right_index=<span class="keyword">True</span>,how=<span class="string">'inner'</span>)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">#overlapping</span></span><br><span class="line">boys = pd.DataFrame(&#123;<span class="string">'k'</span>:[<span class="string">'K0'</span>,<span class="string">'K1'</span>,<span class="string">'K2'</span>],<span class="string">'age'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;)</span><br><span class="line">girls = pd.DataFrame(&#123;<span class="string">'k'</span>:[<span class="string">'K0'</span>,<span class="string">'K0'</span>,<span class="string">'K3'</span>],<span class="string">'age'</span>:[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]&#125;)</span><br><span class="line">print(boys)</span><br><span class="line">print(girls)</span><br><span class="line"><span class="comment">#suffixes用于区分</span></span><br><span class="line">res = pd.merge(boys,girls,on=<span class="string">'k'</span>,suffixes=[<span class="string">'_boy'</span>,<span class="string">'_girl'</span>],how=<span class="string">'inner'</span>)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">#join与merge类似</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Merge</span></span><br><span class="line">  key   A   B</span><br><span class="line">0  K0  A0  B0</span><br><span class="line">1  K1  A1  B1</span><br><span class="line">2  K2  A2  B2</span><br><span class="line">3  K3  A3  B3</span><br><span class="line">  key   C   D</span><br><span class="line">0  K0  C0  D0</span><br><span class="line">1  K1  C1  D1</span><br><span class="line">2  K2  C2  D2</span><br><span class="line">3  K3  C3  D3</span><br><span class="line">  key   A   B   C   D</span><br><span class="line">0  K0  A0  B0  C0  D0</span><br><span class="line">1  K1  A1  B1  C1  D1</span><br><span class="line">2  K2  A2  B2  C2  D2</span><br><span class="line">3  K3  A3  B3  C3  D3</span><br><span class="line"><span class="comment"># 2列key</span></span><br><span class="line">  key1 key2   A   B</span><br><span class="line">0   K0   K0  A0  B0</span><br><span class="line">1   K0   K1  A1  B1</span><br><span class="line">2   K1   K0  A2  B2</span><br><span class="line">3   K2   K1  A3  B3</span><br><span class="line">  key1 key2   C   D</span><br><span class="line">0   K0   K0  C0  D0</span><br><span class="line">1   K1   K0  C1  D1</span><br><span class="line">2   K1   K0  C2  D2</span><br><span class="line">3   K2   K0  C3  D3</span><br><span class="line">  key1 key2   A   B   C   D</span><br><span class="line">0   K0   K0  A0  B0  C0  D0</span><br><span class="line">1   K1   K0  A2  B2  C1  D1</span><br><span class="line">2   K1   K0  A2  B2  C2  D2</span><br><span class="line">  key1 key2    A    B    C    D</span><br><span class="line">0   K0   K0   A0   B0   C0   D0</span><br><span class="line">1   K0   K1   A1   B1  NaN  NaN</span><br><span class="line">2   K1   K0   A2   B2   C1   D1</span><br><span class="line">3   K1   K0   A2   B2   C2   D2</span><br><span class="line">4   K2   K1   A3   B3  NaN  NaN</span><br><span class="line">5   K2   K0  NaN  NaN   C3   D3</span><br><span class="line">  key1 key2   A   B    C    D</span><br><span class="line">0   K0   K0  A0  B0   C0   D0</span><br><span class="line">1   K0   K1  A1  B1  NaN  NaN</span><br><span class="line">2   K1   K0  A2  B2   C1   D1</span><br><span class="line">3   K1   K0  A2  B2   C2   D2</span><br><span class="line">4   K2   K1  A3  B3  NaN  NaN</span><br><span class="line"><span class="comment"># indicator显示提示合并方式</span></span><br><span class="line">   col1 col_left</span><br><span class="line">0     0        a</span><br><span class="line">1     1        b</span><br><span class="line">   col1  col_left</span><br><span class="line">0     1         2</span><br><span class="line">1     2         2</span><br><span class="line">2     2         2</span><br><span class="line">   col1 col_left_x  col_left_y      _merge</span><br><span class="line">0     0          a         NaN   left_only</span><br><span class="line">1     1          b         2.0        both</span><br><span class="line">2     2        NaN         2.0  right_only</span><br><span class="line">3     2        NaN         2.0  right_only</span><br><span class="line">   col1 col_left_x  col_left_y      in_col</span><br><span class="line">0     0          a         NaN   left_only</span><br><span class="line">1     1          b         2.0        both</span><br><span class="line">2     2        NaN         2.0  right_only</span><br><span class="line">3     2        NaN         2.0  right_only</span><br><span class="line"><span class="comment">#index</span></span><br><span class="line">     A   B</span><br><span class="line">K0  A0  B0</span><br><span class="line">K1  A1  B1</span><br><span class="line">K2  A2  B2</span><br><span class="line">     C   D</span><br><span class="line">K0  C0  D0</span><br><span class="line">K2  C2  D2</span><br><span class="line">K3  C3  D3</span><br><span class="line">      A    B    C    D</span><br><span class="line">K0   A0   B0   C0   D0</span><br><span class="line">K1   A1   B1  NaN  NaN</span><br><span class="line">K2   A2   B2   C2   D2</span><br><span class="line">K3  NaN  NaN   C3   D3</span><br><span class="line">     A   B   C   D</span><br><span class="line">K0  A0  B0  C0  D0</span><br><span class="line">K2  A2  B2  C2  D2</span><br><span class="line"><span class="comment">#overlapping</span></span><br><span class="line">    k  age</span><br><span class="line">0  K0    1</span><br><span class="line">1  K1    2</span><br><span class="line">2  K2    3</span><br><span class="line">    k  age</span><br><span class="line">0  K0    4</span><br><span class="line">1  K0    5</span><br><span class="line">2  K3    6</span><br><span class="line">    k  age_boy  age_girl</span><br><span class="line">0  K0        1         4</span><br><span class="line">1  K0        1         5</span><br></pre></td></tr></table></figure><hr><h4 id="可视化-plot"><a href="#可视化-plot" class="headerlink" title="可视化 plot"></a>可视化 plot</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="comment">#plot data</span></span><br><span class="line"><span class="comment">#Series</span></span><br><span class="line">data = pd.Series(np.random.randn(<span class="number">1000</span>),index=np.arange(<span class="number">1000</span>))</span><br><span class="line"><span class="comment">#DataFrame</span></span><br><span class="line">data = pd.DataFrame(np.random.randn(<span class="number">1000</span>,<span class="number">4</span>),<span class="comment">#4个属性</span></span><br><span class="line">index=np.arange(<span class="number">1000</span>),</span><br><span class="line">columns=list(<span class="string">"ABCD"</span>))</span><br><span class="line"></span><br><span class="line">print(data.head())<span class="comment">#显示前五个数据，默认是5</span></span><br><span class="line">data = data.cumsum()<span class="comment">#累加</span></span><br><span class="line">data.plot()</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot methods</span></span><br><span class="line"><span class="comment">#'bar','hist','box','kde','area','scatter','pie','hexbin'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ax = data.plot.scatter(x='A',y='B',color='DarkBlue',label='Class 1')#两个属性</span></span><br><span class="line"><span class="comment"># data.plot.scatter(x='A',y='C',color='DarkGreen',label='Class 2',ax=ax)#ax=ax,表示在一张图打印两张数据</span></span><br><span class="line">plt.showr</span><br></pre></td></tr></table></figure><p><img src="https://github.com/soloistben/images/raw/master/pandas_image/1.PNG" alt="series"><br><img src="https://github.com/soloistben/images/raw/master/pandas_image/2.PNG" alt="dataframe"><br><img src="https://github.com/soloistben/images/raw/master/pandas_image/3.PNG" alt="scatter"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;pandas属性&quot;&gt;&lt;a href=&quot;#pandas属性&quot; class=&quot;headerlink&quot; title=&quot;pandas属性&quot;&gt;&lt;/a&gt;pandas属性&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>octave_property</title>
    <link href="http://yoursite.com/2019/01/05/octave-property/"/>
    <id>http://yoursite.com/2019/01/05/octave-property/</id>
    <published>2019-01-05T15:52:11.000Z</published>
    <updated>2019-07-19T03:45:01.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1）基本操作"><a href="#1）基本操作" class="headerlink" title="1）基本操作"></a>1）基本操作</h3><h4 id="加减乘除、等、不等、注释、与、或、异或"><a href="#加减乘除、等、不等、注释、与、或、异或" class="headerlink" title="加减乘除、等、不等、注释、与、或、异或"></a>加减乘除、等、不等、注释、与、或、异或</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/1.jpg" alt="image" title="基本操作"></p><h4 id="直接输出（不加分号）、加分号阻止输出（形成变量）、类似C一样限制输出变量、格式变换"><a href="#直接输出（不加分号）、加分号阻止输出（形成变量）、类似C一样限制输出变量、格式变换" class="headerlink" title="直接输出（不加分号）、加分号阻止输出（形成变量）、类似C一样限制输出变量、格式变换"></a>直接输出（不加分号）、加分号阻止输出（形成变量）、类似C一样限制输出变量、格式变换</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/2.jpg" alt="image"></p><h4 id="普通矩阵"><a href="#普通矩阵" class="headerlink" title="普通矩阵"></a>普通矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/3.jpg" alt="image"></p><h4 id="行、列矩阵"><a href="#行、列矩阵" class="headerlink" title="行、列矩阵"></a>行、列矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/4.jpg" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/5.jpg" alt="image"></p><h4 id="全1矩阵"><a href="#全1矩阵" class="headerlink" title="全1矩阵"></a>全1矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/6.jpg" alt="image"></p><h4 id="全0矩阵"><a href="#全0矩阵" class="headerlink" title="全0矩阵"></a>全0矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/7.jpg" alt="image"></p><h4 id="单位矩阵"><a href="#单位矩阵" class="headerlink" title="单位矩阵"></a>单位矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/8.jpg" alt="image"></p><h4 id="随机矩阵"><a href="#随机矩阵" class="headerlink" title="随机矩阵"></a>随机矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/9.jpg" alt="image"></p><h4 id="服从高斯分布的随机矩阵"><a href="#服从高斯分布的随机矩阵" class="headerlink" title="服从高斯分布的随机矩阵"></a>服从高斯分布的随机矩阵</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/10.jpg" alt="image"></p><h4 id="范围值（中间可添加增量）"><a href="#范围值（中间可添加增量）" class="headerlink" title="范围值（中间可添加增量）"></a>范围值（中间可添加增量）</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/11.jpg" alt="image"></p><h4 id="高斯分布直方图"><a href="#高斯分布直方图" class="headerlink" title="高斯分布直方图"></a>高斯分布直方图</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/12.jpg" alt="image"></p><h5 id="help-命令（查看命令使用）"><a href="#help-命令（查看命令使用）" class="headerlink" title="help + 命令（查看命令使用）"></a>help + 命令（查看命令使用）</h5><h3 id="2）数据移动操作"><a href="#2）数据移动操作" class="headerlink" title="2）数据移动操作"></a>2）数据移动操作</h3><h5 id="size-A-（返回矩阵类型，几行几列，也是一个1-2的一维矩阵）"><a href="#size-A-（返回矩阵类型，几行几列，也是一个1-2的一维矩阵）" class="headerlink" title="size(A)（返回矩阵类型，几行几列，也是一个1*2的一维矩阵）"></a>size(A)（返回矩阵类型，几行几列，也是一个1*2的一维矩阵）</h5><h5 id="size-A-1-（返回矩阵A的行数）"><a href="#size-A-1-（返回矩阵A的行数）" class="headerlink" title="size(A,1)（返回矩阵A的行数）"></a>size(A,1)（返回矩阵A的行数）</h5><h5 id="length-A-（返回矩阵A的行数）"><a href="#length-A-（返回矩阵A的行数）" class="headerlink" title="length(A)（返回矩阵A的行数）"></a>length(A)（返回矩阵A的行数）</h5><h5 id="load-xx-dat（加载当前目录的xx-dat文件数据，其xx为变量名称，文件数据为变量值，用于存取矩阵数据）"><a href="#load-xx-dat（加载当前目录的xx-dat文件数据，其xx为变量名称，文件数据为变量值，用于存取矩阵数据）" class="headerlink" title="load xx.dat（加载当前目录的xx.dat文件数据，其xx为变量名称，文件数据为变量值，用于存取矩阵数据）"></a>load xx.dat（加载当前目录的xx.dat文件数据，其xx为变量名称，文件数据为变量值，用于存取矩阵数据）</h5><h5 id="who（显示当前已有变量）"><a href="#who（显示当前已有变量）" class="headerlink" title="who（显示当前已有变量）"></a>who（显示当前已有变量）</h5><h5 id="whos（显示当前已有变量详情）"><a href="#whos（显示当前已有变量详情）" class="headerlink" title="whos（显示当前已有变量详情）"></a>whos（显示当前已有变量详情）</h5><h5 id="clear-xx（清除变量）"><a href="#clear-xx（清除变量）" class="headerlink" title="clear xx（清除变量）"></a>clear xx（清除变量）</h5><h5 id="v-priceY-1-10-（将priceY的前十个元素赋值给v，前提priceY有这么多或更多的数据）"><a href="#v-priceY-1-10-（将priceY的前十个元素赋值给v，前提priceY有这么多或更多的数据）" class="headerlink" title="v = priceY(1,10);（将priceY的前十个元素赋值给v，前提priceY有这么多或更多的数据）"></a>v = priceY(1,10);（将priceY的前十个元素赋值给v，前提priceY有这么多或更多的数据）</h5><h5 id="save-vv-mat-v（将v变量的内容保存在本地vv-mat文件中）"><a href="#save-vv-mat-v（将v变量的内容保存在本地vv-mat文件中）" class="headerlink" title="save vv.mat v（将v变量的内容保存在本地vv.mat文件中）"></a>save vv.mat v（将v变量的内容保存在本地vv.mat文件中）</h5><h5 id="save-vv-txt-v-ascii（以ascii形式保存数据）"><a href="#save-vv-txt-v-ascii（以ascii形式保存数据）" class="headerlink" title="save vv.txt v -ascii（以ascii形式保存数据）"></a>save vv.txt v -ascii（以ascii形式保存数据）</h5><h5 id="clear（会清除所以变量）"><a href="#clear（会清除所以变量）" class="headerlink" title="clear（会清除所以变量）"></a>clear（会清除所以变量）</h5><h5 id="load-vv-mat（会将文件内容恢复到原有变量中，即使原因变量被清除也会新建同名变量）"><a href="#load-vv-mat（会将文件内容恢复到原有变量中，即使原因变量被清除也会新建同名变量）" class="headerlink" title="load vv.mat（会将文件内容恢复到原有变量中，即使原因变量被清除也会新建同名变量）"></a>load vv.mat（会将文件内容恢复到原有变量中，即使原因变量被清除也会新建同名变量）</h5><h5 id="A-2-3-（A矩阵中的-行为2列为3位置的元素）"><a href="#A-2-3-（A矩阵中的-行为2列为3位置的元素）" class="headerlink" title="A(2,3)（A矩阵中的 行为2列为3位置的元素）"></a>A(2,3)（A矩阵中的 行为2列为3位置的元素）</h5><h5 id="A-2-（第2行的所有元素）"><a href="#A-2-（第2行的所有元素）" class="headerlink" title="A(2,:)（第2行的所有元素）"></a>A(2,:)（第2行的所有元素）</h5><h5 id="A-2-（第2列的所有元素）"><a href="#A-2-（第2列的所有元素）" class="headerlink" title="A(:,2)（第2列的所有元素）"></a>A(:,2)（第2列的所有元素）</h5><h5 id="A-1-3-（第1和第三行的所有元素）"><a href="#A-1-3-（第1和第三行的所有元素）" class="headerlink" title="A([1 3],:)（第1和第三行的所有元素）"></a>A([1 3],:)（第1和第三行的所有元素）</h5><h5 id="A-2-1-2-3-（A矩阵第2列直接赋值-1-2-3-，前提行列大小符合）"><a href="#A-2-1-2-3-（A矩阵第2列直接赋值-1-2-3-，前提行列大小符合）" class="headerlink" title="A(:,2)=[1;2;3]（A矩阵第2列直接赋值[1;2;3]，前提行列大小符合）"></a>A(:,2)=[1;2;3]（A矩阵第2列直接赋值[1;2;3]，前提行列大小符合）</h5><h5 id="A-A-1-2-3-（在原矩阵中多添加一列-1-2-3-）"><a href="#A-A-1-2-3-（在原矩阵中多添加一列-1-2-3-）" class="headerlink" title="A=[A,[1;2;3;]]（在原矩阵中多添加一列[1;2;3]）"></a>A=[A,[1;2;3;]]（在原矩阵中多添加一列[1;2;3]）</h5><h5 id="A-（将3x3的A矩阵转成9x1列向量一样输出）"><a href="#A-（将3x3的A矩阵转成9x1列向量一样输出）" class="headerlink" title="A(:)（将3x3的A矩阵转成9x1列向量一样输出）"></a>A(:)（将3x3的A矩阵转成9x1列向量一样输出）</h5><h5 id="C-A-B-（将A-B矩阵上下组合成C）"><a href="#C-A-B-（将A-B矩阵上下组合成C）" class="headerlink" title="C=[A;B]（将A,B矩阵上下组合成C）"></a>C=[A;B]（将A,B矩阵上下组合成C）</h5><h5 id="C-A-B-（将A-B矩阵左右组合成C）"><a href="#C-A-B-（将A-B矩阵左右组合成C）" class="headerlink" title="C=[A B]（将A,B矩阵左右组合成C）"></a>C=[A B]（将A,B矩阵左右组合成C）</h5><h5 id="（矩阵中-A-B-等价于-A-B-）"><a href="#（矩阵中-A-B-等价于-A-B-）" class="headerlink" title="（矩阵中[A B]等价于[A,B]）"></a>（矩阵中[A B]等价于[A,B]）</h5><h3 id="3）数据计算操作"><a href="#3）数据计算操作" class="headerlink" title="3）数据计算操作"></a>3）数据计算操作</h3><h5 id="A-B（A与B矩阵对应相乘）"><a href="#A-B（A与B矩阵对应相乘）" class="headerlink" title="A.*B（A与B矩阵对应相乘）"></a>A.*B（A与B矩阵对应相乘）</h5><h5 id="A-2（A矩阵元素平方）"><a href="#A-2（A矩阵元素平方）" class="headerlink" title="A.^2（A矩阵元素平方）"></a>A.^2（A矩阵元素平方）</h5><h5 id="1-A（A矩阵元素倒数）"><a href="#1-A（A矩阵元素倒数）" class="headerlink" title="1./A（A矩阵元素倒数）"></a>1./A（A矩阵元素倒数）</h5><h5 id="log-A-（对数运算）"><a href="#log-A-（对数运算）" class="headerlink" title="log(A) （对数运算）"></a>log(A) （对数运算）</h5><h5 id="exp-A-（指数运算）"><a href="#exp-A-（指数运算）" class="headerlink" title="exp(A)（指数运算）"></a>exp(A)（指数运算）</h5><h5 id="abs-A-（绝对值运算）"><a href="#abs-A-（绝对值运算）" class="headerlink" title="abs(A)（绝对值运算）"></a>abs(A)（绝对值运算）</h5><h5 id="A（取相反数）"><a href="#A（取相反数）" class="headerlink" title="-A（取相反数）"></a>-A（取相反数）</h5><h5 id="v-1（列向量v全部元素加1）等价于-v-ones-length-v-1"><a href="#v-1（列向量v全部元素加1）等价于-v-ones-length-v-1" class="headerlink" title="v+1（列向量v全部元素加1）等价于 v+ones(length(v),1)"></a>v+1（列向量v全部元素加1）等价于 v+ones(length(v),1)</h5><h5 id="A’（A的转置）"><a href="#A’（A的转置）" class="headerlink" title="A’（A的转置）"></a>A’（A的转置）</h5><h5 id="pinv-A-（A的逆）"><a href="#pinv-A-（A的逆）" class="headerlink" title="pinv(A)（A的逆）"></a>pinv(A)（A的逆）</h5><h4 id="sum求和、prod求积、floor向下取整、ceil向上取整"><a href="#sum求和、prod求积、floor向下取整、ceil向上取整" class="headerlink" title="sum求和、prod求积、floor向下取整、ceil向上取整"></a>sum求和、prod求积、floor向下取整、ceil向上取整</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/13.jpg" alt="image"></p><h5 id="sum-A-1-（元素列和）"><a href="#sum-A-1-（元素列和）" class="headerlink" title="sum(A,1)（元素列和）"></a>sum(A,1)（元素列和）</h5><h5 id="sum-A-2-（元素行和）"><a href="#sum-A-2-（元素行和）" class="headerlink" title="sum(A,2)（元素行和）"></a>sum(A,2)（元素行和）</h5><h5 id="sum-sum-A-eye-length-A-（求对角线和，eye是单位阵，前提A为方阵）"><a href="#sum-sum-A-eye-length-A-（求对角线和，eye是单位阵，前提A为方阵）" class="headerlink" title="sum(sum(A.*eye(length(A))（求对角线和，eye是单位阵，前提A为方阵）"></a>sum(sum(A.*eye(length(A))（求对角线和，eye是单位阵，前提A为方阵）</h5><h5 id="flipud-eye-3-（3阶单位矩阵的副对角线为1）"><a href="#flipud-eye-3-（3阶单位矩阵的副对角线为1）" class="headerlink" title="flipud(eye(3))（3阶单位矩阵的副对角线为1）"></a>flipud(eye(3))（3阶单位矩阵的副对角线为1）</h5><h5 id="max-v-（列向量v的最大值）"><a href="#max-v-（列向量v的最大值）" class="headerlink" title="max(v)（列向量v的最大值）"></a>max(v)（列向量v的最大值）</h5><h5 id="val-ins-max-v-（列向量v最大的两个值分别赋值）"><a href="#val-ins-max-v-（列向量v最大的两个值分别赋值）" class="headerlink" title="[val,ins]=max(v)（列向量v最大的两个值分别赋值）"></a>[val,ins]=max(v)（列向量v最大的两个值分别赋值）</h5><h5 id="max-A-（返回每一列的最大元素，组成一维向量组）"><a href="#max-A-（返回每一列的最大元素，组成一维向量组）" class="headerlink" title="max(A)（返回每一列的最大元素，组成一维向量组）"></a>max(A)（返回每一列的最大元素，组成一维向量组）</h5><h5 id="max-max-A-或者max-A-（将形成的一维向量中再找最大值）"><a href="#max-max-A-或者max-A-（将形成的一维向量中再找最大值）" class="headerlink" title="max(max(A))或者max(A(:))（将形成的一维向量中再找最大值）"></a>max(max(A))或者max(A(:))（将形成的一维向量中再找最大值）</h5><p><img src="https://github.com/soloistben/images/raw/master/octave_image/14.jpg" alt="image"></p><h5 id="（1代表找到每一列的最大值，2代表找没一行的最大值）"><a href="#（1代表找到每一列的最大值，2代表找没一行的最大值）" class="headerlink" title="（1代表找到每一列的最大值，2代表找没一行的最大值）"></a>（1代表找到每一列的最大值，2代表找没一行的最大值）</h5><p><img src="https://github.com/soloistben/images/raw/master/octave_image/15.jpg" alt="image"></p><h5 id="v-lt-3（返回一个与v相同类型的矩阵，元素对应的是1和0，1代表true该位置的v的元素比3小）"><a href="#v-lt-3（返回一个与v相同类型的矩阵，元素对应的是1和0，1代表true该位置的v的元素比3小）" class="headerlink" title="v&lt;3（返回一个与v相同类型的矩阵，元素对应的是1和0，1代表true该位置的v的元素比3小）"></a>v&lt;3（返回一个与v相同类型的矩阵，元素对应的是1和0，1代表true该位置的v的元素比3小）</h5><h5 id="find-v-lt-3-（返回比3小的元素下标）"><a href="#find-v-lt-3-（返回比3小的元素下标）" class="headerlink" title="find(v&lt;3) （返回比3小的元素下标）"></a>find(v&lt;3) （返回比3小的元素下标）</h5><h5 id="r-c-find-A-lt-3-（r返回行下标，c返回列下标）"><a href="#r-c-find-A-lt-3-（r返回行下标，c返回列下标）" class="headerlink" title="[r,c]=find(A&lt;3)（r返回行下标，c返回列下标）"></a>[r,c]=find(A&lt;3)（r返回行下标，c返回列下标）</h5><h5 id="magic-3-（返回一个3x3矩阵，行和-列和-对角线和，机器学习基本不用）"><a href="#magic-3-（返回一个3x3矩阵，行和-列和-对角线和，机器学习基本不用）" class="headerlink" title="magic(3)（返回一个3x3矩阵，行和=列和=对角线和，机器学习基本不用）"></a>magic(3)（返回一个3x3矩阵，行和=列和=对角线和，机器学习基本不用）</h5><h3 id="4）数据可视化"><a href="#4）数据可视化" class="headerlink" title="4）数据可视化"></a>4）数据可视化</h3><h4 id="输出函数图像"><a href="#输出函数图像" class="headerlink" title="输出函数图像"></a>输出函数图像</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/16.PNG" alt="image"></p><h4 id="两函数图象在同一坐标中"><a href="#两函数图象在同一坐标中" class="headerlink" title="两函数图象在同一坐标中"></a>两函数图象在同一坐标中</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/17.PNG" alt="image"></p><h4 id="分开输出"><a href="#分开输出" class="headerlink" title="分开输出"></a>分开输出</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/18.PNG" alt="image"></p><h4 id="左右输出"><a href="#左右输出" class="headerlink" title="左右输出"></a>左右输出</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/19.PNG" alt="image"></p><h4 id="改变坐标系"><a href="#改变坐标系" class="headerlink" title="改变坐标系"></a>改变坐标系</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/20.PNG" alt="image"></p><h4 id="添加横纵坐标名称、title名称、保存数据图"><a href="#添加横纵坐标名称、title名称、保存数据图" class="headerlink" title="添加横纵坐标名称、title名称、保存数据图"></a>添加横纵坐标名称、title名称、保存数据图</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/21.PNG" alt="image"></p><h4 id="矩阵输出图像"><a href="#矩阵输出图像" class="headerlink" title="矩阵输出图像"></a>矩阵输出图像</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/22.PNG" alt="image"><br><img src="https://github.com/soloistben/images/raw/master/octave_image/23.png" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/24.png" alt="image"></p><h3 id="5）基础函数使用"><a href="#5）基础函数使用" class="headerlink" title="5）基础函数使用"></a>5）基础函数使用</h3><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/25.PNG" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/26.PNG" alt="image"></p><h4 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/27.PNG" alt="image"></p><h4 id="break操作"><a href="#break操作" class="headerlink" title="break操作"></a>break操作</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/28.PNG" alt="image"></p><h4 id="if操作"><a href="#if操作" class="headerlink" title="if操作"></a>if操作</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/29.PNG" alt="image"></p><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><h4 id="​-在当前目录下建立xxx-m文件"><a href="#​-在当前目录下建立xxx-m文件" class="headerlink" title="​    在当前目录下建立xxx.m文件"></a>​    在当前目录下建立xxx.m文件</h4><h4 id="则xxx是函数名称"><a href="#则xxx是函数名称" class="headerlink" title="则xxx是函数名称"></a>则xxx是函数名称</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/30.PNG" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/31.PNG" alt="image"></p><h4 id="函数可返回多个值"><a href="#函数可返回多个值" class="headerlink" title="函数可返回多个值"></a>函数可返回多个值</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/32.PNG" alt="image"><br><img src="https://github.com/soloistben/images/raw/master/octave_image/33.PNG" alt="image"></p><h4 id="复杂函数"><a href="#复杂函数" class="headerlink" title="复杂函数"></a>复杂函数</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/34.PNG" alt="image"><br><img src="https://github.com/soloistben/images/raw/master/octave_image/35.PNG" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/36.PNG" alt="image"> </p><h3 id="6）函数向量化"><a href="#6）函数向量化" class="headerlink" title="6）函数向量化"></a>6）函数向量化</h3><h4 id="循环的数组-gt-一维行或列向量"><a href="#循环的数组-gt-一维行或列向量" class="headerlink" title="循环的数组 =&gt; 一维行或列向量"></a>循环的数组 =&gt; 一维行或列向量</h4><p><img src="https://github.com/soloistben/images/raw/master/octave_image/37.PNG" alt="image"> <img src="https://github.com/soloistben/images/raw/master/octave_image/38.PNG" alt="image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1）基本操作&quot;&gt;&lt;a href=&quot;#1）基本操作&quot; class=&quot;headerlink&quot; title=&quot;1）基本操作&quot;&gt;&lt;/a&gt;1）基本操作&lt;/h3&gt;&lt;h4 id=&quot;加减乘除、等、不等、注释、与、或、异或&quot;&gt;&lt;a href=&quot;#加减乘除、等、不等、注释、与、或、异
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>numpy_property</title>
    <link href="http://yoursite.com/2019/01/05/numpy-property/"/>
    <id>http://yoursite.com/2019/01/05/numpy-property/</id>
    <published>2019-01-05T15:12:27.000Z</published>
    <updated>2019-01-06T06:53:06.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="numpy-属性"><a href="#numpy-属性" class="headerlink" title="numpy 属性"></a>numpy 属性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">              [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,]])</span><br><span class="line">print(a)</span><br><span class="line">print(<span class="string">'number of dim:'</span>,a.ndim)      <span class="comment">#维度</span></span><br><span class="line">print(<span class="string">'shape:'</span>,a.shape)     <span class="comment">#矩阵类型</span></span><br><span class="line">print(<span class="string">'size:'</span>,a.size)</span><br><span class="line"></span><br><span class="line">a1 = np.array([<span class="number">2</span>,<span class="number">23</span>,<span class="number">4</span>],dtype=np.float64)    <span class="comment">#不同类型数组</span></span><br><span class="line">print(<span class="string">'a1_type:'</span>,a1.dtype)</span><br><span class="line"></span><br><span class="line">z = np.zeros((<span class="number">4</span>,<span class="number">3</span>))<span class="comment">#零矩阵</span></span><br><span class="line">print(z)</span><br><span class="line"></span><br><span class="line">o = np.ones((<span class="number">3</span>,<span class="number">4</span>),dtype=np.int16)<span class="comment">#全1矩阵</span></span><br><span class="line">print(o)</span><br><span class="line"></span><br><span class="line">e = np.empty((<span class="number">3</span>,<span class="number">2</span>))<span class="comment">#空矩阵</span></span><br><span class="line">print(e)</span><br><span class="line"></span><br><span class="line">e1 = np.eye(<span class="number">3</span>)<span class="comment">#单位矩阵</span></span><br><span class="line">print(e1)</span><br><span class="line"></span><br><span class="line">l = np.arange(<span class="number">10</span>,<span class="number">20</span>,<span class="number">3</span>)  <span class="comment">#3是间距</span></span><br><span class="line">print(l)</span><br><span class="line">ll = np.arange(<span class="number">10</span>)</span><br><span class="line">print(ll)</span><br><span class="line">lll = np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))<span class="comment">#重组成对应矩阵</span></span><br><span class="line">print(lll)</span><br><span class="line"></span><br><span class="line">lis = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">5</span>)   <span class="comment">#生成线段</span></span><br><span class="line">print(lis)</span><br><span class="line">lis1 = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)   <span class="comment">#生成线段矩阵</span></span><br><span class="line">print(lis1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">number of dim: 2</span><br><span class="line">shape: (2, 3)</span><br><span class="line">size: 6</span><br><span class="line">a1_type: float64</span><br><span class="line"></span><br><span class="line">[[ 0.  0.  0.]</span><br><span class="line"> [ 0.  0.  0.]</span><br><span class="line"> [ 0.  0.  0.]</span><br><span class="line"> [ 0.  0.  0.]]</span><br><span class="line">[[1 1 1 1]</span><br><span class="line"> [1 1 1 1]</span><br><span class="line"> [1 1 1 1]]</span><br><span class="line">[[  2.67276450e+185   1.69506143e+190]</span><br><span class="line"> [  1.75184137e+190   9.48819320e+077]</span><br><span class="line"> [  1.63730399e-306   0.00000000e+000]]</span><br><span class="line">[[ 1.  0.  0.]</span><br><span class="line"> [ 0.  1.  0.]</span><br><span class="line"> [ 0.  0.  1.]]</span><br><span class="line"> </span><br><span class="line">[10 13 16 19]</span><br><span class="line">[0 1 2 3 4 5 6 7 8 9]</span><br><span class="line">[[ 0  1  2  3]</span><br><span class="line"> [ 4  5  6  7]</span><br><span class="line"> [ 8  9 10 11]]</span><br><span class="line"> </span><br><span class="line">[  1.     3.25   5.5    7.75  10.  ]</span><br><span class="line">[[  1.    2.8   4.6]</span><br><span class="line"> [  6.4   8.2  10. ]]</span><br></pre></td></tr></table></figure><h4 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一维</span></span><br><span class="line">a = np.array([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>])</span><br><span class="line">b = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(<span class="string">'a:'</span>,a,<span class="string">'b:'</span>,b)</span><br><span class="line">c = a-b     <span class="comment">#加减一样操作</span></span><br><span class="line">print(<span class="string">'c=a-b:'</span>,c)</span><br><span class="line">d = b**<span class="number">2</span>    <span class="comment">#b的平方</span></span><br><span class="line">print(<span class="string">'d=b^2:'</span>,d)</span><br><span class="line">s = <span class="number">10</span>*np.sin(a)</span><br><span class="line">print(<span class="string">'s=10*sin(a):'</span>,s)</span><br><span class="line">print(<span class="string">'b&lt;3:'</span>,b&lt;<span class="number">3</span>)</span><br><span class="line">print(<span class="string">'b==3:'</span>,b==<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a: [10 20 30 40] b: [1 2 3 4]</span><br><span class="line">c=a-b: [ 9 18 27 36]</span><br><span class="line">d=b^2: [ 1  4  9 16]</span><br><span class="line">s=10*sin(a): [-5.44021111  9.12945251 -9.88031624  7.4511316 ]</span><br><span class="line">b&lt;3: [ True  True False False]</span><br><span class="line">b==3: [False False  True False]</span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多维</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">b = np.arange(<span class="number">4</span>).reshape((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c = a*b     <span class="comment">#逐个相乘</span></span><br><span class="line">c_dot = np.dot(a,b)     <span class="comment">#矩阵相乘</span></span><br><span class="line">c_dot_2 = a.dot(b)</span><br><span class="line">print(<span class="string">'c=a*b:\n'</span>,c)</span><br><span class="line">print(<span class="string">'c_dot=np.dot(a,b):\n'</span>,c_dot)</span><br><span class="line">print(<span class="string">'c_dot = np.dot(a,b):\n'</span>,c_dot_2)</span><br><span class="line"></span><br><span class="line">arr = np.random.random((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">print(arr)</span><br><span class="line">print(<span class="string">'sum:'</span>,np.sum(arr))</span><br><span class="line">print(<span class="string">'sum_row:'</span>,np.sum(arr,axis=<span class="number">0</span>))</span><br><span class="line">print(<span class="string">'sum_col:'</span>,np.sum(arr,axis=<span class="number">1</span>))</span><br><span class="line">print(<span class="string">'min:'</span>,np.min(arr))</span><br><span class="line">print(<span class="string">'max:'</span>,np.max(arr))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[[1 1]</span><br><span class="line"> [1 0]]</span><br><span class="line">[[0 1]</span><br><span class="line"> [2 3]]</span><br><span class="line">c=a*b: </span><br><span class="line">[[0 1]</span><br><span class="line"> [2 0]]</span><br><span class="line">c_dot=np.dot(a,b): </span><br><span class="line">[[2 4]</span><br><span class="line"> [0 1]]</span><br><span class="line">c_dot = np.dot(a,b):   </span><br><span class="line">[[2 4]</span><br><span class="line"> [0 1]]</span><br><span class="line">[[ 0.65500072  0.70859584]</span><br><span class="line"> [ 0.70246984  0.07589378]</span><br><span class="line"> [ 0.05927354  0.67936378]]</span><br><span class="line">sum: 2.88059749371</span><br><span class="line">sum_row: [ 1.4167441  1.4638534]</span><br><span class="line">sum_col: [ 1.36359656  0.77836361  0.73863732]</span><br><span class="line">min: 0.0592735446598</span><br><span class="line">max: 0.708595838193</span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">14</span>,<span class="number">2</span>,<span class="number">-1</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(A)</span><br><span class="line">print(np.argmin(A))<span class="comment">#最小值下标</span></span><br><span class="line">print(np.argmax(A))</span><br><span class="line">print(np.mean(A)) <span class="comment">#平均值</span></span><br><span class="line">print(np.mean(A,axis=<span class="number">0</span>)) <span class="comment">#列的平均值</span></span><br><span class="line">print(np.average(A))</span><br><span class="line">print(np.median(A))<span class="comment">#中位数</span></span><br><span class="line">print(np.cumsum(A))<span class="comment">#累加</span></span><br><span class="line">print(np.diff(A))<span class="comment">#累差</span></span><br><span class="line">print(np.nonzero(A))<span class="comment">#非零数（两个数组，行列坐标）</span></span><br><span class="line">print(np.sort(A))<span class="comment">#逐行排列</span></span><br><span class="line">print(np.transpose(A))<span class="comment">#转置</span></span><br><span class="line">print(A.T)<span class="comment">#转置</span></span><br><span class="line">print((A.T).dot(A))<span class="comment">#矩阵相乘</span></span><br><span class="line">print(np.clip(A,<span class="number">5</span>,<span class="number">9</span>))<span class="comment">#截取</span></span><br><span class="line">print(np.exp(A)) <span class="comment">#指数</span></span><br><span class="line">print(np.log(A))<span class="comment">#对数</span></span><br><span class="line"></span><br><span class="line">B=np.array([[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">3</span>]]) </span><br><span class="line">print(-B)</span><br><span class="line">C=np.mat(B)<span class="comment">#矩阵化，调用mat()函数可以将数组转化为矩阵</span></span><br><span class="line">print(np.mat(B).I)<span class="comment">#求逆</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[[14 13 12 11]</span><br><span class="line"> [10  9  8  7]</span><br><span class="line"> [ 6  5  4  3]]</span><br><span class="line">11</span><br><span class="line">0</span><br><span class="line">8.5</span><br><span class="line">[ 10.   9.   8.   7.]</span><br><span class="line">8.5</span><br><span class="line">8.5</span><br><span class="line">[ 14  27  39  50  60  69  77  84  90  95  99 102]</span><br><span class="line">[[-1 -1 -1]</span><br><span class="line"> [-1 -1 -1]</span><br><span class="line"> [-1 -1 -1]]</span><br><span class="line">(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int64), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=int64))</span><br><span class="line">[[11 12 13 14]</span><br><span class="line"> [ 7  8  9 10]</span><br><span class="line"> [ 3  4  5  6]]</span><br><span class="line">[[14 10  6]</span><br><span class="line"> [13  9  5]</span><br><span class="line"> [12  8  4]</span><br><span class="line"> [11  7  3]]</span><br><span class="line">[[14 10  6]</span><br><span class="line"> [13  9  5]</span><br><span class="line"> [12  8  4]</span><br><span class="line"> [11  7  3]]</span><br><span class="line">[[332 302 272 242]</span><br><span class="line"> [302 275 248 221]</span><br><span class="line"> [272 248 224 200]</span><br><span class="line"> [242 221 200 179]]</span><br><span class="line">[[9 9 9 9]</span><br><span class="line"> [9 9 8 7]</span><br><span class="line"> [6 5 5 5]]</span><br><span class="line">[[  1.20260428e+06   4.42413392e+05   1.62754791e+05   5.98741417e+04]</span><br><span class="line"> [  2.20264658e+04   8.10308393e+03   2.98095799e+03   1.09663316e+03]</span><br><span class="line"> [  4.03428793e+02   1.48413159e+02   5.45981500e+01   2.00855369e+01]]</span><br><span class="line">[[ 2.63905733  2.56494936  2.48490665  2.39789527]</span><br><span class="line"> [ 2.30258509  2.19722458  2.07944154  1.94591015]</span><br><span class="line"> [ 1.79175947  1.60943791  1.38629436  1.09861229]]</span><br><span class="line">[[-2  0]</span><br><span class="line"> [ 0 -3]]</span><br><span class="line">[[ 0.5         0.        ]</span><br><span class="line"> [ 0.          0.33333333]]</span><br></pre></td></tr></table></figure><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">D = np.arange(<span class="number">3</span>,<span class="number">15</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(D)</span><br><span class="line">print(D[<span class="number">2</span>])<span class="comment">#第三行</span></span><br><span class="line">print(D[<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line">print(D[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">print(D[<span class="number">2</span>,:])<span class="comment">#冒号代表所有数，第3行(下标由0开始)</span></span><br><span class="line">print(D[:,<span class="number">1</span>])<span class="comment">#第2列</span></span><br><span class="line">print(D[<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>])<span class="comment">#第二行的小标[1,3)范围的数，不包括3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#for</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> D:<span class="comment">#迭代行</span></span><br><span class="line">print(row)</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> D.T:<span class="comment">#迭代列</span></span><br><span class="line">print(col)</span><br><span class="line"></span><br><span class="line">print(D.flatten())<span class="comment">#矩阵变成一维（拉直）</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> D.flat:</span><br><span class="line">print(item)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[[ 3  4  5  6]</span><br><span class="line"> [ 7  8  9 10]</span><br><span class="line"> [11 12 13 14]]</span><br><span class="line">[11 12 13 14]</span><br><span class="line">8</span><br><span class="line">8</span><br><span class="line">[11 12 13 14]</span><br><span class="line">[ 4  8 12]</span><br><span class="line">[8 9]</span><br><span class="line">[3 4 5 6]</span><br><span class="line">[ 7  8  9 10]</span><br><span class="line">[11 12 13 14]</span><br><span class="line">[ 3  7 11]</span><br><span class="line">[ 4  8 12]</span><br><span class="line">[ 5  9 13]</span><br><span class="line">[ 6 10 14]</span><br><span class="line">[ 3  4  5  6  7  8  9 10 11 12 13 14]</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td></tr></table></figure><h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#合并</span></span><br><span class="line">E = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])<span class="comment">#E = np.array([1,1,1])[:,np.newaxis]</span></span><br><span class="line">F = np.array([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">print(np.vstack((E,F)))<span class="comment">#上下合并，vertical stack</span></span><br><span class="line">print(np.hstack((E,F)))<span class="comment">#左右合并，horizontal stack</span></span><br><span class="line"></span><br><span class="line">G = np.vstack((E,F))</span><br><span class="line">print(G.shape)</span><br><span class="line">print(G.flatten())</span><br><span class="line"></span><br><span class="line">print(E[np.newaxis,:])<span class="comment">#多加1维度</span></span><br><span class="line">print(E.shape,E[np.newaxis,:].shape)</span><br><span class="line">print(E[:,np.newaxis])</span><br><span class="line">print(E.shape,E[:,np.newaxis].shape)</span><br><span class="line"></span><br><span class="line">E = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])[:,np.newaxis]</span><br><span class="line">F = np.array([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])[:,np.newaxis]</span><br><span class="line">H = np.concatenate((E,F,F,E),axis=<span class="number">1</span>)<span class="comment">#1是横向合并，0是纵向合并</span></span><br><span class="line">print(E)</span><br><span class="line">print(F)</span><br><span class="line">print(H)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[[1 1 1]</span><br><span class="line"> [2 2 2]]</span><br><span class="line">[1 1 1 2 2 2]</span><br><span class="line">(2, 3)</span><br><span class="line">[1 1 1 2 2 2]</span><br><span class="line">[[1 1 1]]</span><br><span class="line">(3,) (1, 3)</span><br><span class="line">[[1]</span><br><span class="line"> [1]</span><br><span class="line"> [1]]</span><br><span class="line">(3,) (3, 1)</span><br><span class="line">[[1]</span><br><span class="line"> [1]</span><br><span class="line"> [1]]</span><br><span class="line">[[2]</span><br><span class="line"> [2]</span><br><span class="line"> [2]]</span><br><span class="line">[[1 2 2 1]</span><br><span class="line"> [1 2 2 1]</span><br><span class="line"> [1 2 2 1]]</span><br></pre></td></tr></table></figure><h4 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment">#分割</span></span><br><span class="line">A = np.arange(<span class="number">12</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(A)</span><br><span class="line">print(np.split(A,<span class="number">3</span>,axis=<span class="number">0</span>))<span class="comment">#横向分割成3份</span></span><br><span class="line">print(np.split(A,<span class="number">2</span>,axis=<span class="number">1</span>))<span class="comment">#纵向分割成2份</span></span><br><span class="line"><span class="comment">#不等量分割</span></span><br><span class="line">print(np.array_split(A,<span class="number">3</span>,axis=<span class="number">1</span>))<span class="comment">#纵向分割成3份</span></span><br><span class="line"></span><br><span class="line">print(np.vsplit(A,<span class="number">3</span>))<span class="comment">#横向分割成3份</span></span><br><span class="line">print(np.hsplit(A,<span class="number">2</span>))<span class="comment">#纵向分割成2份</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[[ 0  1  2  3]</span><br><span class="line"> [ 4  5  6  7]</span><br><span class="line"> [ 8  9 10 11]]</span><br><span class="line">[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</span><br><span class="line">[array([[0, 1],</span><br><span class="line">       [4, 5],</span><br><span class="line">       [8, 9]]), array([[ 2,  3],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [10, 11]])]</span><br><span class="line">[array([[0, 1],</span><br><span class="line">       [4, 5],</span><br><span class="line">       [8, 9]]), array([[ 2],</span><br><span class="line">       [ 6],</span><br><span class="line">       [10]]), array([[ 3],</span><br><span class="line">       [ 7],</span><br><span class="line">       [11]])]</span><br><span class="line">[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</span><br><span class="line">[array([[0, 1],</span><br><span class="line">       [4, 5],</span><br><span class="line">       [8, 9]]), array([[ 2,  3],</span><br><span class="line">       [ 6,  7],</span><br><span class="line">       [10, 11]])]</span><br></pre></td></tr></table></figure><h4 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#赋值</span></span><br><span class="line">a = np.arange(<span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line">b=a</span><br><span class="line">c=a</span><br><span class="line">d=b</span><br><span class="line">a[<span class="number">0</span>]=<span class="number">11</span><span class="comment">#只要a变，其他关联变量也变</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(b <span class="keyword">is</span> a)</span><br><span class="line">print(c)</span><br><span class="line">print(c <span class="keyword">is</span> a)</span><br><span class="line">print(d)</span><br><span class="line">print(d <span class="keyword">is</span> a)</span><br><span class="line"></span><br><span class="line">b = a.copy()<span class="comment">#a和b不关联了</span></span><br><span class="line">print(b <span class="keyword">is</span> a)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[0 1 2 3]</span><br><span class="line">[11  1  2  3]</span><br><span class="line">[11  1  2  3]</span><br><span class="line">True</span><br><span class="line">[11  1  2  3]</span><br><span class="line">True</span><br><span class="line">[11  1  2  3]</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;numpy-属性&quot;&gt;&lt;a href=&quot;#numpy-属性&quot; class=&quot;headerlink&quot; title=&quot;numpy 属性&quot;&gt;&lt;/a&gt;numpy 属性&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td c
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>struct-list</title>
    <link href="http://yoursite.com/2018/12/27/struct-list/"/>
    <id>http://yoursite.com/2018/12/27/struct-list/</id>
    <published>2018-12-27T03:09:17.000Z</published>
    <updated>2019-03-18T01:55:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据结构-顺序表"><a href="#数据结构-顺序表" class="headerlink" title="数据结构 顺序表"></a>数据结构 顺序表</h1><h4 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> SQLIST_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SQLIST_H</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> ElemType;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 线性表的顺序表的动态分配类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">ElemType *data;</span><br><span class="line"><span class="keyword">int</span> MaxSize,length;</span><br><span class="line">&#125;SqList;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListInsert</span><span class="params">(SqList &amp;L, <span class="keyword">int</span> position, ElemType e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListDelete</span><span class="params">(SqList &amp;L,<span class="keyword">int</span> position, ElemType &amp;e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ListSearch</span><span class="params">(SqList &amp;L, ElemType e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListUpdate</span><span class="params">(SqList &amp;L,<span class="keyword">int</span> position, ElemType e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ListShow</span><span class="params">(SqList &amp;L)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ListReverse</span><span class="params">(SqList &amp;L)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Serch_Min</span><span class="params">(SqList &amp;L,ElemType &amp;e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Min</span><span class="params">(SqList &amp;L, ElemType &amp;e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Same</span><span class="params">(SqList &amp;L, ElemType e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Range_order</span><span class="params">(SqList &amp;L, ElemType start, ElemType end)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Range_disorder</span><span class="params">(SqList &amp;L, ElemType start, ElemType end)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getLength</span><span class="params">(SqList &amp;L)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sqlist.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> InitSize 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化List</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//L.data = (ElemType*)malloc(sizeof(ElemType)*InitSize);</span></span><br><span class="line">L.data = <span class="keyword">new</span> ElemType[InitSize];</span><br><span class="line"><span class="keyword">if</span>(!L.data)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line">L.MaxSize = InitSize;</span><br><span class="line">L.length = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入操作(列表，插入位置，插入元素) 平均 O(n)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListInsert</span><span class="params">(SqList &amp;L, <span class="keyword">int</span> position, ElemType e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (position&lt;<span class="number">1</span> || position&gt;L.length+<span class="number">1</span>)<span class="comment">//可插位置有n+1个</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span> (L.length == L.MaxSize)</span><br><span class="line"> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = L.length; i &gt;= position; i--)<span class="comment">//往后移位</span></span><br><span class="line">L.data[i] = L.data[i<span class="number">-1</span>];</span><br><span class="line"></span><br><span class="line">L.data[position<span class="number">-1</span>] = e;<span class="comment">//找到合适位置插入元素</span></span><br><span class="line">L.length++;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除操作(列表，删除位置，返回被删元素) 平均 O(n)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListDelete</span><span class="params">(SqList &amp;L,<span class="keyword">int</span> position, ElemType &amp;e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (position&lt;<span class="number">1</span> || position&gt;L.length) <span class="comment">//可删位置有n个</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">e = L.data[position<span class="number">-1</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = position; i &lt; L.length; i++)<span class="comment">//往前移位</span></span><br><span class="line">L.data[i<span class="number">-1</span>] = L.data[i]; </span><br><span class="line">L.length--;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查找操作(列表，待查找元素) 平均 O(n)</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ListSearch</span><span class="params">(SqList &amp;L, ElemType e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> position;</span><br><span class="line"><span class="keyword">for</span> (position = <span class="number">0</span>; position &lt; L.length; ++position)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (L.data[position] == e)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> position+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//更新操作(列表，位置，待更新元素) 平均 O(1)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListUpdate</span><span class="params">(SqList &amp;L,<span class="keyword">int</span> position, ElemType e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (position&lt;<span class="number">1</span> || position&gt;L.length) <span class="comment">//可更新位置有n个</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">L.data[position<span class="number">-1</span>] = e; </span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//显示操作(列表) 平均 O(n)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ListShow</span><span class="params">(SqList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"no List\n"</span>);</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; L.length; i++)<span class="comment">//往前移位</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d "</span>, L.data[i]);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//返回List长度</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getLength</span><span class="params">(SqList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> L.length;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//元素顺序逆置</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ListReverse</span><span class="params">(SqList &amp;L)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ElemType tmp;</span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"ListReverse no List\n"</span>);</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; L.length/<span class="number">2</span>; i++)  <span class="comment">//往前移位</span></span><br><span class="line">        &#123;</span><br><span class="line">            tmp  = L.data[i];  <span class="comment">//第一与最后的交换，第二与倒数第二交换，以此类推</span></span><br><span class="line">            L.data[i] = L.data[L.length<span class="number">-1</span>-i];</span><br><span class="line">            L.data[L.length<span class="number">-1</span>-i] = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查找最小值(list,返回最小值) 函数返回位置</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Serch_Min</span><span class="params">(SqList &amp;L,ElemType &amp;e)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    <span class="keyword">int</span> position = <span class="number">-1</span>;</span><br><span class="line">    e = L.data[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; L.length; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (L.data[i] &lt; e)</span><br><span class="line">        &#123;</span><br><span class="line">            e = L.data[i];</span><br><span class="line">            position = i+<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> position;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除最小值，并且用最后元素替补(list,返回最小值)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Min</span><span class="params">(SqList &amp;L, ElemType &amp;e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> position = Serch_Min(L,e);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (position == <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        L.data[position<span class="number">-1</span>] = L.data[L.length<span class="number">-1</span>];</span><br><span class="line">        L.length--;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除与e相同的值 O(n)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Same</span><span class="params">(SqList &amp;L, ElemType e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;  <span class="comment">//不等于e的值个数</span></span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; L.length; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (L.data[i] != e)</span><br><span class="line">            &#123;</span><br><span class="line">                L.data[k] = L.data[i];</span><br><span class="line">                k++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(k==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d is not in List\n"</span>, e);</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            L.length = k;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除一定范围的值 [start,end](闭区间)(前提L是个有序表)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Range_order</span><span class="params">(SqList &amp;L, ElemType start, ElemType end)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> s_pos,e_pos;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (start &gt;= end)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// int s_pos = ListSearch(L,start);    //获得start的位置 //不可以这样 start未必在list中</span></span><br><span class="line">    <span class="keyword">for</span> (s_pos = <span class="number">0</span>; s_pos &lt; L.length &amp;&amp; L.data[s_pos] &lt; start ; ++s_pos);    <span class="comment">//找到 &gt;=start 第一个元素</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//printf("%d\n",s_pos);</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (s_pos &gt;= L.length)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;   <span class="comment">//所有值均小于s  </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (e_pos = s_pos ; e_pos &lt; L.length &amp;&amp; L.data[e_pos] &lt;= end ; ++e_pos);    <span class="comment">//找到 &gt;t 第一个元素</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//printf("%d\n",);</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (; e_pos &lt; L.length; s_pos++,e_pos++)</span><br><span class="line">    &#123;</span><br><span class="line">        L.data[s_pos] = L.data[e_pos];</span><br><span class="line">        <span class="comment">//printf("%d\n",L.data[s_pos]);</span></span><br><span class="line">        <span class="comment">//printf("%d\n",s_pos);</span></span><br><span class="line">        <span class="comment">//printf("%d\n",e_pos);</span></span><br><span class="line">    &#125;   </span><br><span class="line"></span><br><span class="line">    L.length = s_pos;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除一定范围的值 [start,end](闭区间)(L是个无序表)</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Delete_Range_disorder</span><span class="params">(SqList &amp;L, ElemType start, ElemType end)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;  <span class="comment">//处于范围内的数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (L.length == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (start &gt;= end)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; L.length; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (L.data[i]&gt;=start &amp;&amp; L.data[i]&lt;=end)</span><br><span class="line">            k++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            L.data[i-k] = L.data[i];    <span class="comment">//当前元素前移k个位置</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    L.length -= k;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//合并A B两个有序表，成新的C有序表</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ListMergebyorder</span><span class="params">(SqList A,SqList B,SqList &amp;C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (A.length+B.length &gt; C.length)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">int</span> i=<span class="number">0</span>,j=<span class="number">0</span>,k=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;A.length &amp;&amp; j&lt;B.length) <span class="comment">//两两比较，小的存入</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(A.data[i] &lt;= B.data[j])</span><br><span class="line">            C.data[k++] = A.data[i++];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            C.data[k++] = B.data[j++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;A.length)               <span class="comment">//还剩一个没有比较完成的顺序</span></span><br><span class="line">        C.data[k++] = A.data[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;B.length)</span><br><span class="line">        C.data[k++] = B.data[j++];</span><br><span class="line">    </span><br><span class="line">    C.length = k+l;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据结构-顺序表&quot;&gt;&lt;a href=&quot;#数据结构-顺序表&quot; class=&quot;headerlink&quot; title=&quot;数据结构 顺序表&quot;&gt;&lt;/a&gt;数据结构 顺序表&lt;/h1&gt;&lt;h4 id=&quot;头文件&quot;&gt;&lt;a href=&quot;#头文件&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
