<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>MR.C</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-04-16T09:28:04.938Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>(soloistben)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NLP</title>
    <link href="http://yoursite.com/2021/04/14/NLP/"/>
    <id>http://yoursite.com/2021/04/14/NLP/</id>
    <published>2021-04-14T09:26:06.000Z</published>
    <updated>2021-04-16T09:28:04.938Z</updated>
    
    <content type="html"><![CDATA[<h5 id="natural-language-processing">Natural Language Processing</h5><ul><li>word embedding 发展历程<ul><li>基于字典组成one-hot模型<ul><li>一方面词典的数目一般都是上万数量级的，造成对于单个词而言，向量表示过于稀疏</li><li>另一方面由于仅仅存储0、1数据，没有办法保存对应的<strong>语序信息</strong>，也就无法通过one-hot编码挖掘词与词之间的语义关系</li></ul></li><li>tf-idf<ul><li>tf是词频，也就是当前词在文档中出现的次数</li><li>idf的计算方法是log(语料库中总文档数 / 包含当前词的文档数)</li><li>tf-idf就是one-hot的一种优化</li><li>文档比较有<strong>代表性的词</strong>发挥更大的作用，规避常见词（the, I, is, ...）</li><li>无法考虑词的位置信息，上下文信息以及一些分布特征</li></ul></li><li>Encoder-Decoder（少用）<ul><li>encoder输入onehot，deocder输出</li></ul></li><li>NNLM（Nerual Network Language Model）<ul><li>基于上下文，上文预测下文</li><li>词向量只是副产物</li></ul></li><li><font color="red">word2vec</font><ul><li>Continuous Bag-of-Word CBOW 上下文来预测当前词（用得更多，bert也类似）<ul><li>计算量大（利用huffman结构，词频高的在树的浅层、词频低的在深层，减少softmax计算量）</li><li>负采样：找些错误样本，在正确样本学习时排斥错误样本</li></ul></li><li>Continuous Skip-gram 当前词来预测上下文（太难了）</li><li>缺点：没有解决一词多义的问题</li></ul></li><li>Glove (Global Vectors for Word Representation)<ul><li>类似word2vec，但是上下文的范围更广，是全局的句子单词</li><li>因为要全部文本，所以无法online training，因此用的比较少，效果并没有比word2vec太好</li><li>缺点：没有解决一词多义的问题</li></ul></li><li>Fasttext<ul><li>将单词拆成字符作为输入，并且利用n-gram（apple -&gt; app, ppl, ple），学习到一个embedding（embedding最终做个分类任务）</li><li>对于低频词生成的词向量效果会更好</li><li>缺点：没有解决一词多义的问题</li></ul></li><li>为了解决<font color="red">一词多义</font>的问题才有后面预训练模型的学习<ul><li>ELMO、BERT</li></ul></li></ul></li><li>Model<ul><li>seq-to-class<ul><li>输入所有token，输出一个class</li><li>输入所有token，对应token各输出一个class（识别句子中所有词汇的词性，word segmentation）（bert里面可以学习该两项特性，未必需要模型预先学习这些处理）</li></ul></li><li>seq-to-seq<ul><li>autoencoder+attention</li></ul></li><li>multiple sequences<ul><li>两个句子分别输入同一模型，再拼接结果</li><li>类似bert，用特定符号连接句子，作为一个输入</li></ul></li></ul></li><li>Task<ul><li>word segmentation 断词<ul><li>对于英文，word之间用空格分开</li><li>对于中文，台湾大学简陈台大 -&gt; (台湾大学)(简陈)(台大) （具体如何划分，看任务）</li></ul></li><li>coreference resolution 指代消解<ul><li>哪些词汇指向相同的东西（主语&lt;-&gt;代词）</li></ul></li><li>summarization 总结文本<ul><li>extractive summarization 提取摘要，每个句子都需要个label，模型识别哪些句子应当放入摘要（原句），作为总结整个文本</li><li>abstractive summarization 模型输出语句总结文本，seq-to-seq模型，并非输出原句，但会有共有的词汇（需要模型对输入词汇有拷贝作用，拷贝重要词汇作用于输入）</li></ul></li><li>machine translation 机器翻译<ul><li>seq-to-seq模型（输入语言，输出另外语言）（输入语音，输出文字）（输入一种语言的语音，输出另外语言的语音）</li></ul></li><li>grammar error correction 语法错误纠正<ul><li>seq-to-seq模型（修改语法）</li></ul></li><li>sentiment classification 情感分析<ul><li>seq-to-class模型（输入评论，输出正负情感）</li></ul></li><li>veracity prediction 立场预测<ul><li>seq-to-class模型（输入post文、评论回应等，输入正反立场）</li></ul></li><li>natural language inference (NLI) 自然语言推理<ul><li>seq-to-class模型（输入premise、hypothesis句子，输出三个类别:contradiction、entailment、neutral）</li><li>两个句子的关系</li></ul></li><li>search engine 搜索引擎<ul><li>seq-to-class模型（匹配问题答案的相关性，relevant）</li></ul></li><li><strong>QA问答</strong><ul><li>（Watson模型）问题处理 - 生成候选答案 - 候选答案评分 - 排序</li><li>阅读理解：输入question和knowledge source（unstructured documents，可以来源于搜索引擎），过滤掉不相关的documents， 输出答案</li></ul></li><li><strong>dialogue 对话模型</strong><ul><li>chatting 尬聊，需要输入历史对话，得到下一次的回复（模型应当加入更多特性：personality个性化、empathy同理心、knowledge知识丰富的）</li><li>task-oriented 任务导向：引导用于完成一个任务（拆分模型：natural language generation (NLG) 提前设定好可以问的问题，需要输入历史对话 -&gt; NLG -&gt; 得到下一次的回复）</li><li>natural language understanding (NLU) -&gt; state tracker -&gt; state -&gt; policy -&gt; NLG -&gt; answer</li><li>NLU: intent classification + slot filling（内容识别，提取内容）</li></ul></li><li><strong>knowledge graph 知识图谱</strong><ul><li>node 为 entity 实体， edge 为 node之间的关系</li><li><font color="red">从文字抽取实体、实体关系</font></li><li>name entity recognition (NER)<ul><li>根据任务确定实体范围，一般为人名、组织、地名、时间</li><li>seq-to-class</li><li>多个名字指向同一个东西，多个东西指向同一个名字，name entity linking</li></ul></li><li>relation extraction<ul><li>若relation是有限的，那么可以看成分类任务</li></ul></li></ul></li></ul></li><li>pretrain + fine-tune (用大量预料做预训练，再针对特定任务的数据微调模型)<ul><li>What is pretrian model?<ul><li>each token -&gt; embedding<ul><li><font color="red">(old method (<strong>word2vec and Glove</strong>): same token -&gt; same embedding, exclude context)</font></li><li>english word -&gt; token : too much words (FastText: 将单词拆成字符作为输入，学习到一个embedding)</li><li>chinese character -&gt; token (EMNLP: 将汉字看为图片，用CNN学习汉字各个部位，得到embedding)</li></ul></li><li><font color="red">Contextualized Word Emnedding </font>(基于上下文来学习单词的embedding) (学习token到一词多义)<ul><li>LSTM, self-attention, tree-base (少用，没有LSTM厉害)</li><li>self-supervised learning 自监督学习（无监督学习）：常见的语言模型，根据上文学习到embedding，该embedding还能预测下文（设计model，下文不能被偷看，此处self-attention需要加限制，mask掉下文需要预测部分）</li><li>token-level<ul><li><strong>ELMO</strong> (Embedding from Language Models) (bi-lstm)、<ul><li>语言中预测一个单词，需要看它常常跟哪些词汇在一起（上下文）</li><li>双向：基于上文预测下文，基于下文预测上文，再结合两部分embedding（按什么比例结合，看任务）</li><li>缺点：每个LSTM<strong>只看到了部分的文本</strong>，各自工作</li></ul></li><li><strong>GPT</strong> (Generative Pre-Training)（从上文预测下文，Left-to-Right）不希望fine-tune，直接pretrain解决问题<ul><li>Transformer decoder部分</li><li>GPT3</li><li>in-context learning 纯文本输入，让模型理解文本内容<ul><li>few-shot learning： 输入：给出问题描述和少许例子，让模型读题目写答案</li><li>one-shot learning： 输入：给出问题描述和1个例子，让模型读题目写答案</li><li>zero-shot learning： 输入：给出问题描述，让模型读题目写答案</li></ul></li><li>closed book QA<ul><li>直接问问题，模型给出答案（问加法问题，直接给出答案）</li></ul></li></ul></li><li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformer) <font color="red">(限定文本长度 512)</font><ul><li>Transformer encoder部分</li><li>bert可以<strong>随机mask部分词汇</strong>，根据上下文的self-attention去预测mask的词汇，规避了ELMo的缺点（类似CBOW，求和固定的全部上下文，预测mask东西，bert的attention可以自己需要多少看多少）</li><li>mask 输入的<font color="red">whole word(整个单词)(bert模型)</font>、phrase-level(短语) &amp; entity-level(实体)(ERNIE模型)、mask 多个token (span bert模型)</li><li><strong>Transformer-XL</strong> 可以跨文本读取更长的文本<ul><li>Transformer 问题1：<strong>context fragmentation（内容碎片化）</strong>，一次输入x最长只能是512长度，若输入超过512只能且分成两段喂入模型，这样形成两个片段之间内容信息没有被使用到（也就是处于500的token没有使用到处于600位置上token的信息，反之第二片段，也没有使用到第一段的信息，这样语义上是不完整的）</li><li>XL maens extra-long</li><li>解决方法：segment-level recurrence（跨越片段获取信息）将前面训练过片段固定缓存下来，为下个片段提供信息，反向传播不会影响前面固定片段信息（长度只能达到80%RNN能学的长度，但效果好）（尽可能保存多个片段，效果会更好）</li><li>vanilla transfromer 解决方案：固定窗口大小，在x中滑动，事后面的xt能学到前面的消息，但同时也受限于固定的窗口滑动大小（长度只能达到450% vanilla transfromer能学的长度，效果好，训练速度快）</li><li>Transformer 问题1: <strong>absolute Positional Encoding （绝对路径）</strong>，每个片段内部都使用绝对路径，两个片段没有距离可言，则跨片段获取信息是不起作用的</li><li>解决方法：Relative Positional Encoding（相对路径），直接考虑input x之间的相对位置，而不是绝对位置<ul><li>在计算attention score的时候，只考虑query向量与key向量的相对位置关系，根据正弦函数生成</li><li>在当前key中<strong>拼接</strong>前片段的hidden state长度，query不变</li></ul></li></ul></li><li><strong>XLNet</strong> (内部使用Transformer-XL)<ul><li>属于自回归模型</li><li>预测当前token i的时候，xi不参与预测计算（已知xi再去预测xi，有点无意义）<ul><li>每个输入都有content stream和query stream，前者带有输入信息，后者不带有输入信息（训练得到）（两者类似encode和decode）</li><li>每层都更新content stream和query stream，query stream用于预测当前词的时候作为输入</li><li>微调的时候不需要query stream，content stream已经学到信息了</li></ul></li><li>输入序列是正常顺序，拆分词的时候，进行打乱顺序（解决双向问题）</li><li>“语言模型中，上下文乱序也能学出mask的token”</li></ul></li><li>Reformer, Longformer 解决self-attention计算量的问题</li><li>bert缺点：假如必须上文预测下文，则bert效果不太好，但若是不需要按顺序预测，估计bert也ok</li></ul></li><li>seq2seq 的pretrain<ul><li>autoencoder+attention，破坏（mask、多个mask、删除、乱序）的输入部分，去预测正确的输入（BART/MASS模型）</li><li><strong>BART/MASS</strong>：综合bert和gpt，前面部分使用bert双向attention，后面部分使用单向的attention</li><li>UniLM：综合bert、gpt、BART/MASS三个模型</li></ul></li><li>ERNIE (Enhanced Representation through Knowledge Integration)<ul><li>为中文设计，mask盖住phrase-level(短语) &amp; entity-level(实体)</li><li>加入图谱</li></ul></li><li>Grover (Generating aRticles by Only Viewing mEtadata Records)</li><li>BERT &amp; PALs (Projected Attention Layers)</li><li>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)<ul><li>置换token，语法没错，之改变语义</li><li>不做embedding，对每个token做yes/no二分类，检查句子所有token是否有问题</li><li>先用个small bert的预测一个token，填入原句，再利用model进行判断是否有问题</li></ul></li></ul></li><li>sentence-level，为整个句子进行embedding<ul><li>skip thought (encode上句，decode下句) : 根据上一句生产下一句的embedding，相邻句子，embdding类似</li><li>qiuck thought (分别encode上句、下句) : 避开做生成seq（计算量很大），相近句子，embdding越相似</li><li>bert中，两个句子学习NSP (next stence prediction)，[CLS]做分类（效果不好，检测是否相关还ok，顺序就不太行）</li><li>使用SOP (sentence order prediction) 两句话颠倒（顺序很重要），则要输出错误</li><li>结合NSP和SOP会更好</li></ul></li></ul></li></ul></li><li>How to fine-tune?<ul><li>句子分割符号：seq1 [SEP] seq2</li><li>句首符号：[CLS] seq<ul><li>one class<ul><li>[CLS]可以代表整个句子的embedding，可以用[CLS]的embedding做分类</li><li>用所有embedding求平均或者RNN综合输出一个embedding做分类</li></ul></li><li>class for each token<ul><li>将所有token输入一个分类器做多分类</li></ul></li><li>copy from input<ul><li>Extracetion-base QA: 输入Document、Query，输出文本两个下标，代表答案范围</li><li>输入[CLS] query [SEP] Document，得到document所有token的输出，分别用两个向量，在token中做dot-product or LSTM再接softmax，选择最高成绩，分别作为起始下标和终止下标</li></ul></li><li>seq2seq<ul><li>传统做法为，encoder-decoder + attention，存在decoder中的token没有在encoder见识过（没有pretrain过）</li><li>输入seq1 [SEP] seq2，输出根据[SEP]开始，预测seq2第一个的token，最后个token预测结束字符</li></ul></li></ul></li><li>fine-tune方法<ul><li><strong>pretrain作为特征提取，固定不变，再接入下游任务</strong> （更优，pretrain模型很大）</li><li>pretrain 和 下游任务一起训练 （pretrain有预训练参数不是随机参数，所以也不会直接过拟合）</li><li>折中做法：Adaptor模型，fine-tune时候，pretrain中加入adapter层（self-attention后，前馈神经网络后面），adaptor层与下游任务一起训练，pretrian部分不变</li></ul></li><li>有实验证明 基于pretrain的模型，loss会下降很快，也就是可以快速训练，范化能力更强</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;word embedding 发展历程
&lt;ul&gt;
&lt;li&gt;基于字典组成one-hot模型
&lt;ul&gt;
&lt;li&gt;一方面词典的数
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Interview</title>
    <link href="http://yoursite.com/2021/04/09/Interview/"/>
    <id>http://yoursite.com/2021/04/09/Interview/</id>
    <published>2021-04-09T09:34:54.000Z</published>
    <updated>2021-04-17T03:14:22.106Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>简历项目<ul><li>自适应图聚类<ul><li>GCN细节<ul><li>2016年诞生</li><li>针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片）</li><li><font color="red">基于拓扑结构与节点属性特征，卷积过程：传播节点信息，汇聚邻居节点信息来更新自身节点。</font>（计算公式上，基于邻接矩阵求得拉普拉斯矩阵，再与特征矩阵做内积，意义在于从谱域傅里叶转换到频域做乘积计算，再逆傅里叶转换回谱域，实现卷积的过程）</li><li>CNN的卷积操作属于GCN的一种特殊情况（3x3九宫格中，中心节点汇聚周围8个节点的信息）</li><li>本质实在数据中做<strong>低通滤波</strong>的作用，过滤高频噪声信息，提取出低频信息（类似于CNN人脸识别的时候获得人脸五官的基础特征信息，叠加深层之后，获得整个人脸特征）</li></ul></li><li>用什么方法解决了什么问题？结果如何？<ul><li>问题：图神经网的浅层、深层会出现过平滑问题、所有节点卷积层是固定的</li><li>将RNN中自适应机制转移到GNN中，实现所有节点可以自适应控制自己的卷积层数，有效缓解过平滑问题，学习到更好的embedding，效果在不同数据有3%～6%的提升。</li><li>基于聚类两大特性设计损失函数（簇内的距离应当越小，簇间的距离应当越大），也属于自监督学习（无监督学习）</li></ul></li><li>应用场景<ul><li>基于拓扑结构和特征信息的聚类操作、推荐系统、知识图谱、蛋白质相互作用网络、社交网络、风控网络</li></ul></li></ul></li><li>病毒宿主预测任务<ul><li>问题：分析病毒基因数据，预测宿主</li><li>针对基因学习序列信息，基于宿主构造拓扑结构，实现匹配网络</li></ul></li><li>卷积循环神经网络<ul><li>问题：提取过去时序图片信息、预测未来某一时刻图片信息</li><li>CNN+LSTM</li></ul></li></ul></li><li>机器学习、深度学习<ul><li><p><font color="red">SVM原理</font></p><ul><li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）max margin(w, b) s.t. yi (w^T xi + b) &gt; 0</li><li>最优平面：w x+b，参数需要基于样本学习</li><li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li><li>二次凸优化问题<ul><li>符合参数的偏导为0，才能转换对偶问题（max min交换）</li><li>针对线性可分数据</li></ul></li><li>soft-margin SVM在hard-margin SVM基础上允许一点点错误</li><li><font color="red">kernel函数</font>，是针对完全非线性的数据，<strong>非线性转换到高维空间，转成线性可分问题</strong>，再使用SVM。（高维空间比低维更易线性可分）（深度学习使用多层感知机解决异或问题，可以不转高维）<ul><li>kernel 函数还可以解决对偶问题（计算高维空间内积运算），直接得到内及结果，不需要先得到单个函数结果，再计算内积。</li></ul></li></ul></li><li><p>牛顿法</p></li><li><p>LSTM</p><ul><li>Long short-term memory 长短期记忆，可以解决梯度消失和梯度爆炸</li><li>对于RNN，多出cell state来记忆之前的信息，变化较慢</li><li>忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记（<a href="https://zhuanlan.zhihu.com/p/100948638" target="_blank" rel="noopener">忘记不重要的，记住重要的</a>）<ul><li>sigmoid层，选择部分忘记</li></ul></li><li>记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”<ul><li>sigmoid层，决定什么值需要更新</li><li>tanh层，创建一个新的候选值向量，生成候选记忆</li></ul></li><li>输出阶段。这个阶段将决定哪些将会被当成当前状态的输出<ul><li>sigmoid层，确定细胞状态的哪部分需要输出</li></ul></li></ul></li><li><p>DeepWalk</p><ul><li><p>思想类似word2vec，使用图中节点与节点的共现关系来学习节点的向量表示，<a href="https://leovan.me/cn/2020/04/graph-embedding-and-gnn/" target="_blank" rel="noopener">学习node embedding（在没有feature）</a></p></li><li><p>网络节点的表示中<strong>节点构成的序列</strong>就是随机游走，网络上不断重复地随机选择游走<strong>路径</strong></p></li><li><p>从某个特定的端点开始，游走的每一步都从与当前节点相连的边中随机选择一条（固定长度），沿着选定的边移动到下一个顶点，不断重复这个过程（类似NLP，当前token预测下个token）（每一个节点映射成d维向量）</p><p><img src="https://github.com/soloistben/images/raw/master/interview/deepwalk.jpg" alt="deepwalk 绿色部分即为一条随机游走" style="zoom: 80%;"></p></li><li><p>借用词向量中使用的skip-gram模型</p></li><li><p>优点：并行化（并行游走采样）、适应性（适应网络局部的变化）</p></li></ul></li><li><p>Spectral-GNN和Spatial-GNN的区别</p><ul><li>本质一样，GNN是一个热传导模型/信息扩散模型，从不同方式去汇聚信息，更新信息</li><li>Spatial-GNN<ul><li>直接推广 CNN 的<strong>加权求和思想</strong>，使用不同的领域节点采样方法和不同加权求和方法来更新节点特征</li><li>可以多种不同的采样方式，多种不同的加权求和方式</li><li>GraphSAGE 和 GAT 等都可以 Inductive learning，扩展到新的节点和新的图，因为这一类方法直接学习的采样过程和加权求和方式（GraphSAGE 还支持 mini-batch 的训练方式）</li><li>GraphSAGE：每一层的node的表示都是由上一层生成的，跟本层的其他节点无关<ul><li>聚合邻居节点方法：mean、GCN(归一化的拉普拉斯再sum)、LSTM(忽略顺序)、max-pooling</li><li>GIN理论证明sum方法较好，比较合理</li><li>与GCN比，相对灵活，迁移性强，理论上没差</li></ul></li></ul></li><li>Spectral-GNN<ul><li>从 CNN 的<strong>卷积定理</strong>，f 和 g 的卷积是 f 和 g 傅里叶变换之后乘积的傅里叶逆变换。然后通过拉普拉斯矩阵来实现傅里叶变换和逆傅里叶变换</li><li>算是基于图谱理论的Spatial-GNN的特例</li><li>优点：捕捉graph的全局信息，从而很好地表示node的特征；理论性强</li><li>缺点：<ul><li>Transductive，都是单独的图结构，不能迁移到其他图结构（GAT，可以不受图结构影响）</li><li>同阶邻居权重一样（GAT是可以不一样，attention）</li><li>所有节点一起训练，无法快速学习新的node embedding（GraphSAGE 支持 mini-batch ）</li></ul></li></ul></li><li>新增加点意味着环境变了，那之前的节点的表示自然也应该有所调整<ul><li>Inductive：<strong>对于老节点，可能新增一个节点对其影响微乎其微</strong>，所以可以暂且使用原来的embedding</li><li>Transductive：面对新节点，必须更新全部节点</li></ul></li></ul></li><li><p>有向图GNN</p><ul><li>GCN不可以使用在有向图，<strong>拉普拉斯需要对称才能正交分解</strong>，GAT可以用在有向图</li></ul></li><li><p>CNN卷积核计算、池化计算</p><ul><li>图片大小n*n*c，卷积核大小f*f*c、卷积核个数为m个，padding大小p（为了保留边缘信息），步长stride为s</li><li>padding后图片为(n+2p)*(n+2p)*c</li><li>卷积核后图片三维：<strong>((n+2p-f)/s + 1)*((n+2p-f)/s + 1)*m</strong></li><li>使用尺寸大的卷积核，计算量大，多个小尺寸的，达到一样效果，但是计算量更小</li><li>池化（缩小模型大小，提高计算速度，提高特征的robust ）计算与卷积一样（一般不加padding）</li><li>卷积（padding保持图片大小）+池化（无padding，缩小图片）=卷积层</li><li>与全连接层相比，卷积层有两大优势：<ul><li>parameter sharing<strong>（卷积核）参数共享</strong>（节省参数，在卷积操作时，卷积计算可以在不同图片区域使用相同的参数，一个过滤器适合一个图片某一块，则也会适合另一块）</li><li>sparsity connections<strong>稀疏连接</strong>（输入少，相比全连接而言，不需要输入全部参数，只要满足卷积核大的输入个数），有着两个优势也可以预防过拟合</li><li>参数计算：(f*f+1)*c，1为bias，全连接参数=输入输出所有参数的乘积</li></ul></li></ul></li><li><p>解决过拟合方法</p><ul><li>dropout原理、设置原理<ul><li><p>在训练时，设置dropout使部分神经元暂时隐藏（不起作用），<font color="red"> 减少特征（隐层节点）间的相互作用，缓解过拟合（类似正则化）</font>，梯度下降仅更新未隐藏的神经元。（再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ）</p><p><img src="https://github.com/soloistben/images/raw/master/interview/dropout.png" alt="dropout" style="zoom:80%;"></p></li><li><p>在过拟合严重处可降低dropout概率，不担心过拟合处可提高dropout概率</p></li><li><p>不在输入层使用dropout</p></li><li><p>一般认为设置为0.5或者0.3（Dropout是一个超参，需要根据具体的网络、具体的应用领域进行尝试）</p></li><li><p>在测试时不需要</p></li><li><p>除非出现过拟合状态，否则不用dropout，在计算机视觉常用dropout，在别的领域少用</p></li></ul></li><li>损失函数加入权重正则化一起训练（正则化是损失函数的惩罚项，对某些参数做一些限制）<ul><li><font color="red"> L1, L2的正则化</font><ul><li>L1（平均绝对误差，MAE，mean average error）：目标值与预测值的绝对误差值的总和<ul><li>鲁棒性、不稳定性、可能有多个解（在非稀疏向量上的计算效率就很低）</li><li>符合拉普拉斯分布，是不完全可微的，在图像上会有很多角出现（造成最优值出现在坐标轴上，<strong>因此就会导致某一维的权重为0</strong> ，产生<strong>稀疏权重矩阵</strong>，进而防止过拟合）</li><li>L1正则化是指权值向量中各个元素绝对值之和</li><li><strong>优点</strong>：输出具有稀疏性，<strong>即产生一个稀疏模型，进而可以用于特征选择</strong>（会趋向于产生少量的特征，而其他的特征都是0）；一定程度上，L1也可以防止过拟合</li><li><strong>缺点</strong>：但在非稀疏情况下计算效率低，存在多个解</li><li><font color="red">如果特征符合稀疏性，说明特征矩阵很多元素为0，只有少数元素是非零的矩阵，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响）</font>，此时就可以只关注系数是非零值的特征。</li></ul></li><li>L2（均方误差，MSE，mean squared error）：目标值与预测值的差值均方和<ul><li>不是很鲁棒性、稳定、一个解（计算方便）</li><li>高斯分布，是完全可微的，图像上的棱角比较圆滑（参数<strong>不断趋向于0</strong>）</li><li><font color="red">如果误差大于1，则误差会放大更多（比L1更大），因此模型会对样本更加敏感</font>（若有一个异常样本，模型需要调整or牺牲很多正常样本，来适应单个异常值，正常的样本的误差比这单个的异常值的误差小）</li><li>L2正则化是指权值向量中各个元素的平方和然后再求平方根</li><li><strong>优点</strong>：计算效率高（因为存在解析解）；可以防止模型过拟合；选择更多的特征，这些特征都会接近于0</li><li><strong>缺点</strong>：非稀疏输出；无特征选择</li></ul></li></ul></li></ul></li><li>L1与L2的区别只在于，L2是权重的平方和，而L1就是权重的和</li><li>权重过大会引起梯度爆炸</li><li>降低模型复杂度、减轻过拟合</li><li>数据增强</li><li>提早结束，在出现过拟合之前</li></ul></li><li><p>bert预训练与word2vector比较</p><ul><li>BERT使用Transformer中encoder部分的self-attention、Mask Language Model、Next Sentence Prediction<ul><li>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示（自监督学习属于无监督学习）</li><li><font color="red">Mask Language Model</font>：在训练时在输入中随机mask 15%单词，然后根据上下文预测单词（完型填空）（80%直接mask、10%更换任意单词、10%保留原单词）（若100%mask掉，有可能在微调模型时候，没有见过某些单词）</li><li>Next Sentence Prediction：判断第二句话是否第一句话的下文，50%的IsNext和50%的NotNext（选择题（多分类）、判断题（二分类）、简答题（回归））</li></ul></li><li>word2vector<ul><li>Skip-gram：如果是用一个词语作为输入，来预测它周围的上下文</li><li>CBOW：如果是拿一个词语的上下文作为输入，来预测这个词语本身</li><li>one-hot encode输入单词，输入一个神经网络（隐含层未使用激活函数，线性的），得到的embedding则是整个word2vector模型，同时加入整个模型的训练（质上是一种<font color="red">降维操作</font>，词袋数量很大时候，onehot维度很大，则用神经网络降维到固定的维度）</li><li>训练trick: hierarchical softmax (把 N 分类问题变成 log(N)次二分类) 和 negative sampling(预测总体类别的一个子集)</li><li>word2vector 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练</li></ul></li></ul></li><li><p>多分类问题</p><ul><li>拆成多个二分类<ul><li>例如三分类拆成一个为一类，两个看成另一类，但会出现样本不均衡问题</li><li>四分类拆成一对一对，分别将两个看成一类</li></ul></li></ul></li><li><p>激活函数</p><ul><li>sigmoid、tanh会引起梯度消失，relu不会</li><li>relu在x=0处不可导，但实际情况很少会出现靠近0的数，则可以忽略</li><li>线性函数作为激活函数一般是在输出时（全连接层）</li><li>实际上大多数现象呈现关系都是<strong>正相关</strong>关系，并非线性关系，因此用非线性函数作为激活函数是更适合表达正相关关</li></ul></li><li><p>模型调参（如何让模型更加鲁棒性）</p><ul><li>数据层面上<ul><li>training时候保证数据是打乱顺序，防止学习到输入的顺序信息，会使模型更加鲁棒性</li><li>数据增强（增加数据多样性）</li><li>数据采用，尽可能数据平衡，训练集验证集测试集分布要一致</li><li>数据不平衡的时候，在训练期间应用类别加权操作。<a href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html" target="_blank" rel="noopener">给稀少的类更多的权重，给主要类更少的权重；sklearn计算类权重</a>，<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis" target="_blank" rel="noopener">或者尝试使用过采样和欠采样技术重新采样你的训练集</a></li></ul></li><li>模型过拟合层面<ul><li>针对模型做一些正则化操作（L1、L2、Dropout）</li><li>早停机制</li></ul></li><li>训练层面<ul><li>选择正确的优化器（关心快速收敛，使用自适应优化器，如Adam，但它可能会陷入局部最小；SGD+momentum可以实现找到全局最小值，但它依赖于鲁棒初始化，而且可能比其他自适应优化器需要更长的时间来收敛）<ul><li>Adam和SGD，优化方法<ul><li>优化算法的功能，是通过改善训练方式，来最小化(或最大化)损失函数E(x)</li></ul></li><li><strong>SGD</strong>梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数<ul><li>momentum通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练，使网络能更优和更稳定的收敛；减少振荡过程（通常设定为0.9）</li><li>当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，也减少了振荡过程。</li><li><strong>Adam</strong> （Adaptive Moment Estimation）<ul><li>计算每个参数的自适应学习率</li><li>在稀疏数据集使用，效果好</li></ul></li><li>在数据统计特性明显，分布好计算（估计）的时候，容易调整SGD的参数，使得SGD收敛好（设置好参数，<font color="red">可以达到全局最优</font>）</li><li>在数据统计特性不好，变化大，误差曲面复杂的时候，优先使用傻瓜算法Adam（简化了调参，但默认参数，<font color="red">可能会导致不收敛、局部最优</font>）</li></ul></li></ul></li><li>调整学习率（0.1，0.001，0.000001，以10为阶数进行尝试），微调：0.001；完整训练：&gt;=0.001。使用衰减学习率。</li></ul></li></ul></li><li><p>自回归模型</p><ul><li>是统计上处理时间序列的方法，假设{x1, x2, ..., xt-1, xt} 存在一维线性关系，有依赖关系，需要用xt-1去预测xt，而不是x预测y 。</li><li>预训练模型往往针对自回归语言生成模型设计，自回归每次会使用已生成的序列作为已知信息预测未来的一个单词，最终再把每个时间步生成的单词拼成一个完整的序列输出</li><li>优点：<ul><li>所需数据不多，可用x自身变数数列来进行预测；</li><li>非自回归和半非自回归的依赖关系学习和生成难度较大，它们的生成质量往往弱于自回归模型</li></ul></li><li>缺点：<ul><li>随着数据量增大，自回归模型推断耗时也随之变大</li><li>非自回归模型学习时间更快，每个单词之间没有依赖关系，整个输出序列的每个单词被并行地同步预测（质量则更差）</li><li>只能上文预测下文，无法双向</li></ul></li></ul></li></ul></li><li>python<ul><li>深拷贝原理<ul><li><p>直接赋值：其实就是对象的引用（别名）</p></li><li><p>浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象</p></li><li><p>深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象</p><p><img src="https://github.com/soloistben/images/raw/master/interview/deepcopy.png" alt="deepcopy" style="zoom: 80%;"></p></li></ul></li><li>垃圾回收机制<ul><li>主要通过<strong>引用计数（Reference Counting）</strong>进行垃圾回收<ul><li>每一个对象的核心就是一个结构体（内部有一个引用计数器）</li><li>引用计数+1：创建、引用、作为参数传入到函数、作为元素存储在容器（list、set等）</li><li>引用计数-1： del销毁对象、对象被赋予新对象、对象离开作用域、容器被销毁or容器删除该元素</li><li>引用计数法有其明显的优点，如高效、实现逻辑简单、具备实时性，<font color="red">一旦一个对象的引用计数归零，内存就直接释放了 </font> （缺点是需要单独分配空间来维护引用计数、当释放一个较大的对象时需要较长时间、循环引用是该机制必然存在的，需要算法对其补充）</li><li>只有容器对象才会产生<strong>循环引用</strong>的情况，比如列表、字典、用户自定义类的对象、元组等<ul><li>“标记-清除”(Mark and Sweep)算法（双端链表，一个链表存放着需要被扫描的容器对象，另一个链表存放着临时不可达对象）：<ul><li>标记阶段，遍历所有的对象，是否还有<strong>对象引用它</strong>，那么就标记该对象为可达</li><li>清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。</li><li>垃圾回收的阶段，会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行</li></ul></li><li>分代回收(Generational Collection)<ul><li>上面算法需要暂停应用，分代回收以空间换时间，减少暂停时间</li></ul></li></ul></li></ul></li></ul></li><li>多线程<ul><li><font color="red">多线程类似于同时执行多个不同程序</font></li><li>优点<ul><li>占据长时间的程序中的任务放到后台去处理</li><li>程序的运行速度可能加快</li><li>在一些等待的任务使用多线程，释放一些珍贵的资源占用</li></ul></li><li>线程在执行过程中与进程还是有区别的。每个独立的进程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。</li><li>每个线程都有他自己的一组CPU寄存器，称为线程的上下文，该上下文反映了线程上次运行该线程的CPU寄存器的状态</li><li>指令指针和堆栈指针寄存器是线程上下文中两个最重要的寄存器，线程总是在进程得到上下文中运行的，这些地址都用于标志拥有线程的进程地址空间中的内存。<ul><li>线程可以被抢占（中断）</li><li>在其他线程正在运行时，线程可以暂时搁置（也称为睡眠） -- 这就是线程的退让</li></ul></li><li>线程可以分为:<ul><li><strong>内核线程：</strong>由操作系统内核创建和撤销</li><li><strong>用户线程：</strong>不需要内核支持而在用户程序中实现的线程</li></ul></li><li><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017627212385376" target="_blank" rel="noopener">进程与线程</a><ul><li>每个进程至少要干一件事，一个进程至少有一个线程（线程是最小的执行单元），复杂程序有多个线程（比如Word，它可以多个线程同时进行打字、拼写检查、打印等事情）</li><li>操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样</li><li>多任务三种模式：多进程、多线程、多进程+多线程（复杂、少用）</li><li>涉及到同步、数据共享的问题</li></ul></li></ul></li></ul></li><li>基础知识点（数据结构）<ul><li>数组、链表<ul><li>数组元素存储地址连续（若频繁访问某个下标的元素，选择数组方式，时间复杂度O(1)）</li><li>链表元素存储地址不连续（若频繁访问某个下标的元素，需要从头访问，时间复杂度O(n)）</li></ul></li><li><font color="red">快速排序原理</font><ul><li>选择第一个元素的值做准基数</li><li>在右边找到较小的值放到左边、在左边找到较大值放到右边，达到左侧元素小于准基数，右侧大于准基数</li><li>每次根据选择后点的位置，划分成两部分，再分别进行上诉操作</li><li>达到小到大的排序</li></ul></li></ul></li><li>大数据（spark，hadoop）</li></ol><p>code + quick_sort <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(arr, low, high)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> low&lt;high:</span><br><span class="line">        mid = partition(arr, low, high)<span class="comment"># 找到一个准基数下标</span></span><br><span class="line">        quick_sort(arr, low, mid<span class="number">-1</span>)<span class="comment"># 左边是小于准基数的部分，进行快排</span></span><br><span class="line">        quick_sort(arr, mid+<span class="number">1</span>, high)<span class="comment"># 右边是大于准基数的部分，进行快排</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(arr, low, high)</span>:</span></span><br><span class="line">p = arr[low]<span class="comment"># 设定当前low位置为基准</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        <span class="keyword">while</span> low&lt;high <span class="keyword">and</span> arr[high]&gt;=p:<span class="comment"># 在右边找比基准小的数</span></span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        arr[low] = arr[high]<span class="comment"># 放置左边</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">while</span> low&lt;high <span class="keyword">and</span> arr[low]&lt;=p:<span class="comment"># 在左边找到比基准大的数 </span></span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        arr[high] = arr[low]<span class="comment"># 放置右边</span></span><br><span class="line">    arr[low] = p<span class="comment"># 放置基准</span></span><br><span class="line">    <span class="keyword">return</span> low <span class="comment"># 返回基准下标</span></span><br></pre></td></tr></table></figure></p><ul><li><p>DFS (Depth First Search) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">neighbor_dict = &#123;node1:[neighbor1, ...], ...&#125;<span class="comment"># 邻居集合，数据处理成邻接表形式</span></span><br><span class="line">node_list = [node1, node2, ...]<span class="comment"># 所有节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归</span></span><br><span class="line">visited = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS_Traverse</span><span class="params">(node_list)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> node_list:<span class="comment"># 这层遍历防止非连通子图</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            DFS(G, node)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="string">'''针对node做操作'''</span></span><br><span class="line">    visited.append(node)</span><br><span class="line">    <span class="comment"># 遍历邻居</span></span><br><span class="line">    <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[node]:</span><br><span class="line">        <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            DFS(neighbor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非递归(使用栈)(直接指定某个起始节点，并非完整图的一个DFS)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS_Traverse</span><span class="params">(node, neighbor_dict)</span>:</span></span><br><span class="line">    stack = []<span class="comment"># list模仿栈</span></span><br><span class="line">visited = []</span><br><span class="line">    </span><br><span class="line">    stack.append(node)<span class="comment"># 入栈起始节点</span></span><br><span class="line">  visited.append(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(stack)&gt;<span class="number">0</span>:</span><br><span class="line">        node = stack.pop()<span class="comment"># 弹出栈顶的元素</span></span><br><span class="line">        <span class="string">'''针对node做操作'''</span></span><br><span class="line">        <span class="comment"># 遍历邻居</span></span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[node]:</span><br><span class="line">            <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                stack.append(neighbor)</span><br><span class="line">                visited.append(neighbor)</span><br></pre></td></tr></table></figure></p></li><li><p>BFS (Breadth First Search) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">neighbor_dict = &#123;node1:[neighbor1, ...], ...&#125;<span class="comment"># 邻居集合，数据处理成邻接表形式</span></span><br><span class="line">node_list = [node1, node2, ...]<span class="comment"># 所有节点</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''非递归'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BFS_Traverse</span><span class="params">(node_list)</span>:</span></span><br><span class="line">    queue = []</span><br><span class="line">visited = []</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> node_list:<span class="comment"># 这层遍历防止非连通子图</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            <span class="string">'''针对node做操作'''</span></span><br><span class="line">            queue.append(node)<span class="comment"># 入队起始节点</span></span><br><span class="line">            visited.append(node)</span><br><span class="line">            <span class="keyword">while</span> len(queue):</span><br><span class="line">                start_node = queue.pop(<span class="number">0</span>)<span class="comment"># 弹出队头的起始节点，找邻居</span></span><br><span class="line">                <span class="comment"># 遍历邻居</span></span><br><span class="line">                <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[start_node]:</span><br><span class="line">                    <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                        <span class="string">'''针对node做操作'''</span></span><br><span class="line">                        queue.append(neighbor)<span class="comment"># 入队邻居节点</span></span><br><span class="line">            visited.append(neighbor)</span><br><span class="line">                        </span><br><span class="line"><span class="string">'''BFS 求无权图某节点的最短路径'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BFS_Min_Distance</span><span class="params">(node, node_list)</span>:</span></span><br><span class="line">    queue = []</span><br><span class="line">    visited = []</span><br><span class="line">    d = &#123;node_:<span class="number">0</span> <span class="keyword">for</span> node_ <span class="keyword">in</span> node_list&#125;<span class="comment"># 初始化为0，n个的距离数组</span></span><br><span class="line">    </span><br><span class="line">    queue.append(node)<span class="comment"># 入队起始节点</span></span><br><span class="line">    visited.append(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(queue):</span><br><span class="line">        start_node = queue.pop(<span class="number">0</span>)<span class="comment"># 弹出队头的起始节点，找邻居</span></span><br><span class="line">        <span class="comment"># 遍历邻居</span></span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[start_node]:</span><br><span class="line">            <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                <span class="string">'''针对node做操作'''</span></span><br><span class="line">                d[neighbor] = d[start_node] + <span class="number">1</span> </span><br><span class="line">                queue.append(neighbor)<span class="comment"># 入队邻居节点</span></span><br><span class="line">                visited.append(neighbor)</span><br></pre></td></tr></table></figure></p></li><li><p>LCS (Longest Common Subsequence)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">'''最长公共子序列，（递归）暴力破解版，自顶向下'''</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LCS_rc</span><span class="params">(text1, text2)</span>:</span></span><br><span class="line">    <span class="comment"># 最基本，若其中一个为空序列，则没有公共部分</span></span><br><span class="line">    <span class="keyword">if</span> len(text1)==<span class="number">0</span> <span class="keyword">or</span> len(text2)==<span class="number">0</span>:  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> text1[<span class="number">-1</span>] == text2[<span class="number">-1</span>]:</span><br><span class="line">        <span class="keyword">return</span> LCS_rc(text1[:<span class="number">-1</span>], text2[:<span class="number">-1</span>]) + <span class="number">1</span>   <span class="comment"># 找到一个就加1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 找不到就各缩短一边来再做比较</span></span><br><span class="line">        <span class="keyword">return</span> max(LCS_rc(text1[:<span class="number">-1</span>], text2), LCS_rc(text1, text2[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">'''自底向上'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LCS</span><span class="params">(text1, text2)</span>:</span></span><br><span class="line">    <span class="comment"># 创建二维表格，多创建一行，0行0列作为全零，不变，用于后续计算</span></span><br><span class="line">    <span class="comment"># 利用循环生成list，防止出现引用问题</span></span><br><span class="line">    <span class="comment"># text1 作为纵向，text2作为横向</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*(len(text2)+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(text1)+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(text1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(text2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> text1[i<span class="number">-1</span>] == text2[j<span class="number">-1</span>]:</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span> <span class="comment"># 找到一个公共元素</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>])  <span class="comment"># 取相邻近的较大元素</span></span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]   <span class="comment"># 最后一个元素即公共子序列长度</span></span><br></pre></td></tr></table></figure></li><li><p>0/1背包（动态规划）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>   <span class="comment"># 每件物品只能装一次</span></span><br><span class="line">W = <span class="number">4</span></span><br><span class="line">wt = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  <span class="comment"># 每个物品的重量</span></span><br><span class="line">val = [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>] <span class="comment"># 每个物品的价值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dp[i][w]: 对前i个物品，当前背包容量为w，最大价值则为dp[i][w]</span></span><br><span class="line"><span class="comment"># dp[3][5]=6: 对前3个物品，当前背包容量为5时，可以装下最大价值为6</span></span><br><span class="line">dp = [[<span class="number">0</span>]*(W+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> range(<span class="number">1</span>, W+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 当前容量w</span></span><br><span class="line">        <span class="comment"># 装第i件物品：dp[i-1][w-wt[i-1]]+wt[i-1] (w-wt = 剩余背包容量)</span></span><br><span class="line">        <span class="comment"># 不装第i件物品：dp[i-1][w]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> w-wt[i<span class="number">-1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 装不下了</span></span><br><span class="line">            dp[i][w] = dp[i<span class="number">-1</span>][w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 能装下情况，选价值大的</span></span><br><span class="line">            dp[i][w] = max(dp[i<span class="number">-1</span>][w-wt[i<span class="number">-1</span>]]+val[i<span class="number">-1</span>], dp[i<span class="number">-1</span>][w])</span><br></pre></td></tr></table></figure></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;简历项目
&lt;ul&gt;
&lt;li&gt;自适应图聚类
&lt;ul&gt;
&lt;li&gt;GCN细节
&lt;ul&gt;
&lt;li&gt;2016年诞生&lt;/li&gt;
&lt;li&gt;针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片）&lt;/li&gt;
&lt;li&gt;&lt;font color=&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>C_plus_note</title>
    <link href="http://yoursite.com/2020/11/17/C-plus-note/"/>
    <id>http://yoursite.com/2020/11/17/C-plus-note/</id>
    <published>2020-11-17T08:15:24.000Z</published>
    <updated>2020-11-30T15:06:43.318Z</updated>
    
    <content type="html"><![CDATA[<h4 id="One-Definition-of-Class-without-pointer"><a href="#One-Definition-of-Class-without-pointer" class="headerlink" title="One. Definition of Class (without pointer)"></a>One. Definition of Class (without pointer)</h4><ul><li><p>防止重复引用头文件（e.g. complex.h 对应 __COMPLEX__）（guard 防卫式声明）</p></li><li><p>public（定义提供外部调用函数），private （<font color="red"><strong>数据放入private</strong></font>，仅限类内用）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li><li><p>Constructor Function 构造函数</p><ul><li>构造函数与类名称一样</li><li>Overloading 重载（一个类可以重载多个构造函数，下面特例不允许）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="comment">// (default argument 默认实参，构造函数不需要写返回类型，默认为类)</span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)      <span class="comment">// initialization list 初始化列表（仅构造函数有）</span></span><br><span class="line">    &#123;...&#125;               <span class="comment">// r, i 也可以在函数体内是assignments赋值</span></span><br><span class="line">    <span class="comment">// （但是比初始化列表慢一点）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">complex</span>() <span class="comment">// 与上面构造函数冲突，不允许这么设置</span></span><br><span class="line">        : re(<span class="number">0</span>), im(<span class="number">0</span>)</span><br><span class="line">    &#123;...&#125; </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li>Destructor Function 析构函数（不带指针的类，一般不用写析构函数）</li><li>Initialization list 初始化列表（仅构造函数有），<font color="red"><strong>优先考虑使用初始化列表</strong></font></li><li><p>构造函数一般写在public，也可以写在private中，但仅限类内调用，外部可用singleton调用，但只能调用一个 </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> A&amp; <span class="title">getInstance</span><span class="params">()</span></span>;<span class="comment">// 单例模式</span></span><br><span class="line">    setup() &#123;...&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    A();</span><br><span class="line">    A(<span class="keyword">const</span> A&amp; rhs);</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">A&amp; A::getInstance()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">static</span> A a;<span class="comment">// 只有当调用时，才会创建该对象</span></span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用</span></span><br><span class="line">A::getInstance().setup();</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Template 模板</p><ul><li><p>当未最终确定数据变量的类型时，或者需要多种数据类型，可以<strong>使用Template</strong></p></li><li><p>当设置多种类型，则当前就有上面类的定义，这是模板带来的代码膨胀（这不是缺点，是必须要两套代码）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(T r=<span class="number">0</span>, T i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">T <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function">T <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    T re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">--------------------------------</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"complex.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">complex</span>&lt;<span class="keyword">double</span>&gt; c1(<span class="number">2.0</span>, <span class="number">1.0</span>);   <span class="comment">// &lt;double&gt; 确定变量类型</span></span><br><span class="line"><span class="keyword">complex</span>&lt;<span class="keyword">int</span>&gt; c1(<span class="number">2</span>, <span class="number">1</span>);   </span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;c1.real();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>function template 函数模板</p><ul><li><p>当函数内容一样时，仅传入对象类型不一样，即可使用函数模板</p></li><li><p>编译器会对function template进行引数推导(argument deduction)（自动检测是什么类型） </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">T</span>&amp; <span class="title">min</span> (<span class="title">const</span> <span class="title">T</span>&amp; <span class="title">a</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">b</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">return</span> b&lt;a ? b:a;<span class="comment">// 即使是自定义的类，操作符 &lt; 重载就可以</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>namespace 命名空间（防止命名冲突）</p><ul><li>directive: “using namespace std;”    写demo时常用</li><li>declaration: “using std::cout;”  只声明了一个</li></ul></li><li><p>Inline Function 内联函数</p><ul><li>在类内定义的函数，且不是复杂函数，都可以编译为inline函数</li><li>在类外定义的函数，需要在<font color="red">成员函数前设定特有字符 <strong>inline</strong></font></li><li>优点是执行得快</li><li>是否变成inline，由编译器决定（创建权在程序员手上，决定权在编译器上）</li></ul></li><li><p>pass Value &amp; pass Reference 传递参数：传值与传引用</p><ul><li>直接pass value会因为值大小影响函数速度</li><li>引用本质也是指针，指针4个字节，速度快</li><li><font color="red">**提前考虑是否需要const**</font></li><li><p><font color="red"><strong>建议所有参数均传引用，return也尽量返回引用</strong></font>（在变量生命周期外（局部变量）返回引用是错误的，其余情况都可以返回引用）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;</span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>Friend Function 友元函数</p><ul><li><p>可以直接访问private数据，比函数读取private数据更快（用来做特例，破坏类的整体封装性）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// friend function</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">complex</span>&amp; __doapl (<span class="keyword">complex</span>*, <span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;<span class="comment">// 直接读取private变量</span></span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// this可以隐藏，不可以写</span></span><br><span class="line">    <span class="comment">// this是默认存在，不是临时变量</span></span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>相同class的各个object互为friend</strong>，可以直接获取private数据</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="comment">// constructor function </span></span><br><span class="line">      <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">          : re(r), im(i)</span><br><span class="line">      &#123;...&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// inline function</span></span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">func</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; param)</span></span></span><br><span class="line"><span class="function">      </span>&#123; </span><br><span class="line">          <span class="keyword">return</span> param.re+param.im; <span class="comment">// 直接读取private变量</span></span><br><span class="line">      &#125;</span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> re, im;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>operator overloading 操作符重载</p><ul><li>其实跟定义函数一样，只是操作符更直观</li><li>编译器会去寻找相关的operator的被重写的函数（不管用什么方法，只能写一个，<strong>编译器只选择其中一个使用，没有优先级</strong>）</li><li><p>两种方法：</p><ul><li><p>成员函数（类内），默认有this</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// 全局函数</span></span><br><span class="line">  <span class="comment">// 参数1会被改动，参数2不会被改动</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r) <span class="comment">// const complex&amp; r （可以用complex，传值，但会慢）</span></span><br><span class="line">  &#123;</span><br><span class="line">      ths-&gt;re += r.re;</span><br><span class="line">      ths-&gt;im += r.im;</span><br><span class="line">      <span class="keyword">return</span> *ths;    <span class="comment">// 传递者(*ths)无需知道接收者(complex&amp;)是以reference形式接收</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 类的成员函数</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)<span class="comment">// 默认有this，但不能写出来</span></span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);    <span class="comment">// 返回类型必须是引用complex&amp;（防止c3+=c2+=c1; 连续赋值情况）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>非成员函数（类外），无this；写在类外，例如加法有多个情况，全部写在类内是有局限性</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将加法所有情况，都写出来</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// temp object 临时变量（函数执行完就销毁）</span></span><br><span class="line">    <span class="comment">// 不能当成reference返回</span></span><br><span class="line">    <span class="comment">// 要返回value，才能保存生成的临时变量（局部变量）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+real(y), imag(x)+imag(y));   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+y, imag(x));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(x+real(y), imag(y));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> - (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(-real(x), -imag(x));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==real(y) &amp;&amp; imag(x)==imag(y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==y &amp;&amp; imag(x)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(y)==x &amp;&amp; imag(y)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="title">conj</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x), -imag(x)); <span class="comment">// 共轭复数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// '&lt;&lt;' 该操作符只能写成全局函数（非成员函数）</span></span><br><span class="line"><span class="comment">// os 用于显示，一直在其内容变化，不能写成const </span></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; <span class="string">'('</span> &lt;&lt; real(x) &lt;&lt; <span class="string">','</span> &lt;&lt; imag(x) &lt;&lt; <span class="string">')'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h4 id="Two-Definition-of-Class-with-pointer"><a href="#Two-Definition-of-Class-with-pointer" class="headerlink" title="Two. Definition of Class (with pointer)"></a>Two. Definition of Class (with pointer)</h4><ul><li><p>当定义类中含有指针，则必须有<font color="red">“拷贝构造”、“拷贝赋值”、“析构函数”</font></p><ul><li>拷贝构造、拷贝赋值属于深拷贝（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</li><li>若不实现深拷贝，系统默认是浅拷贝，即两个指针指向同一内存</li><li><p>析构函数则在对象离开作用域（对象定义的花括号内）后，在析构函数释放数据内存，再销毁对象</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __MY_STRING__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __MY_STRING__</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;ostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造函数</span></span><br><span class="line">    String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr=<span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*带指针的类，必须要有 拷贝构造、拷贝赋值、析构函数*/</span></span><br><span class="line">    <span class="comment">// 拷贝构造（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</span></span><br><span class="line">    <span class="comment">//（如果不重写，系统默认是浅拷贝，即两个指针指向同一内存）</span></span><br><span class="line">    String(<span class="keyword">const</span> String&amp; str);  </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拷贝赋值（深拷贝）</span></span><br><span class="line">    String&amp; <span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 析构函数（带有指针的类，需要在对象离开作用域（对象定义的花括号）后，在系够函数释放数据内存）</span></span><br><span class="line">    ~String();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">char</span>* <span class="title">get_data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> m_data;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 指针 动态分配</span></span><br><span class="line">    <span class="comment">// 数组 固定分配</span></span><br><span class="line">    <span class="keyword">char</span>* m_data;<span class="comment">// 32位，指针4Byte</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (cstr)   <span class="comment">// 传入字符串，不是0</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 最后一位有标识符号</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(cstr)+<span class="number">1</span>]; </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, cstr);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;  <span class="comment">//没有传入字符串</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];</span><br><span class="line">        *m_data = <span class="string">'\0'</span>;     <span class="comment">// 默认只有最后的标识符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 深拷贝</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String&amp; String::<span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 拷贝赋值 </span></span><br><span class="line">    <span class="comment">// 判断 两者是否指向同一内存（必须操作）</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> == &amp;str)</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 清除原指针指向</span></span><br><span class="line">    <span class="keyword">delete</span>[] m_data;</span><br><span class="line">    <span class="comment">// 重新分配内存</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::~String()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">delete</span>[] m_data;<span class="comment">// 数组的分类内存，则需要delete[]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> String&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; x.get_data();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">-------------------------------</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"my_string.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">s1</span><span class="params">(<span class="string">"c++"</span>)</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">s2</span><span class="params">(<span class="string">"hello"</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">s3</span><span class="params">(s1)</span></span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s1&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s2&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    s3 = s2;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Three-Stack-amp-Heap"><a href="#Three-Stack-amp-Heap" class="headerlink" title="Three. Stack &amp; Heap"></a>Three. Stack &amp; Heap</h4><ul><li>stack 栈：是存在于作用域中的一块内存空间。(local object / auto object)<ul><li>当在作用域内调用函数时，函数会形成一个stack来防治</li><li>当在作用域内创建新对象（局部变量），则会开辟新内来存储它，当作用域结束，则会被释放掉（调用析构函数）</li></ul></li><li><p>heap 堆：由操作系统提供的全局空间，动态分配。（每次分配内存，需要手动销毁）（heap object）</p><ul><li>若没有手动delete，会出现内存泄漏（memory leak）</li><li>对象离开作用域，指针p结束了，但指向p的对象仍然存在，没有机会去delete它了</li><li>new 一个对象，先是分配内存，再调用构造函数 -&gt; （c 语言）malloc(n) + 数据转型 +调用构造函数<ul><li>指针p为对象在内存中的起始位置，同时是类中的this</li></ul></li><li>delete：先调用析构函数，再释放内存 -&gt; （c 语言）调用析构函数 + free(p)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Complex</span>&#123;</span>...&#125;;<span class="comment">// 定义类</span></span><br><span class="line">...</span><br><span class="line"><span class="function">Complex <span class="title">c3</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">//global object</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="function">Complex <span class="title">c1</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">// c1所占用空间来自stack，离开作用域则会自动销毁</span></span><br><span class="line">    Complex* p = <span class="keyword">new</span> Complex(<span class="number">3</span>);<span class="comment">// 由系统动态分配全局内存，离开作用域不会自动销毁，则需手动销毁</span></span><br><span class="line">    <span class="keyword">delete</span> p；</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> Complex <span class="title">c2</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;<span class="comment">// static object</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>static object 静态对象：离开作用域仍然存在，直至整个程序结束</p><ul><li>静态变量或者函数只有一份内存</li><li>非静态函数或变量，多次被调用，则会产生多份内存</li><li>静态类内变量，则是所有对象共有这个静态数据（必须给它定义，<strong>类内声明，类外定义</strong>）</li><li><p>静态的成员函数没有 this</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span>// 银行账户类</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span> m_rate;<span class="comment">// 利率多少与用户无关，所以应该是共同一样的，因此为静态变量</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">set_rate</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span>&amp; x)</span></span>&#123;m_rate=x;&#125; <span class="comment">// 静态变量只能由静态函数处理</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> Account::m_rate = <span class="number">8.0</span>;<span class="comment">// 定义静态变量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Account::set_rate(<span class="number">5.0</span>);<span class="comment">// 可以直接调用</span></span><br><span class="line">    </span><br><span class="line">    Account a;</span><br><span class="line">    a.set_rate(<span class="number">5.0</span>);<span class="comment">// 对象调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li>global object 全局对象：作用域为整个程序，直至整个程序结束</li><li><p><strong>new 动态分配内存块</strong>（VC编译器）</p><ul><li><p>Cookie</p><ul><li>当在debug模式调用程序时，除了类的大小，不仅有上下两个cookie（最终大小必须是类的倍数，不足则加pad）而且会分配更多空间（方便系统回收内存）</li><li><p>在正常执行模式调用程序时，只有类的大小和cookie（最终大小必须是类的倍数）</p><p><img src="http://github.com/soloistben/images/raw/master/cplus_image/动态分配内存.png" alt="动态分配内存" style="zoom: 50%;"></p></li><li><p>程序员只看到绿色那块（类的大小），不会看到完整cookie的大小</p></li><li><strong>Cookie (16): 00000041 表示 大小为4×16=64，1表示操作系统已经分配出去</strong>（因为16进制在二进制后面四个比特位均为0），0表示已经归还给操作系统，记录了整体长度，方便程序知道回收大小</li></ul></li><li><p>动态分配数组<br><img src="http://github.com/soloistben/images/raw/master/cplus_image/动态分配数组内存.png" alt="动态分配数组内存" style="zoom:67%;"></p><ul><li>3 表示数组大小</li><li>数组的分类内存，则需要delete[]，（系统才知道删除的是个数组），否则会出错<br><img src="http://github.com/soloistben/images/raw/master/cplus_image/删除数组.png" alt="删除数组" style="zoom:75%;"></li></ul></li></ul></li></ul><h4 id="Four-Object-Oriented-Programming-OOP-Object-Oriented-Design-OOD"><a href="#Four-Object-Oriented-Programming-OOP-Object-Oriented-Design-OOD" class="headerlink" title="Four. Object Oriented Programming (OOP), Object Oriented Design (OOD)"></a>Four. Object Oriented Programming (OOP), Object Oriented Design (OOD)</h4><ul><li><p>类与类之间的关系</p><ul><li><p><strong>Inheritance</strong> 继承（<strong>is a</strong>）</p><ul><li><p>public, protected, private 三种继承（Java只有public继承）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>：<span class="title">public</span> <span class="title">Base</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><pre><code>+ 父类所有参数和函数都被继承（Derived(子类)包含Base(父类)）+ 构造函数：先Base的默认构造函数（若没有默认的，并且多个构造函数，则需要表明用哪个构造函数），再Derived构造函数（由内而外）+ 析构函数：先Derived析构函数，再Base析构函数（由外而内）+ base class 的析构函数必须是**virtual function（虚函数）**，否则出现undefined behavior  + 在成员函数前加上virual，就变成成员函数  + 父类中函数，继承的是调用权  + non-virtual函数：不希望子类override（重新定义）该函数  + virtual函数：希望子类override（重新定义）该函数，并且默认有一个定义（空函数也是定义的一种）+ pure virtual 函数：希望子类必须override（重新定义）该函数，默认没有定义    <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> <span class="keyword">const</span></span>=<span class="number">0</span>;<span class="comment">// pure virtual</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">error</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; msg)</span></span>;<span class="comment">// impure virtual</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">ObjectID</span><span class="params">()</span> <span class="keyword">const</span></span>;<span class="comment">// non-virtual</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></code></pre><ul><li><strong>Composition</strong> 复合 （<strong>has a</strong> 包含）    <ul><li>class A 内部定义 class B的对象（表示 class A has a class B）</li><li>class A is Container 容器, class B is Component 组件（A 包含B）</li><li>构造函数：先Component的默认构造函数（若没有默认的，并且多个构造函数，则需要表明用哪个构造函数），再Container构造函数（由内而外）</li><li>析构函数：先Container析构函数，再Component析构函数（由外而内）</li></ul></li><li><strong>Delegation</strong> 委托 （<strong>Composition by Reference</strong>）<ul><li>class A 内部定义 class B的对象指针</li><li>需要用class B的对象时，才会去创建它，不像Composition一样同步创建class对象<ul><li>Composition：当A对象复制三遍，则三个A对象分别创建三个不同B对象</li><li>Delegation：当A对象复制三遍，则三个A对象指向同一个B对象，则可以节省空间（共享内存）<ul><li>共享内存：不能轻易发生修改</li><li>当需要修改其中一个A对象时，拷贝一个B对象，再修改；剩下两个A对象共享原本的B对象（copy on write, COW）</li></ul></li></ul></li><li>书写风格：Handle/Body（pimlp）：Container当成外部接口，Composition当成内部操作</li><li><strong>Inheritance + Compsition</strong></li></ul></li><li>子类中继承了父类，子类又有复合，构造函数顺序为父类、复合、子类</li><li>子类中继承了父类，父类又有复合，构造函数顺序为复合、父类、子类<ul><li><strong>Delegation + Inheritance</strong>（最强组合）</li></ul></li><li><p>一份数据，多种表现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Subject</span></span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">int</span> m_value;</span><br><span class="line">      <span class="built_in">vector</span>&lt;Observer*&gt; m_views;<span class="comment">// Delegation 委托</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">attach</span><span class="params">(Observer* obs)</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          m_views.push_back(obs);</span><br><span class="line">  &#125;</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">set_val</span><span class="params">(<span class="keyword">int</span> value)</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          m_value = value;<span class="comment">// 一旦数据改变</span></span><br><span class="line">          notify();<span class="comment">// 通知所有数据的可视化</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">notify</span><span class="params">()</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          <span class="comment">// 将所有可视化遍历更细显示的数据值</span></span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m_views.size(); ++i)</span><br><span class="line">              m_views[i] -&gt; update(<span class="keyword">this</span>, m_value);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Observer</span>// 可以被 <span class="title">Inheritance</span> 继承，形成多种数据可视化</span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(Subject* sub, <span class="keyword">int</span> vlaue)</span></span>=<span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li>Composite <a href="https://www.bilibili.com/video/BV1gb411g7pa?p=13" target="_blank" rel="noopener">video</a></li><li>Prototype (在父类可以创建未来的派生子类；子类创造自己，委托给父类) </li></ul><h4 id="Five-Generic-Programming"><a href="#Five-Generic-Programming" class="headerlink" title="Five.  Generic Programming"></a>Five.  Generic Programming</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;One-Definition-of-Class-without-pointer&quot;&gt;&lt;a href=&quot;#One-Definition-of-Class-without-pointer&quot; class=&quot;headerlink&quot; title=&quot;One. Definitio
      
    
    </summary>
    
    
    
      <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>EM</title>
    <link href="http://yoursite.com/2020/10/06/EM/"/>
    <id>http://yoursite.com/2020/10/06/EM/</id>
    <published>2020-10-06T13:07:10.000Z</published>
    <updated>2020-10-06T13:07:10.975Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hidden_Markov_Model</title>
    <link href="http://yoursite.com/2020/10/06/Hidden-Markov-Model/"/>
    <id>http://yoursite.com/2020/10/06/Hidden-Markov-Model/</id>
    <published>2020-10-06T12:55:04.000Z</published>
    <updated>2020-10-06T15:17:47.984Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>Hidden Markov Model = Markov Random Field + Time</strong> </p><ul><li>动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）</li><li><p>GMM高斯混合模型（样本之间独立同分布）；但是动态模型，样本之间不是独立同分布</p><p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM1.png" alt="HMM1" style="zoom: 67%;"></p></li><li><p>图中动态模型包括：横向的时间关系（Time），纵向的混合关系（Mixture，不同变量混合）</p></li><li><strong>若hidden variable是离散的，则动态模型为HMM</strong>；线性连续的，则是Kalmm Filter 卡尔曼滤波器；非线性连续的，则是Partide Filter</li></ul><p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM2.png" alt="HMM2" style="zoom:75%;"></p></li><li><p>HMM: param λ = (π, A, B)（π是初始概率分布、A转移矩阵、发射矩阵）</p><ul><li><strong>observed variable O</strong>: o~1~, o~2~, …, o~t~,… → V = {v~1~, …v~M~} （观察值，观测变量o的值域）</li><li><strong>hidden variable I</strong>: i~1~, i~2~, …, i~t~,… → Q = {q~1~, …q~N~} （隐变量i的值域）</li><li>$A = [a<em>{ij}], a</em>{ij} = P(i<em>{t+1}=q</em>{j}|i<em>{t}=q</em>{i})$</li><li>$B = [b<em>{j(k)}], b</em>{j(k)} = P(o<em>{t}=v</em>{k}|i<em>{t}=q</em>{j})$</li></ul></li><li>两个假设<ul><li>齐次马尔可夫假设<ul><li>$P(i<em>{t+1}|i</em>{t}, i<em>{t-1}, …, i</em>{1}, o<em>{t}, o</em>{t-1}, …, o<em>{1}) = P(i</em>{t+1}|i_{t})$</li><li>隐状态i~t+1~只与隐状态i~t~有关</li></ul></li><li>观测独立假设<ul><li>$P(o<em>{t}|i</em>{t}, i<em>{t-1}, …, t</em>{1}, o<em>{t-1}, …, o</em>{1}) = P(o<em>{t}|i</em>{t})$</li><li>观测状态o~t~只与观测状态i~t~有关</li></ul></li></ul></li><li><p>HMM解决三个问题（已知参数 λ = (π, A, B)）</p><ul><li>Evalution → P(O|λ) （已知参数下，求解O现象的概率）→ 前向后向算法</li><li>Learning → 求解参数，λ = argmax P(O|λ) → EM算法（以前是Baum Welch算法）</li><li>Decoding → 找到一个状态序列，使 P(I|O)达到最大（I～ = argmax P(I|O)）<ul><li>预测问题：$P(i<em>{t+1}|o</em>{1},…,o<em>{t-1}, o</em>{t})$ 已知t个观察序列，预测下一时刻t+1的隐状态</li><li><strong>滤波问题</strong>：$P(i<em>{t}|o</em>{1},…,o<em>{t-1}, o</em>{t})$ 已知t个观察序列，求解当前时刻t的隐状态</li></ul></li></ul><p><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=82" target="_blank" rel="noopener">白板系列</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hidden Markov Model = Markov Random Field + Time&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GMM高斯
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>PGM_Inference</title>
    <link href="http://yoursite.com/2020/10/06/PGM-Inference/"/>
    <id>http://yoursite.com/2020/10/06/PGM-Inference/</id>
    <published>2020-10-06T07:09:49.000Z</published>
    <updated>2020-10-06T15:52:39.989Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Inference 推断 → 求概率<ul><li>P(X) = P(x~1~, x~2~, …, x~p~)</li><li>边缘概率：$P(x<em>{i}) = Σ</em>{1}…Σ<em>{i-1} Σ</em>{i+1}…Σ_{p} P(X) $（除了i的都积分）</li><li>条件概率：$P(x<em>{a}|x</em>{b}),(x=x<em>{a}\bigcup x</em>{b})$</li><li>最大后验 MAP Inference：$\hat{Z} = argmaxP(Z|X) \propto argmax P(Z,X)$（贝叶斯定理）（不一定直接求后验，只要得到最大即$\hat{Z}$可）</li></ul></li><li>方法<ul><li>精确推断<ul><li>Variable Elimination (VE) 变量消除法</li><li><strong>Belief Propagation</strong> (BP) 信念传播（与反向传播不一样）（弥补VE缺点）<ul><li>另外一个名称：Sum-Product Algorithm</li><li>针对树结构</li></ul></li><li>Junction Tree Algorithm<ul><li>基于BP，由树结构扩展到普通图结构</li></ul></li></ul></li><li>近似推断<ul><li>Loop Belief Propagation<ul><li>基于BP，针对有环图</li></ul></li><li>Mente Carlo Inference<ul><li>Importance Sampling</li><li>MCMC (Markov Chain Mente Carlo)</li><li>基于采样</li></ul></li><li>Variational Inference<ul><li>确定性近似</li></ul></li></ul></li></ul></li></ul><h5 id="Variable-Elimination"><a href="#Variable-Elimination" class="headerlink" title="Variable Elimination"></a>Variable Elimination</h5><p><img src="https://github.com/soloistben/images/raw/master/PGM_Inference/VE1.png" alt="VE1" style="zoom:75%;"></p><ul><li>假设a,b,c,d均为二值的随机变量{0,1}</li><li>$P(d) = Σ<em>{a,b,c} P(a,b,c,d) = Σ</em>{a,b,c} P(a)P(b|a)P(c|b)P(d|c)$ （将各个条件概率看成因子）<ul><li>→ P(a=0)P(b=0|a=0)P(c=0|b=0)P(d|c=0) + P(a=1)P(b=0|a=1)P(c=0|b=0)P(d|c=0) +…+ P(a=1)P(b=1|a=1)P(c=1|b=1)P(d|c=1) = 8*因子积</li><li>由于马尔可夫的性质，结点只与相邻的相关，与其他无关，可以优先将有关的先合并。</li><li>→ $Σ<em>{b,c} P(c|b)P(d|c) Σ</em>{a} P(a)P(b|a)$（$Σ<em>{a} P(a)P(b|a) = Σ</em>{a} P(a,b) = P(b)$， <strong>将P(a)看成一个函数Φ(a)，P(b|a)看成Φ(a,b)，则Σ~a~ P(a)P(b|a)看成Φ~a~(b)</strong>）</li><li>→ $Σ<em>{c} P(d|c) Σ</em>{b}P(c|b)Φ<em>{a}(b) = Σ</em>{c} P(d|c) Φ<em>{b}(c) = Φ</em>{c}(d)$</li></ul></li><li>若是无向图，则为 $P(a,b,c,d) = \frac{1}{Z}\prod Φ(x)$</li><li>主要思想：乘法分配律（ab+ac=a(a+c)）</li><li>缺点：1、重复计算（求另外一个点时，则需要重新求（则没有存储中间结果，若是链很长，则计算量很大））；2、消去次序（一般相关最少的先消去，但在无向图找到最优消去次序是NP-Hard问题）</li></ul><h5 id="Belief-Propagation"><a href="#Belief-Propagation" class="headerlink" title="Belief Propagation"></a>Belief Propagation</h5><ul><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Inference 推断 → 求概率&lt;ul&gt;
&lt;li&gt;P(X) = P(x~1~, x~2~, …, x~p~)&lt;/li&gt;
&lt;li&gt;边缘概率：$P(x&lt;em&gt;{i}) = Σ&lt;/em&gt;{1}…Σ&lt;em&gt;{i-1} Σ&lt;/em&gt;{i+1}…Σ_{p} P(X) $
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Exponential_Family_Distribution</title>
    <link href="http://yoursite.com/2020/10/05/Exponential-Family-Distribution/"/>
    <id>http://yoursite.com/2020/10/05/Exponential-Family-Distribution/</id>
    <published>2020-10-05T08:43:34.000Z</published>
    <updated>2020-10-07T13:13:35.336Z</updated>
    
    <content type="html"><![CDATA[<h4 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h4><ul><li>Gaussian Distribution, Bernoulli Distribution (Categorical Distribution), Binomial Distribution (Multinomial Distribution), Poisson Distribution, Beta Distribution, Dirichlet Distribution, Gamma Distribution</li><li><font color="red">$P(x|η) = h(x) e^{η^{T} Φ(x) - A(η)}$</font> （分为三部分）<ul><li>η属于参数、p维向量；A(η): log partition function（配分函数）；h(x)与η无关，往往设置为1</li><li>partition function（源自于统计物理学）：$P(x|θ) = \frac {1}{Z} \hat{P}(x|θ)$ （Z为归一化因子，$Z=\int \hat{P}(x|θ) d_{x}$）</li><li>$P(x|η) = \frac {1}{e^{A(η)}} h(x) e^{η^{T} Φ(x)} → e^{A(η)} = Z → A(η) = log Z$，所以A(η)是log partition function</li></ul></li><li>特点<ul><li><strong>充分统计量 sufficient statistics</strong>：Φ(x)<ul><li>统计量：对样本的加工，关于样本的一个函数，均值、方差等</li><li>充分：统计量就可以完整表达样本特征信息了</li><li>Online Learning （可以仅存储样本统计量信息，就不需要存储大量样本，起到压缩数据的效果）</li></ul></li><li><strong>共轭</strong><ul><li>$P(Y|X) = \frac{P(X|Y)P(Y)}{\int P(X|Y)P(Y) d_{Y}}$（后验概率是求不出来，或者积分难问题，太复杂，难以求解）</li><li>计算f(Y)后验分布的期望：近似推断（变分推断、MCMC）</li><li>若似然P(X|Y)与先验P(Y)共轭（如二项分布和Beta分布），则后验与先验同分布，则仅需算后验分布的参数即可，就可以不用计算积分</li></ul></li><li><strong>最大熵</strong>（无信息先验）<ul><li>没有先验，则认为是所有样本等概率，但无法定量分析，则引入最大熵</li><li>赋予先验：共轭（为了计算方便）；最大熵（无信息先验）</li></ul></li></ul></li><li>模型和推断<ul><li><strong>广义线性模型</strong><ul><li>目标：解决分类、回归问题</li><li>线性组合（w^T^x）</li><li>link function（激活函数的反函数）</li><li>指数族分布（y|x ~ 指数族分布）（如线性回归：y|x ~ N(μ,σ^2^)；线性分类：y|x ~ 0/1分布（Bernoulli））</li></ul></li><li><strong>概率图模型</strong><ul><li>无向图：RBM 波尔兹曼机</li></ul></li><li><strong>变分推断</strong><ul><li>指数族分布可以简化变分推断</li></ul></li></ul></li><li>Gaussian Distribution<ul><li>$P(x|θ) = \frac{1}{\sqrt{2\pi}σ} e^{\frac{(x-μ)^2}{-2σ^2}},θ=(μ,σ^2)$  一维高斯分布<ul><li>η = η(θ)，A(η) = A(η(θ))，将θ映射成η​</li><li>$P(x|\theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2\mu x+\mu ^2)} = e^{-\frac{1}{2}log^{2\pi \sigma^2}} e^{-\frac{1}{2\sigma^2} {\begin{bmatrix} -2\mu \ 1 \end{bmatrix}}{\begin{bmatrix}x \ x^2 \end{bmatrix}} -\frac{\mu^2}{2\sigma^2}} \ = exp({\begin{bmatrix} \frac{\mu}{\sigma^2} \ \frac{-1}{2\sigma^2} \end{bmatrix}}{\begin{bmatrix}x \ x^2 \end{bmatrix}} - (\frac{\mu^2}{2\sigma^2} + \frac{1}{2} log^{2\pi \sigma^2}))$<ul><li>$η^T = {\begin{bmatrix}\frac{μ}{σ^2}\ \frac{-1}{2σ^2} \end{bmatrix}}, Φ(x) = {\begin{bmatrix}x \ x^2 \end{bmatrix}}, A(η)=\frac{μ^2}{2σ^2}+ \frac{1}{2} log^{2\piσ^2}$</li><li>设定$η={\begin{bmatrix}η \ η^2 \end{bmatrix}}$，则$A(η) = -\frac{η_1^2}{4η_2}+\frac{1}{2}log^{-\frac{\pi}{η_2}}$</li></ul></li></ul></li></ul></li><li><strong>充分统计量Φ(x) 与 对数配分函数A(η)</strong> <ul><li>P(x|η) 概率积分为1</li><li>$P(x|η) = \frac{1}{e^{A(η)}}h(x)e^{η^T Φ(x)}→e^{A(η)}=\int h(x)e^{η^TΦ(x)}d_x$</li><li>两边对η求导：$e^{A(η)}*A’(η) = \frac{d(\int h(x)e^{η^TΦ(x)}d<em>{x})}{dη}=\int h(x) Φ(x)e^{η^TΦ(x)}d</em>{x}$</li><li>$A’(η) =\int h(x) Φ(x)e^{η^T Φ(x)-A(η)}d<em>{x}=\int Φ(x)P(x|η)d</em>{x} = E_{P(x|η)}[Φ(x)]$</li><li>$A’’(η)=Var_{P(x|η)} [Φ(x)]$，则A(η)是一个凸函数</li></ul></li><li><p><strong>充分统计量Φ(x) 与 极大似然估计</strong></p><ul><li>Data: D = {x~1~,x~2~,…,x~N~} N个样本</li><li>$η_{MLE}=argmax \ log^{P(D|η)}=argmax \ log^{\prod P(x_i|η)}=argmax \sum log^{P(x_i|η)} \ = argmax \sum log^{h(x_i)e^{η^T Φ(x_i)-A(η)}} = argmax \sum [log^{h(x_i)}+η^T Φ(x_i)-A(η)]$<ul><li>$→ η_{MLE} ∝argmax \ \sum [η^T Φ(x_i)-A(η)]$</li><li>$\frac{d(Σ [η^T Φ(x<em>i)-A(η)])}{dη} =\sum \frac{d(η^T Φ(x_i)-A(η))}{dη}=\sum[Φ(x_i)-A’(η)] =\sum Φ(x_i)- N*A’(η)=0 \ → A’(η</em>{MLE})=\frac{1}{N}\sum Φ(x_i)$</li><li>$η<em>{MLE} = A’^{(-1)}(η</em>{MLE})$（反函数） → 则η~MLE~是可求解的，只需要记录即$\frac{1}{N}\sum Φ(x_i)$可，不需要记录所有样本</li></ul></li></ul></li><li><p><strong>最大熵角度</strong></p><ul><li>若一个事件发生概率为p，其信息量为-log p （若p=1,信息量就为0；一个确定的事件没有信息量）</li><li>熵：$E[-log^p]=\int -p(x)log^{p(x)} d_{x}=-\sum p(x)log^{p(x)}$ （对信息的衡量，对信息可能性的衡量）（熵只与x的分布有关，与取值无关）</li><li>最大熵&lt;=&gt;等可能（利用最大熵对等可能定量分析）（最大熵，对未知的分布进行猜测，因为不知道，所以认为都是等可能的）（<strong>要想熵最大，未知的分布必须是等可能</strong>）<ul><li>$H[p]=-\sum p(x)log^{p(x)}$</li><li>假设x是离散的，P(x=1) = p~1~，P(x=2) = p~2~，…，P(x=k) = p~k~，Σp~i~ = 1</li><li>则 $max \ H[P] = max[-\sum p_i log^{p_i}],s.t.\sum p_i=1$</li><li>$\hat{p_i} = argmax \ H[P] = argmin \ \sum p_i log^{p_i}$ （优化问题）</li><li>拉格朗日：$L(p, λ) = \sum p_i log^{p_i} - λ(1- \sum p_i)，\frac{dL}{dp_i} = log^{p_i} + 1 - λ = 0 → p_i = e^{λ-1}$ (常数)</li><li>则 p~1~ = p~2~ = … p~k~ = 1/k，p(x)是了离散型均匀分布</li></ul></li><li>最大熵原理：<strong>在满足已知事实（约束条件）（已知数据）下，什么分布是具有最大熵的分布</strong><ul><li>Data = {x~1~,x~2~,…,x~N~} 通过经验分布（概率分布$\hat{P}(X=x) = \hat{p}(x) = \frac{count(x)}{N}$）定量描述数据</li><li>需要P分布的$E<em>{\hat{p}}[x], Var</em>{\hat{p}}[x]$，设定f(x)是任意关于x的向量函数（$f=[f<em>1,f_2,…,f_Q]^T$），则$E</em>{\hat{p}}[f(x)]=Δ$（即是已知事实）</li><li>$P(X)→p(x),H[p]=-\sum p(x)log^{p(x)}$<br>$= argmin ∑ p(x)log^{p(x)}, s.t.∑p(x)=1,E<em>p[f(x)]=∑p(x)f(x)=E</em>{\hat{p}}[f(x)]=Δ $</li><li>拉格朗日：$L(p, λ_0, λ) = \sum p(x)log^{p(x)} - λ_0(1- \sum p(x)) - λ^T(Δ - \sum p(x)f(x))$</li><li>求导：$\frac{dL}{d_{p(x)}} = \sum [log^{p(x)} + 1] - \sum λ_0 - \sum λ^T f(x) = Σ[log^{p(x)}+1-λ_0-λ^T f(x)] = 0 \ \rightarrow log^{p(x)}+1-λ_0-λ^T f(x)=0 →p(x) = e^{λ^T f(x) - (1-λ_0)}$</li><li>令η=λ^T^，Φ(x)=f(x)，A(η)=(1-λ~0~)</li><li>则用最大熵原理推出，p(x)是指数分布时熵最大</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;指数族分布&quot;&gt;&lt;a href=&quot;#指数族分布&quot; class=&quot;headerlink&quot; title=&quot;指数族分布&quot;&gt;&lt;/a&gt;指数族分布&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Gaussian Distribution, Bernoulli Distribution (Categ
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Dimensionality_Reduction</title>
    <link href="http://yoursite.com/2020/10/02/Dimensionality-Reduction/"/>
    <id>http://yoursite.com/2020/10/02/Dimensionality-Reduction/</id>
    <published>2020-10-02T13:10:36.000Z</published>
    <updated>2020-10-07T06:08:16.415Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>过拟合</p><ul><li>解决方法：增加数据、正则化、降维</li><li><p>原因：<strong>维度灾难</strong></p><ul><li>在没有很多数据集时，只能降维</li><li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li><li><p>从几何层面看：</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p><ul><li>2维正方形面积：1，圆形：$\pi*(0.5)^2$</li><li>3维正方体体积：1，球体体积：$\frac{4}{3}<em>\pi</em>(0.5)^3 = K*(0.5)^3$</li><li>D维超立方体体积：1，超球体体积： $K*(0.5)^D$</li><li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p></li><li><p>D维外超球体体积：$K*1^D = K$，环形体积：外超球体体积 - 内超球体体积 = $K - K(1-e)^D$</p><ul><li>V外/V内  $=1-(1-e)^D  =1$（$0&lt;e&lt;1$，D趋向无穷大之后，$(1-e)^D$趋向于0）</li><li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li></ul></li></ul></li></ul></li></ul></li><li>Data<ul><li>N个p维样本 X（维度N×p）（设$I$为N维全1列向量）</li><li>样本均值 $\hat{X}=\frac{1}{N} \sum x_i = \frac{1}{N} X^T I$（维度p×1）</li><li>方差 $S = \frac{1}{N} \sum (x_i - \hat{X})(x_i - \hat{X})^T = \frac{1}{N} X^T (I - \frac{1}{N} I I^T) (1-\frac{1}{N} I I^T)^T X = \frac{1}{N} X^T H H^T X = \frac{1}{N} X^T H X$<ul><li>（维度p×p）</li><li>$H =  I-\frac{1}{N} I I^T$  centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li><li>$H^T = H, H^2 = H H^T = (I-\frac{1}{N}I I^T) (I-\frac{1}{N} I I^T)^T = I-\frac{1}{N} I I^T = H$</li><li>$H^n = H$</li></ul></li></ul></li><li><p>降维方法</p><ul><li>直接降维 （特征选择：人工选取重要特征 ）</li><li><p>线性降维</p><ul><li><p><strong>Principal Components Analysis PCA 主成分分析</strong></p><ul><li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p></li><li><p>最大投影方差</p><ul><li>找到一个u~1~平面，使投影间距达到最大（投影到u~2~平面，距离太小，没有意义）<ul><li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li><li>第1步：中心化：先将所有数据平移，利于计算，即 $x_i - \hat{X}$</li><li>第2步：投影到u~1~平面：$(x_i - \hat{X})^T u_1$（设定 $|u_1|=1$，即$u_1^T u_1=1$）</li><li>第3步：投影方差$ J = \frac{1}{N} \sum [(x_i - \hat{X}^T u_1]^2 = \frac{1}{N} \sum [u_1^T (x_i - \hat{X}) (x_i - \hat{X})^T u_1] = u_1^T S u_1$</li></ul></li><li>$\hat{u_1} = argmax \ u_1^T S u_1,s.t. u_1^T u_1=1$<ul><li>使用拉格朗日求解</li><li>$L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)$</li><li>$\frac{dL}{du_1} = 2 S u_1 - 2λ u_1 = 0 → S u_1 = λ u_1$ </li><li><strong>u~1~为eigen-vector特征向量，λ为eigen-value特征值</strong></li><li>解法1：对方差矩阵特征分解，即可求解PCA</li><li>解法2：直接对原始数据进行操作：中心化后的 $HX = UΣV^T$ 进行奇异值分解（U和V均为正交矩阵），$S = X^T H X = X^T H^T H X = (VΣU^T)(UΣV^T) = V Σ^2 V^T$（可先忽略1/N常数）（维度p×p），因此直接求解HX奇异值分解，也就是求解了S的特征分解</li><li>假设 $B = H X X^T H = (UΣV^T)(VΣU^T) = U Σ^2 U^T$（维度N×N），则B与S有一样的eigenvalue，（S特征分解得到方向V（主成分）然后通过$HX V$得到新坐标）（B特征分解直接得到坐标U，称为主坐标分析 <strong>principal coordinate analysis PCoA</strong>）</li><li>$HX V = UΣV^T V = UΣ$（UΣ是坐标矩阵），$BUΣ = U Σ^2 U^T UΣ = UΣ Σ^2$（UΣ是特征向量组成的矩阵）</li></ul></li></ul></li><li>最小重构代价<ul><li>投影在u~1~平面的点恢复到原来样子的代价</li><li>设从原p维降到q维（下面u代表特征）<ul><li>$x_i = \sum_k^p (x_i^T u_k) u_k, \hat{x_i} =  \sum_k^q (x_i^T u_k) u_k$  (用特征$u_k$描述样本$x_i$，$x_i^T u_k$为距离大小，$u_k$为单位大小，则第k维描述为$(x_i^T u_k) u_k$)</li><li>代价函数 $L =\frac{1}{N} \sum||x<em>i - \hat{x_i}||^2 = \sum</em>{k=q+1}^p u<em>k^T S u_k  = \sum</em>{k=q+1}^p λ_k, s.t. u_k^T u_k=1$</li><li>$u_k = argmin \ L$ </li></ul></li></ul></li><li>概率角度<ul><li>P-PCA<ul><li>设定observed data X为p维（特征为连续型数据），latent variable Z为q维（q&lt;p）</li><li>设定$Z$~$N(0, I)$（服从高斯分布，q维）</li><li>$X = WZ + u + ε$​（X是Z的一个线性变换加噪声）</li><li>$X|Z$~$N(WZ + u, σ^2 I)，X~N(u, WW^T+σ^2 I)$</li><li>噪声$ε$~$N(0, σ^2 I)$（p维）</li><li>这也是一种Linear Gaussian Model（称该模型各向同性，对各方向影响是一样的）</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>求P(Z), P(X|Z), P(X), 最后求后验P(Z|X)、用EM求参数W, u, σ</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR4.png" alt="DR4" style="zoom:75%;"></p><pre><code>  + 从服从高斯分布的Z中，投影点在方向W，进行线性变换得到X，也得到服从高斯分布的X|Z，方向W上有很多的高斯分布（各向同性）；X的分布不在方向W上，且中间很宽（因为高斯分布中间高两边低）[详情](https://www.bilibili.com/video/BV1aE411o7qd?p=27)</code></pre><ul><li>MDS<ul><li>非线性降维</li></ul></li><li>流型</li><li>Isomap</li><li>LLE</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;过拟合&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解决方法：增加数据、正则化、降维&lt;/li&gt;
&lt;li&gt;&lt;p&gt;原因：&lt;strong&gt;维度灾难&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在没有很多数据集时，只能降维&lt;/li&gt;
&lt;li&gt;每增加一维，二值的特征，都是2的指数倍增
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>SVM</title>
    <link href="http://yoursite.com/2020/10/02/SVM/"/>
    <id>http://yoursite.com/2020/10/02/SVM/</id>
    <published>2020-10-02T08:50:04.000Z</published>
    <updated>2020-10-09T10:27:18.933Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Data : N个p维样本 X（维度N×p），y = {-1,1}</li><li><p>SVM 三宝：间隔，对偶，核技巧</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p></li><li><p>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</p></li><li><p><strong>hard-margin SVM</strong>（硬间隔）</p><ul><li>最大间隔分类器 $= max \ margin(w, b),s.t. y_i(w^T x_i+b) &gt; 0,for i = 1,…,N$</li><li>点到直线距离，垂直线最短</li><li>$margin(w, b) = min \ distance(w, b, x_i) = min \ \frac{1}{||w||} |w^T x_i + b|$</li><li>$→ max<em>{w,b} \ min_x \frac{1}{||w||} |w^T x_i + b|,(s.t. y_i(w^T x_i+b) &gt; 0)=max</em>{w,b} \frac{1}{||w||} min_x y_i(w^T x_i + b)$ (存在$r&gt;0$，使$y_i(w^T x_i + b) =r$，可以设置$r=1$)</li><li>$→ max \ \frac{1}{||w||} ,s.t. min \ y_i(w^T x_i + b) = 1 → min \ \frac{1}{2} w^T w , s.t. y_i(w^T x_i + b)\geqslant 1$(convex optimization 二次凸优化问题)(primal problem原问题)</li><li>拉格朗日：$L(w, b, λ) = \frac{1}{2} w^T w + \sum λ_i(1-y_i(w^T x_i + b)) (λ_i \geqslant  0, (1-y_i(w^T x_i + b)) \leqslant 0$；若$(1-y_i(w^T x_i + b))&gt;0$，L为正无穷，无解；仅在 $λ_i=0， (1-y_i(w^T x_i + b)) =0$，达到最大L)</li><li><strong>primal problem 原问题</strong>&lt;=&gt; $min<em>{w,b} \ max</em>λ \ L(w, b, λ),s.t. λ_i \geqslant 0$ （对w，b没有限制）$→ min \ \frac{1}{2} w^T w$</li><li><p><strong>dual problem 对偶问题</strong>：$max<em>λ \ min</em>{w,b} \ L(w, b, λ),s.t. λ_i \geqslant 0$</p><ul><li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li><li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li><li>$\frac{dL}{db} = \frac{d[\sum λ_i(1-y_i(w^T x_i + b))]}{db} = \frac{d[- Σ λ_i y_i b]}{db} = -\sum λ_i y_i =0$，带入原式$L = \frac{1}{2} w^T w + \sum λ_i - \sum λ_i y_i w^T x_i$</li><li>$\frac{dL}{dw} = w - \sum λ_i y_i x_i = 0  → w = \sum λ_i y_i x_i$，带入原式$L=\frac{1}{2} w^T w + \sum λ_i - w^T w = \sum λ_i - \frac{1}{2}w^T w$</li><li><font color="red">$→ min \ \frac{1}{2} w^T w - \sum λ_i,s.t. λ_i\geqslant0, \sum λ_i y_i =0$</font>  （此处不能求λ偏导）</li><li>$→ min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i,s.t. λ_i\geqslant 0, \sum λ_i y_i =0$</li><li><strong>KKT条件</strong>：参数偏导为0（$\frac{dL}{db}=0，\frac{dL}{dw}=0，\frac{dL}{dλ}=0$），$λ_i(1-y_i(w^T x_i + b))=0， λ_i\geqslant0，(1-y_i(w^T x_i + b)\leqslant0$</li><li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li><li>$\hat{w} = \sum λ_i y_i x_i$, （存在$x_k,y_k,1-y_k(w^T x_k + b)=0$）$\hat{b} = y_k - w^T x_k = y_k - (\sum λ_i y_i x_i^T) x_k$</li></ul></li><li><p>$f(x) = sign(\hat{w}^T x + \hat{b})$</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p><ul><li>落在虚线的样本点就是$x_k（y_k(w^T x_k + b)=1$），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li></ul></li></ul></li><li><p><strong>soft-margin SVM</strong>（软间隔）</p><ul><li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li><li><p>soft-margin SVM在hard-margin SVM基础上允许一点点错误，$min \ \frac{1}{2} w^T w + loss $</p><ul><li>分错点的个数：$loss = \sum I{y_i(w^T x_i + b)&lt;1}$ （关于w是不连续的，无法求导，因此不采取）</li><li><p>hinge 距离：$hinge \ loss = max {0, 1-y_i(w^T x_i + b)}$</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p></li><li><p>$min \ \frac{1}{2} w^T w + C \sum max {0, 1-y_i(w^T x_i + b)},s.t. y_i(w^T x_i + b)\geqslant 1$ （超参数C）</p></li><li><p>→ 设定$ξ_i = y_i(w^T x_i + b)，min \ \frac{1}{2} w^T w + C \sum ξ_i,s.t. y_i(w^T x_i + b)\geqslant (1-ξ_i), ξ_i\geqslant 0$（同样用对偶问题方式来求解）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p></li></ul></li></ul></li><li><p>约束优化问题</p><ul><li>primal problem 原问题：$min \ f(x),s.t. m_i(x) \leqslant 0, n_j(x)=0 (i=1,…,M, j=1,…,N)$</li><li>原问题的无约束形式（关于x的函数）：拉格朗日：<font color="red">$L(x, λ, η) = f(x) + \sum λ<em>i m_i(x) + \sum η_i n_i(x)  → min_x \ max</em>{λ,η} \ L(x, λ, η),s.t. λ_i\geqslant 0$</font></li><li>证明两者等价：如果违法约束$m<em>i(x)&gt;0，max</em>λ \ L → ∞$；反之，$max_λ \ L$ 必有最大值（$λ_i=0$时）（即排除了$m_i(x)&gt;0$情况，过滤掉违反约束的情况）</li><li>dual problem 对偶问题（关于λ,η的函数）：<font color="red">$max_{λ,η} \ min_x \ L(x, λ, η),s.t. λ_i\geqslant 0$</font><ul><li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （$max<em>{λ,η} \ min_x \ L(x, λ, η)  \leqslant  min_x \ max</em>{λ,η} \ L(x, λ, η)$）</li><li>证明：$min<em>x \ L \leqslant L \leqslant max</em>{λ,η} \ L  →  A(λ,η) \leqslant L \leqslant B(x)  →  A(λ,η) \leqslant B(x)  →  max \ A(λ,η) \leqslant min \ B(x)$</li><li>$→ max<em>{λ,η} \ min_x \ L(x, λ, η) \leqslant min_x \ max</em>{λ,η} \ L(x, λ, η)$</li><li>（在$min_x \ L$已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li><li>强对偶性：对偶问题=原问题</li></ul></li><li><p>几何解释</p><ul><li>primal problem: $min \ f(x),s.t. m_1(x)\leqslant 0$ (D定义域，D=dom~f~ ∩ dom~m1~)， <font color="red">原问题最优解：$p^* = min f(x)$</font></li><li>$L(x, λ) = f(x) + λ m<em>1(x),s.t. λ\geqslant 0$ <font color="red">对偶最优解：$d^* = max</font></em>λ \ min_x \ L(x, λ)$&lt;/font&gt;</li><li><p>将问题投影入二维空间：引入集合(区域) $G = {(m_1(x), f(x))|x∈D}$</p><ul><li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li><li>凸集是指集合内任意两点的连线都在集合内</li><li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li><li>目标函数f如果不是凸函数，则不是凸优化问题</li><li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li><li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li><li>结论：凸函数的局部最优解就是全局最优解</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p><ul><li><p>$p^* = inf {f(x)|(m_1(x), f(x))∈G, m_1(x) \leqslant 0}$（集合中没有最小值概念，对应的是下确界）</p><ul><li>$P*$ 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li></ul></li><li><p>$d^* = max_λ \ g(λ) , g(λ) = min_x \  f(x) + λ m_1(x) , g(λ) = inf {f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G}$</p><ul><li>一条过原点直线 $f(x)+λm_1(x)= 0$(斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）$g(λ)\leqslant p^*$ </li><li>当调整斜率$λ^<em>$，得到一个 $g(λ^</em>) = f(x) + λ^<em> m_1(x)$ 同时与G的俩角相切，此时，直线与纵轴的交点为$d^</em>$（绿线）</li><li><p>$d^<em> \leqslant p^</em>$ （凸优化+slater条件 → $d^<em> = p^</em>$）（SVM是二次规划问题，符合slater条件）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p></li></ul></li></ul></li></ul></li><li><p>slater条件</p><ul><li>Convex + Slater → Strong Duality （充分不必要条件）</li><li>定义：存在$\hat{x}$在relint，使$m_i(x)&lt;0 (i=1,…,M)$<ul><li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li><li>仿射函数即由由1阶多项式构成的函数，一般形式为 $f (x) = Ax + b$（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li></ul></li><li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li><li>放松的slater条件：在$m_i(x)$中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足$m_i(x)&lt;0$ （凸二次规划问题：目标函数f是凸的，不等式约束$m_i$是仿射函数，等式约束$n_j$也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li></ul></li><li>KKT条件<ul><li>KKT  &lt;=&gt; Strong Duality ($d^<em> = p^</em>$)（充要条件）</li><li>从$p^<em>$得到最优$x^</em>$，从$d^<em>$得到$λ^</em>$、$η^*$</li><li>可行域（可行条件）：$m_i(x^<em>) \leqslant 0, n_j(x^</em>)=0, λ^*\geqslant 0$</li><li>互补松弛<ul><li>$d^<em> = max_{λ,η} \ g(λ,η) = g(λ^</em>, η^<em>) = min_x \ L(x, λ^</em>, η^<em>) \leqslant L(x^</em>, λ^<em>, η^</em>) \ = f(x^<em>) + \sum λ_im_i(x^</em>) + \sum η_in_i(x^<em>) = f(x^</em>) + \sum λ_im_i(x^<em>) \leqslant f(x^</em>) = p^*$</li><li>（$λ_i\geqslant 0，m_i \leqslant 0，则 λ_i m_i \leqslant 0$）</li><li><font color="red">互补松弛条件 ：$\sum λ_i m_i(x^*)  = 0 → λ_i^* m_i(x^*) $</font></li></ul></li><li>梯度为0<ul><li>$min_x \ L(x, λ^<em>, η^</em>) &lt;= L(x^<em>, λ^</em>, η^*)$</li><li>$x^*$是对应x最小值，则 $\frac{dL}{dx} = 0$</li></ul></li></ul></li></ul></li><li><p><strong>kernel SVM</strong></p><ul><li>Kernel Method（思想角度）</li><li>Kernel Trick（计算角度）</li><li><p>Kernel function</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png" alt="SVM9"></p><ul><li><p><strong>非线性带来高维转换（从模型角度）</strong></p><ul><li>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</li><li><p>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p></li><li><p>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</p></li><li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）     </li><li><font color="red">非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</font><p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p></li><li><p>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</p></li><li><p>三种方法转高维：1、类似MLP直接转高维；2、Kernel方法转高维；3、深度学习运用与或非构建有向无环图（神经网络）（与或非（三种基础运算均可用PLA表示）解决异或问题（复合运算）），神经网络：复合表达式、复合函数、MLP（FeedForward Neural Network）</p><ul><li><p>XOR：x~1~⊕x~2~ = (¬x~1~∧x~2~)∨(x~1~∧¬x~2~)</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png" alt="FNN1" style="zoom: 67%;"></p></li></ul></li></ul></li></ul></li><li><p><strong>对偶表示带来内积（从优化角度）</strong></p><ul><li>从频率视角归化到优化问题</li><li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题<ul><li>Doul Problem: $min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i  s.t. λ_i \geqslant 0, \sum_i  λ_i y_i =0$</li><li>内积：$x_i^T x_j$</li><li>非线性转换：$Φ(x_i)^T Φ(x_j)$ （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此很$Φ(x_i)^T Φ(x_j)$难i求解和计算量很大）</li><li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到$Φ(x_i)^T Φ(x_j)$ 结果</strong>（不需要先求Φ(x)再求内积）</li></ul></li><li><strong>Kernel function : $K(x, x’) =  Φ(x)^T Φ(x’) = \ &lt;Φ(x), Φ(x’)&gt;$</strong> <ul><li>存在$x, x’∈X$，使$K(x, x’) =  Φ(x)^T Φ(x’)$，则K就是一个核函数（如$K(x, x’)=e^{\frac{-(x-x’)^2}{2σ^2}}$）</li><li>蕴含了：非线性转换+内积</li></ul></li></ul></li><li>一般核函数指<strong>正定核函数</strong> <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a><ul><li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x’)为核函数</li><li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x’)，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使$K(x, x’) = \ &lt;Φ(x), Φ(x’)&gt;$，则K(x, x’)为正定核函数</li><li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x’)，若满足两个条件（对称性、正定性）则为正定和函数<ul><li>对称性：$K(x, x’) = K(x’, x)$</li><li>正定性：任取N个元素，x~1~,x~2~,…,x~N~∈X，对应的Gram矩阵是半正定的（K=[K(x~i~, x~j~)]）（两个定义等价，即证明：<strong>K(x, x’) = &lt;Φ(x), Φ(x’)&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li><li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（<f, g=""> = <g, f="">）、正定性（内积大于等于0，<f, f=""> &gt;= 0）、线性性（<r~1~ f~1~="" +="" r~2~="" f~2~="" ,="" g=""> = r~1~ <f~1~, g=""> + r~2~ <f~2~, g="">））  </f~2~,></f~1~,></r~1~></f,></g,></f,></li></ul></li><li>必要性证明<ul><li>在Hilbert Space中的Φ(x)具有对称性性质，$K(x, x’) = &lt;Φ(x), Φ(x’)&gt; = &lt;Φ(x’), Φ(x)&gt; = K(x’, x)$</li><li>K=[K(x~i~, x~j~)]（维度N×N）（半正定：任意a列向量，$a^T K a \geqslant 0$）</li><li>$a^T K a = \sum<em>i \sum_j a_i a_j K</em>{ij} = \sum_i \sum_j a_i a_j K(x_i, x_j) = \sum_i \sum_j a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; → 线性性\ → \sum_i \sum_j a_i a_j Φ(x_i)^T Φ(x_j) = \sum_i a_i Φ(x_i)^T \sum_j a_j Φ(x_j) = [\sum_i a_i Φ(x_i)]^T \sum_j a_j Φ(x_j) \ = \ &lt;\sum_i a_i Φ(x_i), \sum_j a_j Φ(x_j)&gt; \ =  ||\sum_i a_i Φ(x_i)||^2 &gt;= 0，半正定性$</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Data : N个p维样本 X（维度N×p），y = {-1,1}&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SVM 三宝：间隔，对偶，核技巧&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/soloistben/images/raw/master/sta
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Decision_Tree</title>
    <link href="http://yoursite.com/2020/10/02/Decision-Tree/"/>
    <id>http://yoursite.com/2020/10/02/Decision-Tree/</id>
    <published>2020-10-02T08:49:49.000Z</published>
    <updated>2020-10-07T06:14:10.687Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>基于数据特征构造决策树</p><ul><li>有向边</li><li><p>结点</p><ul><li>内部结点(internal node)-&gt;表示特征</li><li><p>叶子结点(leaf node)-&gt;表示类别</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p></li><li><p>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</p></li></ul></li><li>决策树表示给定特征条件下类的条件概率分布。<ul><li>一个条概率分布定义特征空间的一个划分上</li><li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li></ul></li><li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果<br><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li></ul></li><li><p>决策树学习过程：特征选择、决策树生成、剪枝</p><ul><li><p><strong>ID3算法</strong>（分类、多叉树）</p><ul><li><p>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）</p><ul><li>Information Gain 信息增益。信息增益越大对应的特征越重要</li><li>Entropy 熵，表示随机变量不确定的度量</li><li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"></p></li></ul></li><li><p>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</p></li><li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li></ul></li><li><p><strong>C4.5算法</strong>（分类、多叉树）</p><ul><li><p>特征选择</p><ul><li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p></li></ul></li><li><p>生成：与ID3算法类似</p></li><li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li><li>二叉树模型会比多叉树运算效率高</li><li>无剪枝</li></ul></li><li><p><strong>CART</strong> classification and regression tree（分类、回归、二叉树）</p><ul><li><p>分类</p><ul><li><p>特征选择</p><ul><li>Gini基尼指数</li><li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li><li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"></p></li><li><p>生成</p><ul><li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li><li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li></ul></li><li><p>损失函数</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p><ul><li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li></ul><p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p></li></ul></li><li><p>回归</p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;基于数据特征构造决策树&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有向边&lt;/li&gt;
&lt;li&gt;&lt;p&gt;结点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内部结点(internal node)-&amp;gt;表示特征&lt;/li&gt;
&lt;li&gt;&lt;p&gt;叶子结点(leaf node)-&amp;gt;表示类别&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Statistics</title>
    <link href="http://yoursite.com/2020/09/11/Statistics/"/>
    <id>http://yoursite.com/2020/09/11/Statistics/</id>
    <published>2020-09-11T08:34:56.000Z</published>
    <updated>2020-10-07T08:54:38.465Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Statistics-for-Machine-Learning"><a href="#Statistics-for-Machine-Learning" class="headerlink" title="Statistics for Machine Learning"></a>Statistics for Machine Learning</h4><h5 id="One-两大派系"><a href="#One-两大派系" class="headerlink" title="One. 两大派系"></a>One. 两大派系</h5><ul><li><strong>频率派：统计机器学习</strong>（model ($f(x)=w^T x+b$), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）<ul><li>正则化（L1,L2）</li><li>核化（Kernel SVM）</li><li>集成化（AdaBoost，RandForest）</li><li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li></ul></li><li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））<ul><li>有向图：Bayesian Network (Deep Directed Network)<ul><li>Sigmoid Belief Network</li><li>Variational Autoencoder (VAE)</li><li>GAN</li></ul></li><li>无向图：Markov Network (Deep Boltzmann Network )</li><li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li><li>统称 Deep Generative Model（但深层很难计算）</li><li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li></ul></li><li>Data<ul><li>X: data, X={x~1~, x~2~, …, x~n~}^T^  Dimension(N, P)</li><li>θ: parameter,   X~p(X|θ)</li></ul></li><li><strong>频率派</strong><ul><li>θ为未知常数；X为随机变量</li><li>$loss = P(X|θ) = \prod_{i=1}^n P(x_i|θ)$<ul><li>x~1~, x~2~, …, x~n~之间独立同分布</li></ul></li><li>Maximum Likelihood Estimation 极大似然估计<ul><li>$θ<em>{MLE}= argmax</em>θ \ log^{P(X|θ)}$</li></ul></li></ul></li><li><strong>贝叶斯派</strong><ul><li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li><li>posterior probability 后验概率<ul><li>$P(θ|X) = \frac{P(X, θ)}{P(X)} = \frac{P(X|θ)P(θ)}{P(X)} = \frac{likelihood*prior}{\int<em>θ P(X|θ) d</em>θ}$</li></ul></li><li>Maximum A Posterior Probability 最大后验概率<ul><li>找到θ在分布中最大值点</li><li>$θ<em>{MLE} = argmax</em>θ \ P(X|θ) = argmax_θ \ P(X|θ)P(θ)$</li><li>P(θ|X)其分母是不变的，则θ~MLE~与分子成正比，只需要算分子</li></ul></li><li>贝叶斯估计<ul><li>$P(θ|X) = \frac{P(X|θ)P(θ)}{\int<em>θ P(X|θ) d</em>θ}$</li><li>必须完整计算整个分子式</li></ul></li><li>贝叶斯预测<ul><li>$X (train), \hat{X} (test),  X → θ → \hat{X}$</li><li>训练数据通过学习参数θ，与测试数据关联</li><li>$P(\hat{X}|X) = \int<em>θ P(\hat{X}, θ|X) d</em>θ = \int<em>θ P(\hat{X}|X)P(θ|X) d</em>θ$</li></ul></li></ul></li></ul><h5 id="Two-Linear-Regression"><a href="#Two-Linear-Regression" class="headerlink" title="Two. Linear Regression"></a>Two. Linear Regression</h5><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p><p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线$f(w) = w^T x + b$（偏置b可先忽略）</p><ul><li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）<ul><li>线性（属性线性、全局线性、系数线性）<ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li><li>系数非线性：神经网络（感知机）</li></ul></li><li>全局性<ul><li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li></ul></li><li>数据未加工<ul><li>预处理：PCA，流行</li></ul></li></ul></li><li><strong>矩阵表达</strong><ul><li>Least Squares 最小二乘估计法（最小平方法）<ul><li>$\mathbf{L(w) = \sum ||w^T x_i - y_i||^2} = \sum (w^T x_i - y_i)^2 \ = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y$</li></ul></li><li>$\mathbf{\hat{w} = argmin \ L(w)}$</li><li>求导$ \frac{dL}{dw} =  2 X^T X w - 2 X^T Y=0 → X^T X w = X^T Y$<ul><li>$\hat{w} = (X^T X)^{-1} X^T Y$ (伪逆：$(X^T X)^{-1} X^T$)</li></ul></li><li>$x_3$的误差为$w^T x_3 - y_3$，即所有误差分成一小段一小段</li></ul></li><li><p><strong>几何意义</strong></p><ul><li>$f(w) = w^T x \Leftrightarrow f(β) = x^T β$</li><li>可以将数据X看成p维的空间，Y是不在该p维空间内</li><li><p>目标：在p维空间中找到一条直线$f(β)$离Y最近，即Y在p维空间的投影</p><ul><li><p>若向量a与向量b垂直，则 $a^T b = 0$</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li><li><p>虚线为$Y - Xβ$ 与$X$的p维空间垂直，$X^T (Y-Xβ) = 0 → β = (X^T X)^{-1} X^T Y$</p></li><li>误差分散在p个维度上</li></ul></li></ul></li><li><p><strong>概率角度</strong></p><ul><li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li><li>数据本身会带有噪声 ε~N(0, σ^2^)</li><li>$y = f(w) + ε = w^T x + ε$<ul><li>$y|x,w$ ~$N(w^T x, σ^2) \Leftrightarrow P(y|x,w) =\frac{1}{\sqrt{2\pi}σ} e^{-\frac{(y - w^T x)^2}{2*σ^2}}$</li></ul></li><li>定义log-likelihood： <ul><li>$L_{MLE} (w) = log^{P(y|x,w)} = log^{\prod P(y_i|x_i,w)} = \sum log^{P(y_i|x_i,w)} \ = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} + log^{e^{-\frac{(y - w^T x)^2}{2σ^2}}}] = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} -\frac{(y - w^T x)^2}{2σ^2}]$</li><li>样本之间独立同分布 </li><li>$\hat{w} = argmax \ L_{MLE} (w) = argmax \ \frac{(y - w^T x)^2}{2σ^2} = argmin \ (y_i - w^T x_i)^2$</li><li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li></ul></li></ul></li><li><p><strong>Regularization 正则化</strong> </p><ul><li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong><ul><li>最直接是加样本数据</li><li>降维or特征选择or特征提取 (PCA)</li><li>正则化（损失函数加个约束）：$argmin [L(w)+λP(w)]$</li></ul></li><li>L1 -&gt; Lasso<ul><li>$P(w) = ||w||_1$</li></ul></li><li>L2 -&gt; Ridge 岭回归<ul><li>$P(w) = ||w||_2 = w^T w$</li><li>权值衰减</li><li>$J(w) = \sum||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w \ = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y$<ul><li>$\hat{w} = argmin \ J(w)$</li><li>$\frac{dJ}{dw} = 2 (X^T X + λ I)w - 2 X^T Y = 0, \hat{w} = (X^T X + λ I)^{-1} X^T Y$</li><li>$X^T X$ 是半正定矩阵+对角矩阵=$(X^T X + λI)$正定矩阵，必然<strong>可逆</strong></li></ul></li></ul></li><li>贝叶斯的角度<ul><li>参数w服从分布，$w$~$N(0,σ_0^2) → P(w) = \frac{1}{\sqrt{2\pi}σ_0} e^{-\frac{||w||^2}{2σ_0^2}}$</li><li>$P(w|y) = \frac{P(y|w)P(w)}{P(y)}$</li><li>MAP: $\hat{w} = argmax \ P(w|y) = argmax \ P(y|w)P(w) = argmax \ log^{P(y|w)P(w)} \ = argmax \ log^{\frac{1}{2\piσ_0σ} e^{(-\frac{(y_i - w^T x_i)^2}{2σ^2} -\frac{||w||^2}{2σ_0^2})}} = argmin [(y_i - w^T x_i)^2 + \frac{σ^2}{σ_0^2}||w||^2] = argmin [L(w)+λP(w)]$</li><li>$λ = \frac{σ^2}{σ_0^2}$</li><li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li></ul></li></ul></li></ul><h5 id="Three-Linear-Classification"><a href="#Three-Linear-Classification" class="headerlink" title="Three. Linear Classification"></a>Three. Linear Classification</h5><ul><li><font color="red">线性回归------>激活函数，降维------->线性分类</font></li><li>硬分类：0/1<ul><li>线性判别分析 (Fisher)</li><li>感知机</li></ul></li><li>软分类：[0,1]区间内概率<ul><li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li><li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li></ul></li><li><p><strong>Perceptron 感知机</strong> (1958年)</p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p><ul><li>判别模型</li><li>样本：{(x~i~, y~i~)}, N个</li><li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li><li>模型：$f(x) = sign(w^T x + b)$ （$w^T x$大于等于0表示为1（分类正确），反之为-1（分类错误））</li><li>策略：loss function（被错误分类的样本个数）<ul><li>$L(w) = \sum I{y_i * (w^T x_i) &lt; 0}$ （非连续函数，不可导）</li><li>$L(w) = \sum-y_i w^T x_i,\frac{dL}{dw} = -y_i x_i$</li></ul></li><li>若是非线性可分，可是使用pocket algorithm</li><li>从感知机到深度学习（发展历程）<ul><li>1958年提出PLA</li><li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li><li>1981年提出MLP（多层感知机），FeedForward Neural Network</li><li>1986年BP+MLP，RNN</li><li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li><li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li><li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li><li>2009年GPU发展，2011年speech，2012年ImageNet</li><li>2013年Variational Autoencoder (VAE)</li><li>2014年GAN</li><li>2016年Alpha Go</li><li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li><li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li></ul></li></ul></li><li><p><strong>线性判别分析</strong></p><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p><ul><li>样本：N个p维样本，二分类(+1,-1)，正样本个数$N1$，均值$X<em>{c1}$，方差$S</em>{c1}$，负样本个数$N<em>{2}$，均值$X</em>{c2}$，方差$S<em>{c2}$，（$X</em>{c1} = \frac{1}{N<em>1} \sum x_i,S</em>{c1} = \frac{1}{N<em>1} \sum (x_i - X</em>{c1})(x<em>i - X</em>{c1})^T$）</li><li>思想：类内小，类间大<ul><li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li><li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li><li>Z平面的法向量为最后找到的分类函数 $w^T x$（因为垂直，则Z平面即w向量）<ul><li>（前提设置$||w||=1$）</li><li>则样本点投影到Z平面为：$|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i$</li></ul></li></ul></li><li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong> <ul><li>$N_1 = \frac{1}{N_1} \sum w^T x_i$</li><li>$S_1 = \frac{1}{N_1} \sum (w^T x_i - Z_1) (w^T x_i - Z_1)^T$</li><li>类间：$(Z_1-Z_2)^2$，类内：$S_1+S_2$</li></ul></li><li>策略：$L(w) = \frac{(Z<em>1-Z_2)^2}{(S_1+S_2)}= \frac{w^T (X</em>{c1} - X<em>{c2})(X</em>{c1} - X<em>{c2})^T w}{w^T(S</em>{c1}+ S_{c2}) w }$<ul><li>分子 =$ [w^T (\frac{1}{N<em>1} \sum x_i - \frac{1}{N_2} \sum x_i)]^2= [w^T (X</em>{c1} - X<em>{c2})]^2 = w^T (X</em>{c1} - X<em>{c2})(X</em>{c1} - X_{c2})^T w$</li><li>分母 =  $w^T S<em>{c1} w +  w^T S</em>{c2} w =  w^T (S<em>{c1}+ S</em>{c2}) w$<ul><li>$S<em>1 =  \frac{1}{N_1}  \sum (w^T x_i - \frac{1}{N_1} \sum w^T x_j)(w^T x_i - \frac{1}{N_1} \sum w^T x_j)^T \ = w^T [\frac{1}{N_1}\sum(x_i - X</em>{c1})(x<em>i - X</em>{c1})^T] w = w^T S_{c1} w$</li></ul></li><li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li><li>$L(w) = \frac{w^T S_b w}{w^T S_w w}, \hat{w} = argmax \ L(w)$<ul><li>$\frac{dL}{dw} = 2S_b w (w^T S_w w)^{-1} + (w^T S_b w) <em> -(w^T S_w w)^{-2} </em> 2 * S_w w = 0$</li><li>$S_b w (w^T S_w w) = (w^T S_b w) S_w w$ （$(w^T S_w w)$ 最终计算得一个实数，一维，没有方向）（求解$\hat{w}$关心的是方向，因为平面的大小可以缩放，所以意义不大）</li><li>$w = (w^T S<em>w w)/(w^T S_b w)S_w^{-1}S_b w$，正比于$(S_w^{-1}S_b w)$ ，正比于$(S_w^{-1}(X</em>{c1} - X_{c2}))$</li><li>（$S<em>b w = (X</em>{c1} - X<em>{c2})(X</em>{c1} - X<em>{c2})^T w，(X</em>{c1} - X_{c2})^T w$为实数）</li><li>（若$S<em>w$是对角矩阵，各向同性，$S_w$正比于单位矩阵，则$w$正比于$(X</em>{c1} - X_{c2})$</li></ul></li></ul></li><li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li></ul></li><li><p><strong>Logistic Regression</strong></p><ul><li><font color="red">线性回归------>sigmoid------->线性分类</font></li><li>判别模型</li><li>model<ul><li>$sigmoid(x) = \frac{1}{1+e^{-x}}$，将$w^T x$映射到处于[0,1]区间的概率值p</li><li>$p_1 = P(y=1|x) = sigmoid(w^T x) = \frac{1}{1+e^{w^T x}}$</li><li>$p_0 = P(y=0|x) = sigmoid(w^T x) = \frac{e^{w^T x}}{1+e^{w^T x}}$</li><li>综合表达：$P(y|x) = p_1^y * p_0^{1-y}$</li></ul></li><li>$\hat{w} = argmax \ P(Y|X) = argmax \ log^{\prod P(y_i|x_i)} = argmax \ \sum log^{P(y_i|x_i)} \= argmax \ \sum[y_i<em>log^{p_1} + (1-y_i)</em>log^{p_0}] \Leftrightarrow (-cross entropy)$<ul><li>MLE &lt;=&gt; loss function (min cross entropy)</li></ul></li></ul></li><li><strong>Gaussian Discriminant Analysis</strong><ul><li>生成模型、连续<ul><li>$\hat{y} = argmax \ P(y|x) = argmax \ P(x|y)P(y)$</li><li>分类：最终比较$P(y=0|x)，P(y=1|x)$大小</li><li>$P(y|x)$正比于$P(x|y)P(y)$，即联合概率$P(x, y)$</li></ul></li><li>Data：N个d维样本，二分类(0,1)，正样本个数$N_1$，方差$S_1$，负样本个数$N_2$，方差$S_2$</li><li><strong>prior probability</strong> <ul><li>先验概率服从伯努利分布</li><li>y ~ Bernoulli，$P(y=1) = p，P(y=0) = 1-p$</li><li>$P(y) = p^y(1-p)^{1-y}$</li></ul></li><li><strong>conditional probability</strong><ul><li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li><li>x|y=1 ~ N(u~1~, σ^2^)</li><li>x|y=0 ~ N(u~2~, σ^2^)</li><li>方差一样（权值共享），均值不一样</li><li>$P(x|y) = N(u_1, σ^2)^y * N(u_2, σ^2)^{1-y}$</li></ul></li><li><strong>loss function</strong><ul><li>$log^{MLE} → L(θ) =  log^{\prod P(x_i, y_i)} = \sum log^{P(x_i|y_i)P(y_i)} = \sum [log^{P(x_i|y_i)} + log^{P(y_i)}] \ =  \sum [log^{N(u_1, σ^2)^{y_i} <em> N(u_2, σ^2)^{1-y_i}} + log^{p^{y_i}</em>(1-p)^{1-y_i}}] \ = \sum[y_ilog^{N(u_1, σ^2)} + (1-y_i)log^{N(u_2, σ^2)} + y_ilog^p + (1-y_i)log^{1-p}]$</li><li>$θ = (u_1, u_2, σ, p)$</li><li>$\hat{θ} = argmax \ L(θ)$</li><li>求解4个参数<ul><li>$p$<ul><li>相关部分 $L = \sum [y_ilog^p + (1-y_i)log^{1-p}]$</li><li>$\frac{dL}{dp} = \sum [\frac{y_i}{p} - \frac{1-y_i}{1-p}] = 0 → \sum [y_i(1-p)- (1-y_i)p] = \sum(y_i - p) = 0$</li><li>$\hat{p} = \frac{1}{N} \sum y_i  = \frac{N_1}{N}$（二分类（0,1），$\sum y_i = N_1$）</li></ul></li><li>$u_1$ （同理 $u_2$）<ul><li>相关部分 $L = \sum y_ilog^{N(u_1, σ^2)} = \sum y_ilog^{\frac{1}{(2\pi)^{\frac{d}{2}}σ^{\frac{1}{2}}} e^{\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ}}}$</li><li>$u_1 = argmax L ∝ argmax \sum y_i\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ} = argmax \frac{-1}{2} \sum y_i[(x_i-u_1)^T(x_i-u_1)σ^{-1}] \ = argmax \frac{-1}{2} \sum y_i[x_i^Tσ^{-1}x_i-2u_1^Tσ^{-1}x_i+u_1^Tσ^{-1}u_1]$</li><li>$\frac{dL}{du_1} = \frac{-1}{2}\sum y_i[-2σ^{-1} x_i + 2σ^{-1} u_1] = 0 → \sum y_i(u_1 - x_i) = 0$</li><li>$u_1 = \frac{\sum y_i x_i}{\sum y_i} = \frac{\sum y_i x_i}{N_1}$</li></ul></li><li>σ<ul><li>相关部分$L = \sum [y_ilog^{N(u_1, σ^2)}+(1-y_i)log^{N(u_2, σ^2)}] = \sum [log^{N(u_1, σ^2)}+log^{N(u_2, σ^2)}]$<ul><li>（二分类，非0即1，可以拆分算，可以省去$y_i$）</li></ul></li><li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li><li>$σ = \frac{1}{N} (N_1S_1 + N_2S_2)$</li></ul></li></ul></li></ul></li></ul></li><li><p><strong>Naive Bayes</strong></p><ul><li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong><ul><li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li><li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li><li>最简单概率图模型</li></ul></li><li>生成模型、离散</li><li>Data<ul><li>X: data, (n, d), n个数据样本，每个d维向量</li><li>Y: class, Y={c~1~, c~2~, …,c~k~}, k个类别</li><li>y: label, (1, n), n个标签</li></ul></li><li><strong>prior probability</strong> <ul><li>P(Y=c~k~)</li><li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li><li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li><li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li><li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li><li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li></ul></li><li><strong>conditional probability</strong><ul><li>$P(X=x|Y=c<em>k) = P(X^{(1)}=x^{(1)}, …, X^{(d)}=x^{(d)}|Y=c_k) = \prod</em>{j}^{d} P(X^{(j)}=x^{(j)}|Y=c_k)$</li><li>特征条件独立（上标表示第j-th维度）</li></ul></li><li><strong>joint probability distributions</strong><ul><li>联合概率分布</li><li>$P(X, Y) = P(X|Y)P(Y)$</li></ul></li><li><strong>posterior probability</strong><ul><li>$P(Y=c<em>k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum P(X=x|Y=c_k)P(Y=c_k)} = \frac{P(Y=c_k)\prod</em>{j}^{d}P(X^{(j)}=x^{(j)}|Y=c<em>k)}{\sum P(Y=c_k)\prod</em>{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}$</li><li>则分类器为<ul><li>$y=f(x) = argmax \ P(Y=c<em>k|X=x) ∝  argmax \ P(Y=c_k)\prod</em>{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)$</li><li>分母不变，则仅与分子成正比</li><li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li><li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li></ul></li></ul></li><li><strong>loss function</strong><ul><li>最大后验概率转-&gt;期望风险最小化</li><li>$L(Y, f(x)) = \left{\begin{matrix}<br> 1 \ if \ Y!= f(x)\<br> 0 \ if \ Y=f(x)<br>\end{matrix}\right.$<ul><li>Y: train label, y=f(x) : predict label</li></ul></li></ul></li><li>期望风险函数：$R_{exp(f)} = E[L(Y, f(x))]$<ul><li>根据联合概率分布：$R_{exp(f)} = E_x \sum [L(c_k|f(x))]P(c_k|X)$</li></ul></li><li>$f(x) = argmin \ Σ[L(c_k|f(x))]P(c_k|X)$<ul><li>根据L(Y, f(x))函数展开，消去Y=f(x)项</li><li>$f(x) = argmin \ \sum P(y \neq c_k|X=x) = argmin \ (1-P(y=c_k|X=x)) = argmax \ P(y=c_k|X=x)$<ul><li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c~k~类别的最大概率为多少）</li></ul></li></ul></li><li>详情案例见统计学习方法(第二版)63页</li><li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Statistics-for-Machine-Learning&quot;&gt;&lt;a href=&quot;#Statistics-for-Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;Statistics for Machine Learnin
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Bayes_MRF</title>
    <link href="http://yoursite.com/2020/08/12/Bayes-MRF/"/>
    <id>http://yoursite.com/2020/08/12/Bayes-MRF/</id>
    <published>2020-08-12T12:10:46.000Z</published>
    <updated>2020-10-07T02:51:13.878Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Bayes-Network贝叶斯网络-amp-Markov-Random-Fields-马尔可夫随机场"><a href="#Bayes-Network贝叶斯网络-amp-Markov-Random-Fields-马尔可夫随机场" class="headerlink" title="Bayes Network贝叶斯网络 &amp; Markov Random Fields 马尔可夫随机场"></a>Bayes Network贝叶斯网络 &amp; Markov Random Fields 马尔可夫随机场</h4><h5 id="One-前提"><a href="#One-前提" class="headerlink" title="One. 前提"></a>One. 前提</h5><ul><li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）<ul><li>Representation 表示<ul><li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li><li>无向图 Markov Network (Markov Random Fields) (无向，则结点的概率仅取决于1阶邻居)</li><li>高斯图 （连续）<ul><li>Gassian Bayes Network</li><li>Gassian Markov Network</li></ul></li></ul></li><li>Inference 推断<ul><li>精确推断<ul><li>Variable Elimination, Belief Propagation, Junction Tree Algorithm</li></ul></li><li>近似推断<ul><li>确定性近似（变分推断）</li><li>随机性近似（蒙特卡洛，MCMC）</li></ul></li></ul></li><li>Learning 学习<ul><li>参数学习<ul><li>完备数据（非隐变量）（有向，无向）</li><li>隐变量（EM）</li></ul></li><li>结构学习 (学习更好的图结构，参数)</li></ul></li></ul></li><li><strong>高维随机变量</strong> P(x~1~, x~2~, …, x~p~) 计算量太大<ul><li>边缘概率 P(x~i~)</li><li>条件概率 P(x~j~|x~i~)</li></ul></li><li><strong>运算原则</strong><ul><li>sum rule: $P(x<em>{1}) = \int</em>{x<em>{2}} P(x</em>{1}, x<em>{2}) d</em>{x_{2}}$ (边缘概率)</li><li>poduct rule: $P(x<em>{1}, x</em>{2}) = P(x<em>{1})*P(x</em>{2}|x<em>{1}) = P(x</em>{2})*P(x<em>{1}|x</em>{2})$</li><li>chain rule: $P(x<em>{1}, x</em>{2}, …, x<em>{p}) = \prod</em>{i=1} P(x<em>{i}|x</em>{1}, x<em>{2}, …, x</em>{i-1})$</li><li>bayesian rule: $P(x<em>{2}|x</em>{1}) = \frac{P(x<em>{1}, x</em>{2})}{P(x<em>{1}) }= \frac{P(x</em>{2})*P(x<em>{1}|x</em>{2}) }{\int P(x<em>{1}, x</em>{2}) d{x_{2}}}$</li></ul></li><li><strong>缺点</strong><ul><li><font color="red">高维复杂 P(x~1~, x~2~, ..., x~p~) 计算量太大</font></li><li>简化 <ul><li>每个<strong>维度之间相互独立</strong>    (特性太强了)<ul><li>$P(x<em>{1}, x</em>{2}, …, x<em>{p}) = \prod</em>{i=1} P(x_{i})$</li><li>Naive Bayes 朴素贝叶斯 $P(x|y) = \prod<em>{i} P(x</em>{i}|y)$</li></ul></li><li>Markov Property 马尔可夫特性<ul><li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li><li>x~i+1~ 只与 x~i~相关，与其他 x~i-1~,…,x~1~无关</li></ul></li><li><strong>条件独立性</strong>（可降低计算复杂度）<ul><li>给定x~B~情况下，集合x~A~与集合x~C~无关 （x~A~，x~B~，x~C~无交集）</li><li>x~B~ 只与x~C~相关<br><a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a>    <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li></ul></li></ul></li></ul></li></ul><h5 id="Two-Bayes"><a href="#Two-Bayes" class="headerlink" title="Two. Bayes"></a>Two. Bayes</h5><ul><li>链式法则 $P(x<em>{1}, x</em>{2}, …, x<em>{p}) = \prod</em>{i=1} P(x<em>{i}|x</em>{1}, x<em>{2}, …, x</em>{i-1})$</li><li><p>因子分解 $P(x<em>{1}, x</em>{2}, …, x<em>{p}) = \prod</em>{i=1} P(x<em>{i}|x</em>{p(i)})$（x~p(i)~为x~i~父亲集合，即指向x~i~的结点）（条件独立性）<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p></li><li><p><strong>tail to tail</strong></p><ul><li>因子分解 -&gt; $P(A,B,C) = P(A)P(B|A)P(C|A)$</li><li>链式法则 -&gt; $P(A,B,C) = P(A)P(B|A)P(C|A,B)$<ul><li>则 $P(C|A) = P(C|A,B)$（A，B同时发生时，不影响C），则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，B,C相互独立，“倒V路径”</font>）</li></ul></li><li>条件独立性：$P(B|A)P(C|A) = P(B|A)P(C|A,B) = P(B,C|A)$</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p><ul><li><p><strong>head to tail</strong></p><ul><li>因子分解 -&gt; $P(A,B,C) = P(A)P(B|A)P(C|B)$</li><li>链式法则 -&gt; $P(A,B,C) = P(A)P(B|A)P(C|A,B)$</li><li>发生B时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞，A,C相互独立</font>）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p></li><li><p><strong>head to head</strong></p><ul><li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A、B有关系，不独立则难以分解</font>）</li><li>因子分解 -&gt; $P(A,B,C) = P(A)P(B)P(C|A,B)$ （父亲结点先于子结点）</li><li>链式法则 -&gt; $P(A,B,C) = P(A)P(B|A)P(C|A,B)$<ul><li>$P(B) = P(B|A)$，则默认情况下，C还没被观察，A,B相互独立</li></ul></li><li>这个模式是想判断 $P(A|C) == P(A|C,B)$，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p><ul><li>（<font color="red">若D被观测，路径也是是连通，A、B有关系</font>）</li></ul></li><li><p>有向图是的条件独立性（证明发生x~B~, x~A~, x~C~相互独立）</p><ul><li><p>D-separation</p><ul><li><p>x~A~, x~B~, x~C~ 三个集合两两无交集<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p></li><li><p>若A与C，存在B~1~，B~2~关系，且属于x~B~集合；存在B~3~，B~4~关系，且不属于x~B~集合</p></li><li>则符合：发生x~B~时，存在上述情况， x~A~, x~C~相互独立 （<strong>全局马尔可夫性</strong>）</li><li><p>$P(x<em>{i}|x</em>{-i}) = \frac{P(x<em>{i}, x</em>{-i})}{P(x<em>{-i})} = \frac{p(x)}{\int P(x</em>{i}) d{x<em>{i}}} = \frac{\prod</em>{j} P(x<em>{j}|x</em>{p(j)})}{\int \prod<em>{j} P(x</em>{j}|x<em>{p(j)})d</em>{x_{i}}}$</p><ul><li>x~-i~表示集合{x~1~,…x~p~}中去除x~i~, <strong>x/x~i~</strong></li><li>$\prod<em>{j} P(x</em>{j}|x_{p(j)})$ 分为与x~i~有关和无关两部分</li><li><p>则 $\frac{\prod<em>{j} P(x</em>{j}|x<em>{p(j)})}{\int \prod</em>{j} P(x<em>{j}|x</em>{p(j)})d<em>{x</em>{i}}}$无关部分则可以约去</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"></p></li><li><p>$P(x<em>{i}, x</em>{-i}) = \prod<em>{j} P(x</em>{j}|x_{p(j)})$，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket</p></li><li>一个人与全世界的关系=一个人与身边人的关系</li></ul></li></ul></li></ul></li><li><p>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）</p><ul><li><p>离散</p><ul><li><p>单一</p><ul><li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; $P(x|y) = \prod<em>{i} P(x</em>{i}|y=1)$  (x 是p维， 当y被观测时，x各维度相互独立)</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p></li></ul></li><li><p>混合</p><ul><li><p><strong>GMM</strong> 高斯混合模型（多个高斯分布） -&gt; 做聚类</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/GMM.png" alt="GMM" style="zoom:75%;"></p></li></ul></li><li>时间<ul><li><strong>Markov Chain</strong> 马尔可夫链</li><li><strong>Gaussian Process</strong> 无限维高斯分布</li></ul></li><li>动态模型 = 混合 + 时间<ul><li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li><li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li><li><strong>Partide Filter</strong> （非连续，非高斯）</li></ul></li></ul></li><li><p>连续</p><ul><li><strong>Gaussian Bayes Network</strong> 高斯图</li></ul></li></ul></li></ul><h5 id="Three-MRF"><a href="#Three-MRF" class="headerlink" title="Three. MRF"></a>Three. MRF</h5><ul><li>无向图</li><li><p>条件独立性，发生x~B~时，x~A~与x~C~无关 （<strong>global markov</strong> 全局马尔可夫）</p><ul><li>存在集合x~A~, x~C~被x~B~分割（对应 bayes D-separation）， 那么发生x~B~时，x~A~与x~C~无关 </li></ul></li><li><p><strong>local markov</strong> 局部马尔可夫</p><ul><li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p></li></ul></li><li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p><ul><li>在集合x~-i,-j~(集合没有i，j结点), 对于任意两个点x~i~, x~j~没有直接连接，则相互独立, (i ≠ j)</li></ul></li><li>条件独立性体现的三个方面，并且相互等价，可以互推 global markov &lt;=&gt; local markov &lt;=&gt; pair markov &lt;=&gt;  基于最大团的因子分解<ul><li>clique团，最大团<ul><li>集合间的结点相互联通</li><li>在一个团无法添加结点，则是最大团</li><li>(c~1~, c~2~,…表示团)</li></ul></li></ul></li><li><p>因子分解 $P(x) = \frac{1}{Z} \prod<em>{i}^{p} Φ(x</em>{c<em>{i}}),Z = Σ</em>{x<em>{1}}…Σ</em>{x<em>{p}}\prod</em>{i}^{p}Φ(x<em>{c</em>{i}})$ （用因子分解证明条件独立性）</p><ul><li>Φ 势函数， 必须为正（大于0）<ul><li>$Φ (x<em>{c</em>{i}}) = e^{-E(x<em>{c</em>{i}})}$   (E为能量函数，也叫势函数)</li><li>Φ (x~ci~)  &lt;-&gt; P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</li><li>$P(x) = \frac{1}{Z} \prod<em>{i}^{p} Φ(x</em>{c<em>{i}}) = \frac{1}{Z} \prod</em>{i}^{p} e^{-E(x<em>{c</em>{i}})} = \frac{1}{Z} e^{-\sum E(x<em>{c</em>{i}})}$ </li><li>Gibbs Distribution是统计物理的名称，与指数族分布形式一样（俩个等价）</li><li>最大熵原理：在满足已知事实，最终推出分布属于指数族分布 </li><li>结论：Gibbs Distribution &lt;=&gt; Markov Random Field</li></ul></li><li>c~i~最大团，x~ci~最大团随机变量集合</li><li>Z 为联合概率分布的归一化因子</li><li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li><li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li><li>Pair-MRF 因子分解：$P(x) = \frac{1}{Z} \prod<em>{i} Φ(x</em>{i}) \prod<em>{j} Φ(x</em>{i}, x_{j})$   (考虑边)</li><li><p>最大后验概率推理（图像分割问题）：max~x~ P(x)</p><ul><li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"></p></li><li><p>假设$θ(x<em>{i}) = -logΦ(x</em>{i}),θ(x<em>{i}, x</em>{j}) = -logΦ(x<em>{i},x</em>{j})$</p></li><li><p>$max<em>{x} P(x)$ -&gt; 能量最小化 -&gt; $min</em>{x} E(x) = \sum θ(x<em>{i})  + \sum θ(x</em>{i}, x_{j})$</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"></p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p></li><li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"></p></li><li><p>结点的设置势函数，一个前景一个背景</p><p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"></p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Bayes-Network贝叶斯网络-amp-Markov-Random-Fields-马尔可夫随机场&quot;&gt;&lt;a href=&quot;#Bayes-Network贝叶斯网络-amp-Markov-Random-Fields-马尔可夫随机场&quot; class=&quot;headerlin
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>HMM_CRF</title>
    <link href="http://yoursite.com/2020/08/09/HMM-CRF/"/>
    <id>http://yoursite.com/2020/08/09/HMM-CRF/</id>
    <published>2020-08-09T12:29:28.000Z</published>
    <updated>2020-10-06T13:11:42.336Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场"><a href="#Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场" class="headerlink" title="Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场"></a>Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场</h4><h5 id="One-Time-Sequence-Series"><a href="#One-Time-Sequence-Series" class="headerlink" title="One. Time Sequence / Series"></a>One. Time Sequence / Series</h5><ul><li>在时间序列中，起伏状态可以被观测到（股票走势就是一种时间序列，股票中的涨跌）</li><li>无法观测到的是时间序列的<strong>隐状态</strong>（股票中是否处于牛市状态，这类的就是隐状态）<ul><li>隐状态会有很多</li><li>当知道隐状态存在时，每个隐状态被观测时，都是相互独立（互不干扰）</li><li>隐状态之间是离散的</li></ul></li></ul><h5 id="Two-HMM"><a href="#Two-HMM" class="headerlink" title="Two. HMM"></a>Two. HMM</h5><ul><li>在概率中，隐马尔可夫模型对应所有状态是相互独立的</li><li><strong>P(q_t|q_t-1)</strong>  Transition Probability 转移概率<ul><li>P(q_t|q_t-1, q_t-2,…, q_1) = P(q_t|q_t-1)<ul><li>每个状态只取决于前面一个状态，而非前面所有状态</li><li>即当前隐状态到下一个隐状态的概率</li></ul></li><li>Discrete 离散的（用矩阵表示 A，维度(k, k)，k是开个隐状态）</li></ul></li><li><strong>P(y_t|q_t)</strong>  Emmission/Measurement Probability 发射概率 / 观测概率<ul><li>P(y_t|q_1,…, q_t-1, q_t, y_1,…, y_t-1) = P(y_t|q_t)<ul><li>已知当前状态 q_t，得到事实变化 y_t 的概率</li></ul></li><li>Discrete or Continuous</li><li>若是离散时，可以用矩阵表示 B，维度(k, L)，L是y的取值范围（也是离散的）</li><li>若是连续的，无法使用矩阵表示，y可能是连续的一个分布</li></ul></li><li>两个概率决定了整个隐马尔可夫模型 </li><li><p>在信号中用HMM也很多，时间轴每段范围的信号就相当与一个y_t<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm.png" alt="hmm" style="zoom:67%;"></p></li><li><p>每个隐状态的概率和为1，则矩阵行和为1</p></li><li>P(X) = ∫_y P(X, Y) dy </li><li><strong>P(y_1, y_2, y_3)</strong> = Σq_1 Σq_2 Σq_3 P(y_1, y_2, y_3, q_1, q_2, q_3) = Σq_1 Σq_2 Σq_3 <strong>P(y_3|q_3)P(q_3|q_2)*P(y_2|q_2)P(q_2|q_1)*P(y_1|q_1)P(q_1)</strong> <ul><li>P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|y_1, y_2, q_1, q_2, q_3)*P(y_1, y_2, q_1, q_2, q_3)</li><li>利用马尔可夫性质：P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|q_3)*P(q_3|y_1, y_2, q_1, q_2)*P(y_1, y_2, q_1, q_2) = P(y_3|q_3)P(q_3|q_2)*P(y_1, y_2, q_1, q_2)</li><li>P(y_3|q_3), P(q_3|q_2), P(y_2|q_2), P(q_2|q_1), P(y_1|q_1) 在马尔可夫参数A, B矩阵中可得</li><li><strong>P(q_1) 是初始状态</strong>，这需要问题给出</li><li>则马尔可夫模型需要三个参数，λ={A, B, P(q_1)} (假设y是离散的)</li></ul></li><li>马尔可夫模型有什么用？<ul><li>找50人讲10个单词（动物名字）</li><li><strong>λ_cat = argmax_λ log P(y_1, …, y_50|λ)</strong> <ul><li>当初始条件为λ，最大为说cat的情况</li><li>记录所有单词的λ</li><li>用高斯或者高斯混合模型计算λ</li></ul></li><li>当来了个新人，说其中一个单词，那么他说哪个单词概率最大？<ul><li><strong>P(y_new|λ_cat)</strong>, … 概率最大为结果</li><li>若所有该概率计算结果很小时，说明新人说的不是原10个单词</li></ul></li></ul></li><li><p>公式范化（计算量较大）<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm_function.png" alt="hmm_function" style="zoom: 67%;"></p><ul><li><p>一个定义<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB.png" alt="FB" style="zoom:67%;"></p></li><li><p>优化计算量</p><ul><li>alpha_i(t) = P(y_1, …, y_t, q_t=i)</li><li>alpha_i(1) = P(y_1, q_1=i) = P(y_i|q_1=i)P(q1) = b_i(y_1) P(q_1) </li><li>alpha_j(2) = P(y_1, y_2, q_2=j) = Σq_1P(y_1, y_2, q_1=i, q_2=j) = Σq_1 P(y_2|q_2=j)P(q_2=j|q_1=i)*P(y_1, q_1=i) = P(y_2|q_2=j) Σq_1 P(q_2=j|q_1=i)*alpha_i(1) = b_j(y_1) Σq_1 a_ij*alpha_i(1)</li><li>开始递归 alpha_j(t) = b_j(y_t) Σq_1 a_ij*alpha_i(t-1) = P(y_1, …, y_t, q_t=j) <ul><li><strong>P(y_1, …, y_t) = Σj P(y_1, …, y_t, q_t=j) = Σj alpha_j(t)</strong></li></ul></li><li>通过贝叶斯公式<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB_2.png" alt="FB_2" style="zoom:67%;"><ul><li>P(Y, q_t=i|λ) = P(Y, q_t=i)P(q_t=i) = P(y_1, …, y_t|q_t=i)P(y_t+1, …, y_T|q_t=i)P(q_t=i) =  P(y_1, …, y_t, q_t=i)P(y_t+1, …, y_T|q_t=i) = alpha_i(t) beta_i(t)</li></ul></li></ul></li></ul></li><li>如何学习HMM的参数<ul><li>λ = argmax_λ log P(Y|λ) （极大似然估计）</li><li>用EM学习<ul><li>E-step: Σq_1… Σq_t log(P(Y, Q))*P(Q,Y|θ^g) = Σq_1… Σq_t log(P(q_1)*ΠP(q_t|q_t-1)*ΠP(y_t|q_t)) <em> P(Q,Y|θ^g) = Σq_1… Σq_t [log(P(q_1)+Σlog(a_q_t-1,q_t)+Σlog(b_q_t(y_t))] </em> P(Q,Y|θ^g)</li><li>part 5,6还没看完(需要EM基础)</li></ul></li></ul></li></ul><p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLFze15KrfxbGPEHyjxddbbxVvLa5kilFf" target="_blank" rel="noopener">徐亦达系列</a> <a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/dynamic_model.pdf" target="_blank" rel="noopener">pdf</a></p><h5 id="Three-CRF"><a href="#Three-CRF" class="headerlink" title="Three. CRF"></a>Three. CRF</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场&quot;&gt;&lt;a href=&quot;#Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spectral_Cluster</title>
    <link href="http://yoursite.com/2020/07/25/Spectral-Cluster/"/>
    <id>http://yoursite.com/2020/07/25/Spectral-Cluster/</id>
    <published>2020-07-25T05:19:54.000Z</published>
    <updated>2020-07-27T09:32:54.190Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spectral-Cluster-谱聚类"><a href="#Spectral-Cluster-谱聚类" class="headerlink" title="Spectral Cluster 谱聚类"></a>Spectral Cluster 谱聚类</h4><h5 id="一、分类与聚类"><a href="#一、分类与聚类" class="headerlink" title="一、分类与聚类"></a>一、分类与聚类</h5><p>1、分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类别标号y中。</p><p>2、聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。目的是个簇内的元素之间越相似，簇间的相似度越小。</p><h5 id="二、k-means-与-spectral-cluster"><a href="#二、k-means-与-spectral-cluster" class="headerlink" title="二、k-means 与 spectral cluster"></a>二、k-means 与 spectral cluster</h5><p>k-means 每次选择k个中心点，将每个数据点归类到离它最近的那个中心点所代表的簇中，迭代多次，一直到迭代了最大的步数或者前后 <strong>J</strong> 的值相差小于一个阈值为止。(u_k为中心点，r_nk 为数据点 x_n 被归类到 cluster k 的时候为 1 ，否则为 0 )<br>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/kmean.png" alt="kmean"></p><ul><li>结点与附近的结点相似度会更高，距离远的结点相似度更低；因此，对角线上颜色更亮，右上角左下角的区域更暗 </li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/affinity_matrix.png" alt="Similarity Matrix" style="zoom: 25%;"></p><p>然而，k-means初始的中心点是随机选的，每次选择结果不同，大部分情况结果还是令人满意，偶尔也会陷入局部最优。传统 k-means的x_n输入是每个结点的所有信息（即完整的N维度信息）。</p><p>spectral cluster 谱聚类只需要结点之间的相似度矩阵即可，并不需要结点的完整信息，因此不必像k-means那样要求N维的欧氏空间的向量。即抓住了结点之间的主要信息，排除了冗余信息，计算复杂度也更小，所以比传统聚类方法会更加健壮一些。</p><h5 id="三、谱的涵义"><a href="#三、谱的涵义" class="headerlink" title="三、谱的涵义"></a>三、谱的涵义</h5><p>对于谱的概念，简而言之，可以把谱认定为对一个信号（视频，音频，图像，图）分解成为一些简单的元素线性组合（小波基，图基）。为了使得这种分解更加有意义，可以使得这些分解的元素之间是线性无关的（正交的），也就是说这些分解的简单元素可以看作是信号的基。面对研究的东西，往往都会细分的更细小层面才能挖掘更主要的信息，犹如研究生物，则要细分到细胞、基因层面；研究物理，则要细分到质子、夸克层面；目前深度学习研究图片也是细分到像素层级，音频则细分到音频、音位层级。因此针对图也是如此。</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/spectrum.png" alt="spectrum" style="zoom: 50%;"></p><p>在信号处理中，谱就是傅立叶变换，它提供了不同频率下的正弦和余弦波作为基，将信号在这些基进行分解。</p><p>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/fourier.png" alt="fourier" style="zoom: 67%;"></p><p>在图中，“谱”则是指对图的拉普拉斯矩阵的特征分解，特征分解后正交化的特征向量就是对应的正交基，所有的特征值的全体统称为拉普拉斯矩阵的谱，最后则可以用特征向量和特征值来表示图的信息。</p><h5 id="四、Graph-与-Laplacian"><a href="#四、Graph-与-Laplacian" class="headerlink" title="四、Graph 与 Laplacian"></a>四、Graph 与 Laplacian</h5><ul><li><p>Graph G = (V, E, X)</p><p>结构信息：V 表示结点集，E 表示结点之间的边集，A 表示N维的邻接矩阵（若是无向图，则邻接矩阵内元素是0/1; 若是有向图，则邻接矩阵内元素的值是具体权重值w），D表示N维的度矩阵（对应结点的拥有邻居结点数，放置在矩阵对角线上）。</p></li></ul><p>​        特征信息：X表示结点的N维度信息。</p><ul><li><p><strong>为什么特征分解最终选择拉普拉斯矩阵而不是相似矩阵？</strong></p><p>因为拉普拉斯矩阵的独有属性，它是半正定矩阵，则特征值都是大于或等于0, 可以有多个0特征值，每个0特征值对应特征向量上的值大于0对应的节点之间具有连通性，对应一个子图。拉普拉斯矩阵拥有相似矩阵（邻接矩阵）的特性，又拥有独有特性，则更好表达图的信息。</p></li><li><p><strong>为什么需要使用归一化后的拉普拉斯矩阵？</strong></p><p>聚类的目的为两点：第一点是最小化簇间的相似度，第二点是最大化簇内相似度。未归一化的拉普拉斯矩阵仅能达到第一点，归一化的拉普拉斯矩阵    能达到两点要求。(具体如何达到两点的推导，见文章<strong><code>A Tutorial on Spectral Clustering</code></strong>)</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/laplacian.png" alt="Laplacain Matrix" style="zoom: 25%;"></p></li><li><p>特征分解</p><p>求解得到特征值、特征向量，小到大排序特征值，对应特征向量也排序，则最终得到选择前k个最小特征值对应的特征向量。</p><ul><li>在Eigenvector Matrix，第0列是特征值为0对应的特征向量，是个全1列向量（颜色相同）（因为第二步构造的是全连接矩阵，则仅有一个特征值为0）</li><li>第1列对应较亮色块是值大于0的情况，暗色块值小于0，因此较亮色块中对应结点属于一个簇；第2列则是黄色块部分属于一个簇，等。依次类推即可。</li><li>中间图为Eigenvector Matrix 通过t-SNE降到2维的结果，能将不同簇的结点都明显区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接。</li><li>右间图为Eigenvector Matrix 通过t-SNE降到3维的结果，在3D空间领域，也能将不同簇的结点区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接，黄色与棕色有一两个点连接。</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector.png" alt="Eigenvector Matrix" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_2d.png" alt="Eigenvector Matrix 2D" style="zoom: 25%;"><br><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_3d.png" alt="Eigenvector Matrix 3D" style="zoom: 50%;"></p><p>  即spectral cluster可以看作为node feature高维matrix data (n,d) 经过一个<a href="http://blog.pluskid.org/?p=290" target="_blank" rel="noopener">laplace mapping</a>降维得到一个低维embedding  (n, k)，embedding中融入更主要的信息，舍弃冗余信息。        </p><h5 id="五、Spectral-Clustering"><a href="#五、Spectral-Clustering" class="headerlink" title="五、Spectral Clustering"></a>五、Spectral Clustering</h5><p>​    获得embedding即执行k-means</p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/orgin.png" alt="原数据图" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/k_means.png" alt="直接 k-means 的结果" style="zoom:25%;"></p><p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/SpectralClustering.png" alt="谱聚类结果" style="zoom:25%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Spectral-Cluster-谱聚类&quot;&gt;&lt;a href=&quot;#Spectral-Cluster-谱聚类&quot; class=&quot;headerlink&quot; title=&quot;Spectral Cluster 谱聚类&quot;&gt;&lt;/a&gt;Spectral Cluster 谱聚类&lt;/h4&gt;&lt;
      
    
    </summary>
    
    
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>machine_learning</title>
    <link href="http://yoursite.com/2020/05/06/machine-learning/"/>
    <id>http://yoursite.com/2020/05/06/machine-learning/</id>
    <published>2020-05-06T12:20:41.000Z</published>
    <updated>2020-05-06T12:30:41.607Z</updated>
    
    <content type="html"><![CDATA[<h3 id="机器学习-machine-learning-from-TJU"><a href="#机器学习-machine-learning-from-TJU" class="headerlink" title="机器学习 machine learning from TJU"></a>机器学习 machine learning from TJU</h3><h4 id="One-绪论"><a href="#One-绪论" class="headerlink" title="One. 绪论"></a>One. 绪论</h4><ol><li><p>什么是智能？</p><ul><li><strong>Self-adaption 自适应</strong> （迁移学习，通用AI模型 ( Artificial General Intelligence, 即strong AI)）</li><li><strong>Self-consciousness 自我意识</strong> （模糊决策）</li><li>运算智能：快速计算，存储</li><li>感知智能：人类五官的能力（视觉、听觉、触觉等）【已解决】</li><li>认知智能：大脑的能力（逻辑推理、知识理解、决策思考）<strong>（语言处理）</strong>（概念、意识、观念）（理解、思考、决策）【正在解决】</li></ul></li><li><p><strong>Turning Test</strong> 图灵测试：正常人分别和正常人、AI聊天，是否能分清人与AI（是否有用是哲学问题，AI是否伪装，从而不通过Turning Test）</p><p><strong>Behaviorism 行为主义</strong>，仅看行为是否符合智能，不管内部部分（有漏洞）</p><p><strong>Connectionism联结主义</strong>，只看内部构造（用神经网络模拟），符合大脑构造，则认为有智能（婴儿无法通过Turning test，但他结构是符合的）（但无法知道大脑构造，如何产生意识？）</p><p>模拟鸟的飞行，制造飞机（虽然达不到鸟内部的全部飞行系统，但能模拟飞行）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/turning test.png" alt="turning test" style="zoom: 33%;"></p></li><li><p>如何制造 AI？</p><ul><li><strong>“Thinking” by “Searching”</strong>：“思考”即“搜索” (Behaviorism)，类似搜索引擎，信息检索，在已有知识寻找最佳答案（大脑积累知识，面对问题，就是搜索大脑已有知识（但无法确定大脑是如何搜索的））<ul><li>Knowledge Graph 知识图谱</li></ul></li><li><strong>“Learning”</strong>：学习知识，发现新的知识<ul><li>什么是新的知识?（知识-&gt;模式-&gt;稳定的关联关系）</li><li><strong>pattern</strong> 模式识别（机器学习的前身）（语言的语法是一种模式，物理规律也是模式）</li><li>模式识别和机器学习的区别在于：前者喂给机器的是各种特征描述，从而让机器对未知的事物进行判断；后者喂给机器的是某一事物的海量样本，让机器通过样本来自己发现特征，最后去判断某些未知的事物。（机器学习在挖掘数据最终找到模式） （模式识别=数据挖掘）</li></ul></li><li><strong>“Thinking” by “Learning”</strong>：“思考”即“学习”，Known Data-&gt;Model-&gt;Unknown Data<ul><li>Model 模型就是对模式的大概猜测</li><li>y=f(x), f()就是模式</li><li>由于模式很多种，模型则用 y = ax+b 去猜测，算法则调整a和b参数</li><li>机器学习就是科学研究的自动化（确定变量-&gt;做实验-&gt;找到变量之间规律）</li></ul></li></ul></li><li><p>机器学习的基本框架</p><ul><li>一系列可能函数 &amp; 训练数据 -&gt; 通过算法 -&gt; 选出最好的函数</li><li><strong>Supervise Learning</strong> 监督学习，模型只需要找到输入和标记之间的关联关系（标记是人工的，不是自己手标的，就是已标好数据（这种要成本））</li><li><strong>Semi-Supervised Learning</strong> 半监督学习，少量部分样本标记的监督学习</li><li><strong>UnSupervise Learning</strong> 无监督学习，无标记，模型自己总结出类别（聚类）</li><li><strong>Reinforcement Learning</strong> 强化学习，利用间接”标记“来学习<ul><li>围棋的”输赢“，样本是棋局，直接标记是棋子在哪个位置是最好的（但没有这种标记，没有人知道哪里是最好的），间接标记是这个棋局是黑白输赢结果</li><li>online learning or 反复学习</li><li>输出是有反馈，对模型进行奖励机制</li></ul></li><li>基于规则的模型：人定义”特征“，人定义特征和输出之间的关系</li><li>基于统计的模型：人定义”特征“，模型确定特征和输出之间的关系（特征工程）<ul><li>cat？= 0.1*毛色+0.2*耳朵形状+0.3*眼睛形状 …</li></ul></li><li>深度学习模型：人不定义”特征“，模型确定原始信息和输出之间的关系（可以达到 end-to-end model）（人类选择的特征未必是最好的）<ul><li>深度学习可以发现特征（通过是神经网络学习原始信息获得高阶特征，一些人类未必发现的特征）</li><li>越深越能发现复杂特征</li></ul></li></ul></li><li><p>AI的一些重要问题</p><ul><li><p>标记、model、feature</p></li><li><p>什么是好模型？</p><ul><li><p>（泛化能力）描述性，但难以具体化，不可计算（欠拟合Underfitting）</p></li><li><p>（性能）具体可计算，适合范围小，描述性差（过拟合<strong>Overfitting</strong>）（模型复杂性越高容易过拟合）（难以避免）</p></li><li><p>两者折中比较难</p></li><li><p>机器学习的最终目标是在未知数据上效果最好</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/overfitting.png" alt="overfitting" style="zoom: 50%;"></p></li><li><p>严格上，数据需要分 训练集，开发集，测试集（避免虚假结果）</p></li><li><p>”成功就是最大的失败“（越成功会越保守，而事物是发展的，会在新事物上会越失败）</p></li><li><p>面对企业：</p><ul><li>基于规则的模型：问题简单，大量已知知识（可解释性好；基于人归纳，会比较抽象，越抽象越鲁棒性）</li><li>基于统计的模型：数据量不大，有一些明确的特征（可解释性一般，但都能猜到；基于数据归纳，不够抽象）</li><li>深度学习模型：数据量<strong>大</strong>，算力高，没有明确特征，先验知识缺乏（黑箱子，可解释性<strong>差</strong>）</li></ul></li><li><p>面对科研：越复杂越好</p></li></ul></li><li><p><strong>Global Knownledge 世界知识</strong>（人工智能选特征选模型，仍需要辅助的知识（经验知识，常识））</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/global_knownledge.png" alt="global knownledge" style="zoom: 50%;"></p><ul><li>人的学习不需要太多数据样本（小样本学习）</li><li>人可以小样本学习，也是有经验知识（先验知识），但儿童学习语言无法解释（儿童无先验知识，”大脑有普遍语法存在“）</li><li>有了先验知识，就可以对训练样本要求少一些（小样本学习），只学习特殊的知识即可。</li></ul></li><li><p><strong>Explainable</strong> 可解释性</p><ul><li><p>为什么模型会这么决策？</p></li><li><p>有了可解释性，可追溯源头，从根本上改进它。</p></li><li><p>自动驾驶事故率低于人类司机，为什么我们却不信任它？</p><p>（因为自动驾驶出事故的原因不可解释，出事故是概率性的）</p></li><li><p>刷脸支付，出错也是不可解释的</p></li></ul></li><li><p><strong>Ethics</strong> 伦理问题</p><ul><li><p>若有意识的机器人是否拥有人权？</p></li><li><p>AI通过用户的非隐私数据获得隐私数据</p><p>（识别用户的性格或者需求，左右用户做出选择（尤其在选举或者个性化推荐））</p><p>（搜索引擎（<strong>主动获取信息</strong>）是用户信息入口，会影响国家整体发展）</p><p>（现在约50%信息是依照个性推荐（<strong>被动获取信息</strong>），会导致个性分化、信息茧房，会加大偏见和隔阂，局限在自己圈子，最终导致社会撕裂，不接受他人，无法全面认识世界）</p></li><li><p>若AI能预测一个人的犯罪概率，是否在犯罪前先控制他？</p></li></ul></li></ul></li></ol><h4 id="Two-Machine-Learning-Foundations-from-台大林軒田"><a href="#Two-Machine-Learning-Foundations-from-台大林軒田" class="headerlink" title="Two. Machine Learning Foundations from 台大林軒田"></a>Two. Machine Learning Foundations from 台大林軒田</h4><ol><li><p><strong>machine learning = sample data + blurry pattern + not easily programmable definition</strong></p></li><li><p>Data Mining -&gt; feature -&gt; Machine Learning (在机器学习选择特征时，尽量选取特征之间相关性小的特征，相关性越大，则越冗余，就没意义了)</p></li><li><p>Machine Learning use data to compute hypothesis g that approximates target f. (Machine Learning ∈ Statitics)</p></li><li><p><strong>Perceptron 感知器</strong>，h(x) = sign(Σwi xi - threshold) (i =1,2,…)</p><ul><li><p>模拟神经细胞，接收信号，整合起来（<strong>加权求和</strong>），接收整体的信号超过某个阈值，则激活神经细胞</p></li><li><p>将threshold融入权重w，作为w0，h(x) = sign(Σwi xi)  (i =0,1,2,…) = sign(w^T x)，大于0为正例，小于0为负例。（线性感知器）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Perceptron.png" alt="Perceptron" style="zoom:33%;"></p><ul><li><p>令 h(x)=0， x2为纵轴，x1为横轴，x2 = -w1/w2 * x1 - w0/w2，右图分类效果较好（w1比w2大，即x1特征比x2特征更重要）</p></li><li><p>PLA  Perceptron Learning Algorithm 寻找最优划分的线性函数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_1.png" alt="update weight" style="zoom:43%;"></p></li><li><p>前提是线性可分的，则可以在有限步内停止，每次调整都会更接近完美分类面；若非线性，则无法停止。(大多数情况是非线性的，有noise；可用pocket算法，在非线性情况下，在一定调整步数下，选择错误率最低的结果)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_3.png" alt="weight update" style="zoom: 50%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_2.png" alt="PLA" style="zoom:43%;"></p></li></ul></li><li><p>perceptrons &lt;-&gt; linear (binary) classifiers</p></li></ul></li><li><p><strong>Types of Learning</strong> 机器学习的类型</p><ul><li>output space<ul><li>binary or more binary <strong>Classification</strong> （预测类别，划分样本）</li><li><strong>Regression</strong> （预测一个实数，样本的拟合问题，连接样本）<ul><li><strong>可以用回归任务做分类</strong></li><li>先算出一个数，设定阈值，然后可以分类</li></ul></li><li><strong>Structured</strong> learning (一维（序列结构，语法结构学习），二维（图结构），三维（蛋白质结构，分子结构)）</li></ul></li><li>data label<ul><li>supervised, semi-superivised, unsupervised, reinforcement</li></ul></li><li>protocol f =&gt; (x,y)<ul><li><strong>Batch</strong> Learning 批量学习（大量样本）</li><li><strong>Online</strong> Learning 在线学习（在线 =&gt; 持续学习，不断接收数据（少量），更新模型）<ul><li>Online + Batch，先大批量数据学习一个模型，再持续接收少量数据，更新模型（更新模型部分，并不是完全重新学习，否则就是多次批量学习了）</li><li>垃圾邮件分类，先训练通用的模型，根据用户个性再调整，形成个性化（每次再垃圾箱找到需要的邮件，即分错样本，就会为模型产生少量数据）（是否垃圾邮件，对每个人的意义不一样）</li><li>PLA，Reinforcement Learning</li></ul></li><li><strong>Active</strong> Learing 主动学习<ul><li>属于一种 Online Learning</li><li>同样基于少量样本调整模型，但Active Learning 模型主动向用户获取数据，Online Learning 是被动获得数据</li><li>垃圾邮件分类，若删除多个同用户的邮件，Active Learning会提问是否标记其邮件为垃圾邮件，若是，立即标记该用户为重要特征；而Online Learning则是等待用户标记垃圾邮件，需要多次标记才可以。</li></ul></li></ul></li><li>input space<ul><li><strong>concrete</strong> features 具体特征（物理意义明确）</li><li><strong>Raw</strong> features 原始特征（图片的像素，亮度，黑白）</li><li><strong>Abstract</strong> feature 抽象特征（没用任何物理意义） </li></ul></li></ul></li><li><p><strong>Feasibilityof Learning</strong> 学习的可行性</p><ul><li><p>是否可以学习知识？</p></li><li><p>训练样本是有限的，无法保证能学习到最好的 f()，只能逼近</p></li><li><p>Hoeffding’s Inequality，模型在训练样本的错误率v，模型在整体样本错误率u</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/hoeffding.png" alt="hoeffding" style="zoom:43%;"></p><ul><li>“v = u” is probably approximately correct (PAC)</li></ul></li><li><p>在训练集效果好，在测试集的效果也会好的概率？</p><ul><li><p>有M个候选函数，则错误率就很大，则过拟合。（前提是数据在候选函数之间相互独立（线性无关））</p></li><li><p>若M个候选函数中存在线性相关的候选函数，即不是<strong>相互独立</strong>，则M不是无穷大，则有希望减少过拟合</p></li><li><p>Ein 测试集错误率，Eout未知数据错误率（机器学习做两件事，模型在训练集使Ein变小，再使Ein和Eout尽可能相等）</p></li><li><p>样本N越大，则结果越可靠</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pac.png" alt="PAC" style="zoom:43%;"></p><ul><li><p>相同数据（x1 x2）喂入两个候选函数（两条红线），得到结果一样，则两个候选函数<strong>相关</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/error.png" alt="error" style="zoom:43%;"></p></li><li><p>对PLA而言，分类是候选函数将样本一分为2（则分类相同的候选函数相关，则两个函数视为等价），n个样本，最多也是2^n个<strong>类别</strong>，即<strong>M=2^n</strong>，则<strong>M不是无穷大</strong>的（对所有问题，都不是无穷大的）</p></li><li><p>实际情况是 <strong>M&lt;&lt;2^n</strong></p></li><li><p>growth function 成长函数，给定n个样本，返回实际可分类别数</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/growth_funtion.png" alt="growth_funtion" style="zoom:43%;"></p></li><li><p>问题不同，成长函数不一样（成长函数上限则为break point突破点）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different.png" alt="different" style="zoom:43%;"></p></li><li><p>机器学习要达到的目标</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Ein_Eout.png" alt="Ein_Eout" style="zoom:43%;"></p></li><li><p>参数的个数是决定模型复杂度的核心指标</p></li><li><p>自由度=这个模型的有多少参数，一个参数是一个维度，参数越多，自由度越高</p><ul><li><p>VC维=参数个数</p></li><li><p>VC维 the formal name of maximum non-break point</p></li><li><p>break point是成长函数的上限，k决定了成长函数的最多参数数量，从而决定了vc维</p></li><li><p>dvc = min_k-1</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/VC_dimension.png" alt="VC_dimension" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li></ol><ol><li><p><strong>Regression</strong> 回归</p><ul><li><p>Noise: 样本标记错误（正例标记成反例）</p></li><li><p>Probabilistic 概率函数：对输出不是确定性的，都是概率性的（容忍存在Noise）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different_error.png" alt="different_error" style="zoom:43%;"></p></li><li><p>分类：判断sample是否符合目标f()</p></li><li><p>回归：让sample离目标f()越近（error用平方，是在最低点是可微的，用绝对值是不可微的）</p><ul><li>用回归无法直接做分类，但可以缩小分类的范围，err_0/1 &lt;= err_sqr</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/logistics_regression.png" alt="logistics_regression" style="zoom:43%;"></p></li><li><p><strong>logistic regression</strong> （非线性回归）</p><ul><li>err 需要计算两个概率分布的差值（KL散度）</li><li>当数据为病人的特征数据，但患病只有0/1（患病与不患病），则需要求出整体患病的概率分布。</li><li>极大似然估计：给定输入输出，确定一个分布。</li><li>用logistics regression 训练一个分布接近极大似然估计的分布</li></ul></li><li><p><strong>Gradient Descent</strong> 梯度下降，用于update weight（随机梯度下降，是随机采样点，大方向和直接梯度下降是一致的，但复杂度翻倍）（步长=学习率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Gradient Descent.png" alt="Gradient Descent" style="zoom: 50%;"></p></li></ul></li><li><p><strong>Multiclass Classification </strong>多分类问题</p><ul><li><p>四分类拆成多个二分类问题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Multiclass Classification.png" alt="Multiclass Classification" style="zoom:50%;"></p><ul><li><p>正例大大少于负例，样本不平衡</p></li><li><p>四分类分成两类，形成一对一对的分类</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pairwise classifier.png" alt="pairwise classifier" style="zoom:43%;"></p></li></ul></li><li><p>Nonlinear Transform 训练分类</p></li><li><p>将非线性转成线性</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/nonlinear.png" alt="nonlinear" style="zoom:43%;"></p></li></ul></li><li><p><strong>Regularization</strong> 正则化</p><ul><li><p>缩小高次空间，控制在一定区间内，防止过拟合同时仍具有高次空间的能力</p></li><li><p>降低复杂度，减轻过拟合（缩减候选函数的个数M）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/regularization coefficient.png" alt="regularization coefficient" style="zoom:43%;"></p></li><li><p>可加入loss function一起训练正则化</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Regression.png" alt="Regression" style="zoom:43%;"></p></li><li><p>L1（有一堆特征，但有些是无用的，用L1可去除一些无用特征），L2（常用，比较柔和）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/l1_l2.png" alt="l1_l2" style="zoom:43%;"></p></li></ul></li></ol><h4 id="Three-NLP"><a href="#Three-NLP" class="headerlink" title="Three. NLP"></a>Three. NLP</h4><ol><li><p>what is NLP?</p><ul><li><p>Turning Test 基于 NLP</p></li><li><p>感知智能 -&gt; CV</p></li><li><p>认知智能 -&gt; NLP</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_nlp.png" alt="text_nlp" style="zoom:43%;"></p></li><li><p>语义角色标注（施动者，受动者，描述）</p></li><li><p>理解 <strong>NLU: L -&gt; R</strong>;  生成 <strong>NLG: R -&gt; L</strong></p></li><li><p>让机器get到语言中的meaning</p></li><li><p>NLP 中不允许存在歧义（程序语言没有歧义，自然语言是存在歧义的）</p><ul><li>NLP 需要解决语义之间歧义</li></ul></li><li><p>创造一个 <strong>interlingua 中间语</strong>，允许所有自然语言均可以翻译成 interlingua，自然语言是动态的，则 interlingua 几乎不可创造<em>（没有 interlingua，就很难表示 meaning）</em></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_translate.png" alt="ml_translate" style="zoom:50%;"></p></li><li><p>对自然语言做语法分析，只能越来越逼近 interlingua</p></li><li><p>在深度学习，则用 respentation / embedding 向量来表示语义的 meaning</p></li><li><p>知识图谱 -&gt; 让机器获取先验知识（前期需要NLP处理数据挖掘实体之间的关系）</p></li></ul></li><li><p>NLP’s hard point</p><ul><li><p>“哈士奇”不管在哪都是“哈士奇”；“同志”在不同语境表示不一样</p></li><li><p>歧义 &amp; 动态</p></li><li><p><strong>语言的本质是谎言</strong>。真话：能正确反映真正事实的话，谎言：不能正确表达真正事实的话（同一句话，不同人理解不一样，都带有各自的偏见，所有不能正确表达真正的事实）</p></li><li><p>符号系统：人类创造符号来表达信息，语言是其中一种。</p></li><li><p><strong>所指：meaning；能指：表达meaning的工具</strong></p><ul><li>所指，能指之间规律不可寻，具有任意性，但在特定的时间，地点可以有局部确定的规律（人类可根据古代壁画符号风格判断年份）</li></ul></li><li><p>语言的任意性所导致的歧义性、动态性，乃至非真实性是语言处理的根本性困难（非真实性：描述抽象概念，没有实体对应（白马非马））</p></li><li><p>基本歧义（语法结构、词义、词性…）</p></li><li><p>旧知识 -&gt; 先验知识 -&gt; 先验知识 + 小样本 -&gt; 新知识</p></li><li><p>乔姆斯基：存在一些普遍语法，并非局部的，是所有语言学的共性</p><ul><li>例如小孩子就可以小样本学习语言，但没有先验知识（并非多次听到语言，毕竟是教不会动物说话）</li><li>“递归”，语言存在递归结构</li><li>递归存在（语言/语义）自指结构（是产生悖论的主要原因之一）（“这句话是错的”）</li></ul></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics.png" alt="rule_statistics" style="zoom: 49%;"><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics_2.png" alt="rule_statistics" style="zoom:32%;"></p><ul><li>只需要判断<strong>特定词汇</strong>，就可以使用CNN（只需要判断句子中有“高兴”词语，就可以判定情感），但判断整句话的所有词，则需要使用RNN</li></ul></li><li><p>语言理解的关键：<strong>背景知识</strong>（上下文）</p><ul><li>在NLP中，所有词都是有歧义，必须要有一个固定场景，才能确定一个词的意思</li><li>语用即语义：词是在场景怎么用的，就是语义</li><li>例如描述人，重点在人与其他事物的关系，并不是人体内在结构</li><li>graph 是表示事物之间的关系</li><li>学习基于事物之间关联，相关性</li><li><strong>相关性</strong>恰恰是破解<strong>任意性</strong>（歧义&amp;动态）的钥匙！</li><li>NLP 用上下文约束自然语言的任意性</li></ul></li><li><p>表示学习（利用上下文表示语义）</p><ul><li>基于特征的可解释表示</li><li>基于深度学习编码的不可解释表示</li><li>与其他非语言对象相结合的表示：如网络表示学习</li><li>作为其他学习模型的输入：深度学习模型、线性学习模型</li></ul></li><li><p><strong>word embedding</strong> 词向量表示学习：<strong>基于上下文用向量表示这个词</strong>，向量则可以计算的</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_embedding.png" alt="word_embedding" style="zoom:43%;"></p><ul><li>坑：不在一个语义空间的词不能像比较；即使向量数值一样，意义不一样，词就是不一样（人名和电影不在一个空间）</li><li>两个word embedding之间差值，可以表示两者的关系</li></ul></li><li><p>预训练模型（通用知识的自动获取）：表示学习、语言模型、针对特定任务的与训练</p><ul><li><p>基于很大数据学习最基本的（几何）元素，作为其他模型的输入</p></li><li><p>属于传统机器学习（已知最基本元素，用于训练提取）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pre_training.png" alt="pre_training" style="zoom:43%;"></p></li><li><p>深度学习（黑盒子，不知道特征是否重要，用深度学习抽取特征）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DL.png" alt="DL" style="zoom:43%;"></p></li></ul></li><li><p>填补先验知识和模型能力（模型搜索空间）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_Process.png" alt="ml_Process" style="zoom:43%;"></p></li></ul></li><li><p>NLP基本任务</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/NLP_task.png" alt="NLP_task" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/essential model.png" alt="essential model" style="zoom:43%;"></p></li><li><p>NLP’s essential models (<strong>Linear Models</strong>)</p><ul><li><p><strong>EM算法</strong>（<strong>Expectation-maximization algorithm</strong> 期望最大化算法）</p><ul><li><p>“猜测隐藏在文字背后的信息”</p></li><li><p>在概率模型中寻找参数<strong>最大似然估计</strong>或者<strong>最大后验估计</strong>的算法, 其中概率模型依赖于无法观测的<strong>隐性变量</strong>。</p><ul><li>观察结果依赖于隐藏状态。只能看到观察结果,看不到隐藏状态。如何知道隐藏状态生成观察结果的概率(模型参数)?</li></ul></li><li><p>知道其中一个，可以互相推导 （有输入输出（观察值是输入），做监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM.png" alt="EM" style="zoom:43%;"></p></li><li><p>当两个都不知道（即 只有输入，没有输出，则为无监督学习）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM algorithm.png" alt="EM algorithm" style="zoom:43%;"></p></li><li><p>EM算法一定能收敛，但只能局部最优，无法全局最优，可通过尝试多个初始值（瞎猜参数）来改进最优效果</p></li></ul></li><li><p><strong>ME (Maximum Entropy) 最大熵模型</strong></p><ul><li><p>“用特征去束缚语言的任意性”</p></li><li><p>信息熵：用来描述信息的不确定性</p><ul><li><p><strong>一个物理系统越无序（无能力流动，则无序），则能量越小，信息熵越大；越有序（能力按有序方向流动），能量越大，信息熵越小</strong></p></li><li><p>一个体系的能量达到完全均匀分布时，这个系统的熵就达到最大值 </p></li><li><p>封闭系统总熵时不断增大的（能量传递完成，达到均衡），局部会出现熵减小的情况</p></li><li><p>能力来自于差异（判读是否有动能/势能/热能，对比其周围是否存在差异，有差异才存在能力流动）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Entropy.png" alt="Entropy" style="zoom:43%;"></p></li></ul></li><li><p><strong>分布均匀 &lt;=&gt; 熵最大 =&gt; 最合理结果</strong></p></li><li><p>信息熵越大信息量越大（信息的不确定性越强）（熵越大的系统承载信息的能力越大）</p></li><li><p>最大熵：保留全部的不确定性,把风险降到最小</p></li><li><p>最大熵原理指出,需要对一个随机事件的概率分布进行预测时,我们的预测应当<strong>满足全部已知的条件</strong>，而<strong>对未知的情况不要做任何主观假设</strong>。在这种情况下,概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。</p></li><li><p>条件熵</p><ul><li><p>在给定输入的情况下，计算输出概率，实际上在计算以输入为条件的条件熵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Condition Entropy.png" alt="Condition Entropy" style="zoom:43%;"></p></li></ul></li><li><p>最大熵模型：求解带约束(特征函数)的最优化问题</p><ul><li>引入拉格朗日乘子,定义拉格朗日函数,转化为特征加权和</li></ul></li></ul></li><li><p><strong>隐马尔可夫链 HMM</strong></p><ul><li><p>“语言是一个串”</p></li><li><p>一个隐状态序列产生一个观察值序列。每个隐状态依赖于前一个隐状态</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM.png" alt="HMM" style="zoom:43%;"></p><ul><li>转移概率：隐状态之间转移（转换）的概率</li><li>发射概率：隐状态产生观察值的概率</li></ul></li><li><p>HMM能解决的问题</p></li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Model.png" alt="HMM_Model" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Trasfer.png" alt="HMM_Trasfer" style="zoom:43%;"></p><ul><li>有监督的情形：知道观察序列对应的状态值，直接对训练语料进行统计计数即可，即最大似然</li><li>无监督的情形：不知道观察序列对应的状态值，只知道可能的状态集合（用EM）</li></ul></li><li><p><strong>生成与判别</strong></p><ul><li><p>“纵观全局 or 聚焦一处” 分别对应 生成 or 判别</p></li><li><p>机器学习有两大类模型：生成式模型、判别式模型</p></li><li><p>生成模型：学习得到<strong>联合概率分布P(x,y)</strong>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。</p></li><li><p>判别模型：学习得到<strong>条件概率分布P(y|x)</strong>，即在特征x出现的情况下标记y出现的概率。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/generation and discrimination.png" alt="generation and discrimination" style="zoom:43%;"></p><ul><li>已知生成模型可以得到各个判别模型；已知判别模型无法得到生成模型，除非已知所有可能的判别关系</li><li>判别模型：<ul><li>优点：所需数据量小,计算量小，对单一类别判定准确率高。可随意增加新特征。</li><li>缺点：无法全局优化，只能完成目标任务，没有提供额外信息的潜力，应用范围受限。</li></ul></li><li>生成模型：<ul><li>优点：信息全面，可实现全局优化。</li><li>缺点：所需数据量大，计算量大，增加新特征的计算成本高。</li></ul></li></ul></li><li><p>为什么HMM是生成式模型? </p><ul><li>建模所有状态之间的转移关系和状态与所有词汇的发射关系</li><li>所有隐状态概率都要计算</li></ul></li><li><p>最大熵是判别式模型</p><ul><li>给定条件，计算结果（计算条件概率）</li></ul></li></ul></li><li><p><strong>MEMM 最大熵隐马：最大熵 + HMM</strong> (偏向最大熵)</p><ul><li><p>在解决序列标注问题时，HMM的输入信息只有参数(π, A, B)和观察序列(即每个状态对应的字)，<strong>没有办法接受更丰富的特征</strong>(例如更多的上下文文字等)。为了解决这个问题，提出了MEMM模型。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM.png" alt="MEMM" style="zoom: 33%;"></p></li><li><p>MEMM模型在预测当前状态时,将前一个状态和与当前观察值相关的一组特征一起做为最大熵模型的输入,来预测当前状态。MEMM与HMM不同，是判别式模型。（有点像RNN）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM model.png" alt="MEMM model" style="zoom:43%;"></p></li><li><p>HMM计算产生整个观察序列的最优状态序列，是全局最优。</p></li><li><p>MEMM计算单个观察值判定的单个最优状态，是局部最优。</p></li></ul></li><li><p><strong>CRF (Conditional Random Field) 条件随机场 (域)</strong></p><ul><li><p>“在更加宽广的上下文上进行判别”</p></li><li><p>HMM是特殊的CRF</p></li><li><p>CRF计算由整个观察序列判定的最优状态序列，是<strong>全局最优</strong>。其中，每个可能的状态序列的概率这样计算：对于序列中的每个状态计算一组特征函数值，然后计算所有状态的特征函数值之和并归一化。</p></li><li><p>判别式模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CRF.png" alt="CRF" style="zoom: 33%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_MEMM_CRF.png" alt="HMM_MEMM_CRF" style="zoom:43%;"></p></li></ul></li><li><p><strong>SVM (support vector machine) 支持向量机</strong></p><ul><li><p>“从线性到非线性分类”</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM.png" alt="SVM" style="zoom: 33%;"></p></li><li><p>支持向量机(SVM)同最大熵一样是一种常用的线性(Log linear)分类器。</p></li><li><p>SVM求使得Margin最大的分类面,并将<strong>Margin上的向量称为支持向量</strong>（绿边上的向量点）。</p></li><li><p>通过计算样本与哪一类支持向量的内积更大来判断样本类别。</p></li><li><p>对于线性不可分的样本集,可以将其投射到高维空间中来分割。</p></li><li><p>SVM用核函数来方便计算高维空间中样本点之间的内积：核函数可以在原空间中计算高位空间中的内积。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM model.png" alt="SVM model" style="zoom:43%;"></p></li><li><p><strong>kernel function</strong> （在原维度计算kernel函数就可视为在高维做计算）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/kernel function.png" alt="kernel function" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Topic Models</strong> 主题模型</p><ul><li><p>LSA, pLSA, LDA</p></li><li><p>判断图片相似度：匹配像素</p></li><li><p>判断两篇文章相似度：匹配词汇，每篇文章词汇分布</p><ul><li><p>或者匹配词汇出现频率</p></li><li><p>下图有误，行不全是D1，是表示不同文章</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_match.png" alt="word_match" style="zoom:43%;"></p></li><li><p>词汇在文章出现，也相当于文章的上下文</p></li><li><p>避免选择类似冠词the出现频率过高，或者频率过低但又很重要的词汇</p></li><li><p>逆文档频率（term frequency-inverse document frequency, TF-IDF）</p><ul><li><p>TF-IDF 越高，则改词汇就很重要</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/TF-IDF.png" alt="TF-IDF" style="zoom:43%;"></p></li><li><p>但这个词汇向量只在乎了词语的出现频率（属于基于词袋），忽略了语义，词汇的信息（词汇序列才产生信息，与DNA碱基对序列同理）</p></li></ul></li><li><p>主题模型是基于词袋的模型</p></li><li><p>可用于检索，是否与某个‘词’相关，可以使用，但想知道与这个‘词’更细节的语义则不行</p></li><li><p><strong>潜在语义分析LSA（Latent Semantic Analysis）</strong></p><ul><li><p>主题就是一个潜在语义</p></li><li><p>n个文章 -&gt; k个主题 -&gt; m词汇</p></li><li><p>文章-主题矩阵（文章涉及主题分布），主题-词汇矩阵（主题涉及词汇分布）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSA.png" alt="LSA" style="zoom: 25%;"></p></li><li><p>拥有右边矩阵，想得到左边两个矩阵</p></li><li><p>SVD 奇异值分解（矩阵分解）（存在负值，负值不符合物理意义的解释）</p></li><li><p>LSA只是形式上拟合了文档-主题-词汇的关系,但并没有真正表达这种关系</p></li></ul></li></ul></li><li><p><strong>概率潜在语义分析 pLSA</strong></p><ul><li><p>在LSA基础上，输出概率值，才可赋予物理意义</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA.png" alt="pLSA" style="zoom:43%;"></p></li><li><p>箭头表示依赖关系</p></li><li><p>P(d) 文章被抽中的概率（属于先验概率）</p></li><li><p>P(z|d) 给定文章，主题的概率，P(w|z) 给定主题，词汇的概率（两个属于后验概率）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA model.png" alt="pLSA model" style="zoom:43%;"></p></li><li><p>求出所有边的概率，则可以得到 文本-主题矩阵，主题-词汇矩阵</p></li><li><p><strong><em>只知道观察值，想知道内部的两个隐状态，则用 EM算法</em></strong></p></li></ul></li><li><p><strong>潜在狄利克雷分配 LDA</strong>（latent Dirichlet allocation）</p><ul><li><p>pLSA中单词和主题的先验分布都假设是均匀分布的，也就是假设我们对他们的先验分布一无所知。这种假设使得pLSA比较容易出现过拟合。</p></li><li><p>文档生成话题和话题生成单词的过程是典型的多项分布,在贝叶斯学习中，狄利克雷分布常作为多项分布的先验分布使用 LDA 将狄利克雷分布做为话题和单词生成的先验分布</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA.png" alt="LDA" style="zoom:43%;"></p></li><li><p>给文章根据狄利克雷分布随机分配个主题</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA model.png" alt="LDA model" style="zoom:43%;"></p></li></ul></li></ul></li><li><p><strong>Deep Learning Models</strong> 深度学习模型</p><ul><li><p>属于生成式模型</p></li><li><p>线性模型：所有特征加权求和，通过激活函数，则成为线性感知机</p><ul><li>logistic（缩小输出范围）</li><li>softmax（所有值变为概率分布，总值=1）</li><li>KL散度（交叉熵）评估输出概率分布和真实分布的差距</li></ul></li><li><p>人工神经网络</p><ul><li>神经元激活规则<ul><li>主要是指神经元输入到输出之间的映射关系,一般为非线性函数。</li></ul></li><li>网络的拓扑结构<ul><li>不同神经元之间的连接关系。</li></ul></li><li><p>学习算法</p><ul><li>通过训练数据来学习神经网络的参数。</li></ul></li><li><p>ANN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ANN.png" alt="ANN" style="zoom: 50%;"></p></li><li><p><strong>全连接前馈神经网络</strong></p><ul><li>在前馈神经网络中,各神经元分别属于不同的层。整个网络中无反馈，信号从输入层向输出层单向传 播,可用一个有向无环图表示。</li><li>全连接复杂度高，发挥全部能力</li><li>容易过拟合（使用dropout价格低复杂度，避免过拟合）</li><li>通用近似定理<ul><li>对于具有线性输出层和至少一个使用 “挤压” 性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够,它可以以任意精度来近似任何从一个定义在实数空间中的有界闭集函数</li><li>“挤压” 性质：将输入数值范围挤压到一定的输出数值范围 </li><li>不是“挤压”性质的就是线性的</li></ul></li><li>反向传播更新参数</li></ul></li></ul></li><li><p>深度学习三个步骤</p><ul><li>定义网络 -&gt; 损失函数 -&gt; 优化</li></ul></li><li><p>梯度爆炸</p><ul><li>若初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后,可能会导致求导的结果很大，形成梯度爆炸</li></ul></li><li><p><strong>梯度消失</strong></p><ul><li><p>若使用标准化初始w，那么各个层次的相乘都是0-1之间的小数,而激活函数f的导数也是0-1之间的数,其连乘后,结果会变的很小，导致梯度消失</p></li><li><p>Activation Function</p><ul><li>sigmoid是非0均值（相当于加了一个偏置），还计算指数（指数计算相对复杂）</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/activation function.png" alt="activation function"></p></li><li><p><strong>CNN</strong></p><ul><li><p>让每个神经元不代表一个像素，而是代表一个区域，而且，区域更容易捕捉局部特征</p></li><li><p>生物学上局部感受野</p></li><li><p>结构特点：<strong>局部连接，权重共享</strong></p></li><li><p>同时使用多组卷积核,每个负责提取不同特征</p></li><li><p><strong>padding</strong> 在图片外面填充一圈0（在没有padding，则边缘像素被访问概率相对对较低）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/conv.png" alt="conv" style="zoom: 33%;"></p></li><li><p><strong>pooling 池化</strong>：卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CNN.png" alt="CNN" style="zoom:43%;"></p></li><li><p>优点：善于提取特征、适用于分类、可并行、效率高</p></li></ul></li></ul></li><li><p><strong>RNN</strong></p><ul><li><p>假设每次输入都是独立的，也就是说每次网络的输出只依赖于当前的输入</p></li><li><p>一个网络的输出做为另一个网络的输入</p><ul><li>y3是取决于前面y1, y2 (但重要程度是不一样的，越近越重要（但不是什么时候都符合这个，在序列较长时，存在远距离相关，则需要LSTM/GRU）)，但没有考虑到后者y4, y5（若需要考虑上下文，则需要双向RNN）</li><li>最后的y则包含所有信息</li><li>马尔科夫链每个状态只由前一个状态影响 而RNN每一个节点由前面所有节点影响</li></ul><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN.png" alt="RNN" style="zoom:43%;"></p></li><li><p>LSTM （长短期记忆神经网络）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p>GRU （降低复杂度，能达到LSTM的效果）</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GRU.png" alt="GRU" style="zoom:43%;"></p></li><li><p>各种类型RNN</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN model.png" alt="RNN model" style="zoom:43%;"></p></li><li><p>层叠循环神经网络：可以捕捉更加抽象的内涵</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deep RNN.png" alt="deep RNN" style="zoom:43%;"></p></li><li><p>双向循环神经网络：可以捕捉两侧的上下文信息</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/BRNN.png" alt="BRNN" style="zoom:43%;"></p></li><li><p>递归神经网络 Recursive Neural Network</p><ul><li><p>自然语言的句法结构</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Recursive NN.png" alt="Recursive NN" style="zoom:43%;"></p></li><li><p>递归神经网络实在一个有向图无循环图上共享一个组合函数</p></li><li><p>叶子节点为输入</p></li><li><p>对于有歧义的句子（句法的歧义，<strong>不知道（形容）词语指向哪个主语</strong>）或者图片（图片中<strong>物品属于哪个主人</strong>），（需要从属关系的时候）可以用递归神经网络，用树的结构区别句法的结构和物品所属</p></li><li><p>可以退化为循环神经网络（属于RNN的特例）</p></li></ul></li><li><p>优点：善于累积序列信息、适用于序列标注或编码、不可并行、效率低</p></li></ul></li><li><p><strong>Attention</strong></p><ul><li><p>基于RNN的机器翻译中的注意力现象：源语言词汇对每个目标语的依赖程度不同</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/autoenocder.png" alt="autoenocder" style="zoom:43%;"></p></li><li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/attention.png" alt="attention" style="zoom:43%;"></p></li></ul></li><li><p>different network</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different network.png" alt="different network" style="zoom:43%;"></p></li><li><p>word embedding</p><ul><li>one-hot(独热)编码<ul><li>向量维度为数据库中总词汇数,每个词向量在其对应词处取值为1，其余处为0</li><li>存在的问题: 维度灾难，语义鸿沟</li></ul></li><li><p>分布式表示 Distributed Representation</p></li><li><p>假设一个单词的语义和这个单词的上下文是相关的，我们可以使用这个单词的上下文来表示这个单词的语义信息</p></li><li><p>延申：语义相似的单词也应该具有相似的上下文。</p><ul><li><p>上下文(context): 在附近出现的所有单词的集合。—&gt; 窗口window</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DR.png" alt="DR" style="zoom:43%;"></p></li><li><p>如何训练分布式</p><ul><li><p>共现矩阵 Co-occurrence Matrix</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/COM.png" alt="COM" style="zoom:43%;"></p></li><li><p>潜在语义分析 LSA (Latent Semantic Analysis)</p></li></ul></li><li><p>奇异值分解 Singular Value Decomposition</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVD.png" alt="SVD" style="zoom:43%;"></p></li><li><p>前馈神经网络语言模型 FNNLM</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM.png" alt="FNNLM" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM_2.png" alt="FNNLM_2" style="zoom:43%;"></p></li><li><p>Word2Vec</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word2vec.png" alt="word2vec" style="zoom:43%;"></p></li><li><p>CBOW (continuous bag-of-words)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CBOW.png" alt="CBOW" style="zoom:43%;"></p></li><li><p>Skip-Gram</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/skip-gram.png" alt="skip-gram" style="zoom:43%;"></p></li></ul></li></ul></li><li><p>Pre-training 预训练 </p><ul><li>先验知识 -&gt; 学习模型 &lt;- 经验数据<ul><li>模型选择</li><li>参数设定</li><li>领域知识</li></ul></li><li>预训练模型提供了先验知识，不需要知道后面的任务目标，获得embedding</li><li>越深层，特征越具体</li><li>小样本学习 = 预训练 + 微调</li><li>目标任务与云训练模型最好是同类型的，效果会更好</li><li>RNN:不能准确捕捉远距离依赖，不能并行<ul><li>解决并行问题:给每个词编码，然后在词编码上用NN输出一个向量，NN可以并行。可以并行的网络可以做的更深</li><li>解决远距离依赖问题：给每个词编码的时候用注意力机制</li></ul></li><li>Transformer是一个典型的Encoder-Decoder模型，最初用于机器翻译。其中间部分(Encoder的输出)，是一个句子的向量表示。因此,Transformer的Encoder部分可以用作句子向量的预训练模型。</li></ul></li><li><p><strong>GNN</strong></p><ul><li><p>卷积模型、序列模型</p></li><li><p>无论卷积还是序列模型，实际上都假定输入对象的结构是一个<strong>均匀</strong>的网络。换言之，就是基本元素(像素、词汇)之间的关系结构是处处相同的。（符合欧式距离）</p></li><li><p>但是，现实中元素之间的结构并不总是均匀的。而任意图才是元素结构的一般化表示,网格与序列都只是一般图的特例</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/non_eu_and eu.png" alt="non_eu_and eu" style="zoom:43%;"></p></li><li><p>具有一般图结构的对象十分广泛，都无法用普通的CNN和RNN有效处理</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_data.png" alt="graph_data" style="zoom:43%;"></p></li><li><p>若使用基于局部特征的方法来处理，一般图，如何定义卷积核的尺寸和方法? (<strong>CNN -&gt; GCN</strong>)</p></li><li><p>若使用序列的方法来处理一般图，如何给出序列的行走路线? (<strong>RNN -&gt; deepwalk</strong>)</p></li><li><p>通过NN获得<strong>embedding</strong>：包含 feature information + structure information</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_process.png" alt="graph_process" style="zoom:43%;"></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/gnn_process.png" alt="gnn_process" style="zoom:43%;"></p></li><li><p><strong>SDNE (Structural deep network embedding)</strong></p><ul><li><p>同时优化一阶和二阶相似度</p></li><li><p>每个结点用一个自编码器来重建领域信息,从而建模二阶相似度</p></li><li><p>节点之间使用拉普拉斯特征映射(反映节点之间的距离)来惩罚使得相邻节点距离较远的编码结果</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SDNE.png" alt="SDNE" style="zoom: 50%;"></p></li></ul></li><li><p><strong>DeepWalk</strong></p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deepwalk.png" alt="deepwalk" style="zoom:43%;"></p></li><li><p><strong>Node2Vec</strong></p><ul><li><p>与DeepWalk的最大区别在于，node2vec采用有偏随机游走,在广度优先(bfs)和深度优先(dfs)图搜索之间进行权衡,从而产生比DeepWalk更高质量和更多信息量的嵌入</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/node2vec.png" alt="node2vec" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/bsf_dsf.png" alt="node2vec_" style="zoom:43%;"></p></li><li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/dsf_bsf.png" alt="dsf_bsf" style="zoom:43%;"></p></li></ul></li><li><p><strong>Metapath2vec</strong>: 异质性网络中的顶点表示</p><ul><li><p>随机路径必须符合预设的若干元路径(Metapath)</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/metapath2vec.png" alt="metapath2vec" style="zoom:43%;"></p></li></ul></li><li><p><strong>LINE</strong>: explicitly preserves both first-order and second-order proximities.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LINE.png" alt="LINE" style="zoom:43%;"></p></li><li><p><strong>PTE</strong>: learn heterogeneous text network embedding via a semi-supervised manner.</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/PTE.png" alt="{TE" style="zoom:43%;"></p></li><li><p><strong>GCN</strong></p><ul><li><p>以每个节点为核心，将其邻域设为卷积范围，卷积方法是汇聚邻居节点的信息做为核心节点的表示。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GCN.png" alt="GCN" style="zoom:43%;"></p></li></ul></li><li><p><strong>GAT</strong></p><ul><li><p>基本的GCN中邻居节点的权重是平均的。</p></li><li><p>GAT中邻居节点的权重是可以训练的参数。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GAT.png" alt="GAT" style="zoom:43%;"></p></li></ul></li><li><p>GCN和GAT 是Transductive learning: 训练语料包含待标注语料，标注在训练过程中完成。</p><ul><li>优点：质量高</li><li>缺点：扩展性差(标注新样本需要全局重新训练)</li><li>GCN和GAT的缺点：网络的任何变化都要重新进行全局训练 (类似word embedding)</li></ul></li><li><p><strong>GraphSage</strong> 是Inductive learning：训练语料不包含待标注语料，先训练获得模型,然后泛化到测试语料上。</p><ul><li><p>GraphSage学习一个由邻居节点形成中心节点表示的神经网络模型(聚合函数)</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage.png" alt="graphsage" style="zoom:43%;"></p></li><li><p>GraphSage是分层的，类似神经网络的层次。每一层的节点表示由前一层的邻居节点通过聚合函数获得。</p></li><li><p>随着层次的推进，每个结点实际上不仅可以获得邻居结点的信息，还可以获得更远距离的结点的信息。</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage2.png" alt="graphsage2" style="zoom:43%;"></p></li><li><p>GraphSage的参数学习需要设计一个损失函数。（有监督/无监督）</p></li><li><p>对于无监督学习，损失函数应该让临近的节点的拥有相似的表示。</p></li></ul></li><li><p>文本分类: <strong>Text-GCN 2019</strong></p><ul><li><p>以文档和词汇为结点构造异质性网络，训练获得文档的向量表示并分类到类别。</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_gcn.png" alt="text_gcn" style="zoom:43%;"></p></li></ul></li><li><p><strong>关系抽取: 2018</strong></p><ul><li><p>以依存句法树做为GCN的输入图,得到词汇的表示,进而分类词汇是否为关系标记词</p><p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/关系抽取.png" alt="关系抽取" style="zoom:43%;"></p></li></ul></li><li><p><strong>个性化推荐: 2018</strong></p><ul><li><p>建立用户-用户-物品关系图</p></li><li><p>在关系图上分别得到用户和物品的表示</p></li><li><p>基于用户和物品的表示建立Rating预测模型</p><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/个性化推荐.png" alt="个性化推荐" style="zoom:43%;"></p></li></ul></li></ul></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;机器学习-machine-learning-from-TJU&quot;&gt;&lt;a href=&quot;#机器学习-machine-learning-from-TJU&quot; class=&quot;headerlink&quot; title=&quot;机器学习 machine learning from TJU&quot;&gt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>RNN_LSTM</title>
    <link href="http://yoursite.com/2020/05/06/RNN-LSTM/"/>
    <id>http://yoursite.com/2020/05/06/RNN-LSTM/</id>
    <published>2020-05-06T12:11:28.000Z</published>
    <updated>2020-05-06T12:18:07.061Z</updated>
    
    <content type="html"><![CDATA[<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><ul><li><p><strong>recurrent</strong> (it performs the same function for every input)</p></li><li><p>the output of the current input depends on the <strong>past one</strong> computation </p></li><li><p>RNN can use their <strong>internal state (memory)</strong> to process sequences of inputs</p></li><li><p>In RNN, all the inputs are <strong>related</strong> to each other (In other neural networks, all the inputs are independent of each other)</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/rnn.png" alt="rnn" style="zoom:50%;"></p></li><li><p>activation function is <strong>tanh()</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/tanh.png" alt="tanh" style="zoom:43%;"></p></li><li><p>output</p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/yt.png" alt="yt" style="zoom:43%;"></p></li><li><p>Advantages</p><ul><li><strong>RNN</strong> can model sequence of data so that each sample can be assumed to be dependent on previous ones </li><li><strong>RNN</strong> are even used with convolutional layers to extend the effective pixel neighborhood. </li></ul></li><li><p>Disadvantages</p><ul><li>Gradient vanishing and exploding problems </li><li>Training an RNN is a very difficult task </li><li>It cannot process very long sequences if using <em>tanh</em> or <em>relu</em> as an activation function</li></ul></li></ul><h4 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM (Long Short-Term Memory)"></a>LSTM (Long Short-Term Memory)</h4><ul><li><p>LSTM is a modified version of RNN, which makes it easier to <strong>remember</strong> past data in memory </p></li><li><p>The <strong>vanishing gradient</strong> problem of RNN is resolved here </p></li><li><p>LSTM is well-suited to classify, process and predict <strong>time series given time lags of unknown duration</strong></p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p></li><li><p><strong>Input gate</strong> — discover which value from input should be used to modify the memory </p><ul><li><strong>Sigmoid</strong> function decides which values to let through <strong>0,1.</strong></li><li><strong>tanh</strong> function gives weightage to the values which are passed deciding their level of importance ranging from<strong>-1</strong> to <strong>1</strong> </li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/input_gate.png" alt="input_gate" style="zoom:43%;"></p></li><li><p><strong>Forget gate</strong> — discover what details to be discarded from the block</p><ul><li><strong>sigmoid</strong> function looks at the previous state(<strong>ht-1</strong>) and the content input(<strong>Xt</strong>) and outputs a number between <strong>0</strong> (<em>omit this</em>) and <strong>1</strong>(<em>keep this**</em>)<strong> for each number in the cell state </strong>Ct−1**. </li></ul><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/forget_gate.png" alt="forget_gate" style="zoom:43%;"></p></li><li><p><strong>Output gate</strong> — the input and the memory of the block is used to decide the output </p><p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/output_gate.png" alt="output_gate" style="zoom: 33%;"></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;recurrent&lt;/strong&gt; (it performs the same function
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Amino_acids_proteins</title>
    <link href="http://yoursite.com/2020/05/06/Amino-acids-proteins/"/>
    <id>http://yoursite.com/2020/05/06/Amino-acids-proteins/</id>
    <published>2020-05-06T11:17:00.000Z</published>
    <updated>2021-01-24T09:29:48.438Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Amino-acids-amp-proteins"><a href="#Amino-acids-amp-proteins" class="headerlink" title="Amino acids &amp; proteins"></a>Amino acids &amp; proteins</h4><ul><li><p><a href="https://www.bilibili.com/video/BV1aK41157GP?from=search&amp;seid=9348904521485452131" target="_blank" rel="noopener">what is protein</a></p></li><li><p>所有蛋白质由21种（基本单元）氨基酸构成</p></li><li><p>Amino acids 由 carbon (C), Oxygen (O), Hydrogen (H), Nitrogen(N), Sulfur (S) 构成</p><ul><li>硒代半胱氨酸是唯一的含有一个 Sel (硒原子) 的标准氨基酸</li></ul></li><li>Amino acids 由 Amino Group (氨基), Carboxyl Group (羧基),  Side Chain (侧链), Alpha Carbon (中央碳原子) 组成<ul><li>side chain 是不同氨基酸的唯一不同的部分，它决定了氨基酸的性质<ul><li><strong>Hydrophobic</strong> Amino acids 疏水性氨基酸具有丰富碳侧链，因此不能很好与水相互作用</li><li><strong>Hydrophilic</strong> Amino acids 亲水性（极性）氨基酸可以很好与水相互作用</li><li><strong>Charged</strong> Amino acids 带电荷的氨基酸与带相反电荷的氨基酸或其他分子相互作用</li></ul></li></ul></li><li><strong>primary structure（一级结构）</strong>是通过DNA编码的线性氨基酸序列，蛋白质中的氨基酸通过连接一个氨基酸氨基与另一个氨基酸羧基的肽键相连。每次肽键结合时都会释放一个水分子，相连的碳、氮、氧原子的序列构成了protein backbone（蛋白质的骨架）。</li><li>这些蛋白质链通常折叠成两种类型 <strong>secondary structure（二级结构）</strong><ul><li>alpha helix（螺旋）<ul><li>通过附近的氨基酸的氨基和羧基之间的 hydrogen bond (氢键) 稳定下来的右手螺旋线圈</li></ul></li><li>beta sheet（折叠）<ul><li>当两个或者多个相邻的链被氢键固定，形成 beta sheet</li></ul></li></ul></li><li><strong>tertiary structure（三级结构）</strong><ul><li>蛋白质链的三维形状</li><li>这种形状由构成链的氨基酸的性质决定</li><li>许多蛋白质形成球状，把疏水侧链包围在<strong>内部</strong>，远离周围的水</li><li>膜结合蛋白外面聚集着疏水残基，以便它们可以与膜中的脂质相互作用</li><li>带电荷的氨基酸允许蛋白质与具有互补电荷的分子相互作用</li><li>许多蛋白质的功能依赖于它们的三维形状<ul><li>血红蛋白形成一个袋状，以在中心保持血红素，一种含有铁原子的小分子，用来与氧气结合</li></ul></li></ul></li><li>quaternary structure（四级结构）<ul><li>两条或者更多条多肽链可以通过几个亚基结合在一起，形成一个功能分子</li><li>血红蛋白的四个亚基相互作用以便他们的符合物可以在肺部吸收更多氧气，并将其他释放到体内</li></ul></li><li>protein size<ul><li>大多数蛋白质小于光的波长</li><li>血红蛋白分子的尺寸约为6.5nm</li></ul></li><li>蛋白质的三维形状决定了他们的功能<ul><li>defense（防御） <ul><li>antibody 抗体灵活的手臂通过识别并与病原体结合以识别它们，作为免疫系统破坏的目标来保护我们远离疾病</li></ul></li><li>communication<ul><li>insulin 胰岛素是一种小而稳定的蛋白质可以在血液中旅行时保持形状，来调节血糖</li></ul></li><li>enzymes<ul><li>alpha 淀粉酶是一种在唾液中消化淀粉的酶</li></ul></li><li>transport<ul><li>钙泵由镁辅助并由ATP提供动力，在每次肌肉收缩后，将钙离子移动回肌浆网</li></ul></li><li>storage<ul><li>铁蛋白是一种带通道的球形蛋白质，根据有机体的需要，允许铁原子进入和退出，在铁蛋白内部形成一个空间，使铁原子附着在其内壁，铁蛋白以无毒形式储存铁。</li></ul></li><li>structure<ul><li>胶原蛋白形成强大的三重螺旋，用来在整个身体中支撑结构。胶原蛋白分子可以形成细长的原纤维，并聚集形成胶原纤维，这种类型胶原蛋白存在于皮肤和筋中</li></ul></li></ul></li><li><p>CDS (coding region 基因编码区，<a href="https://www.omicsclass.com/article/805" target="_blank" rel="noopener">Coding DNA Sequence</a>)</p><ul><li>完整一段基因，能翻译成蛋白质的区域是“间隔的、不连续的”（蛋白质编码序列和非蛋白质编码序列两部分组成）</li><li>编码序列（编码区（基因序列）中可翻译成蛋白质的序列）</li><li><p>非编码序列（编码区（基因序列）中不可翻译成蛋白质的序列）</p><p><img src="https://github.com/soloistben/images/raw/master/protein/exon_intron.jpeg" alt="exon_intron"></p><p><img src="https://github.com/soloistben/images/raw/master/protein/mRNA_protein.jpg" alt="Gene"></p></li><li><p>启动子（属于ORF的调控序列）是在DNA上，是作用于转录阶段，mRNA不包含启动子</p></li><li><font color="red">对于真核生物的大部分ORF在转录时，是包括外显子和内含子的。转录后，RNA在内含子处进行自我切割，只有外显子可以转录为成熟mRNA</font></li><li>mRNA上的CDS区域外显子的核苷酸—翻译—&gt;氨基酸</li><li><strong>CDS</strong>（mRNA）是<a href="https://weibo.com/ttarticle/p/show?id=2309404030921119537751" target="_blank" rel="noopener">mRNA上从起始密码子到终止密码子之间的RNA序列</a>（指编码一段蛋白产物的序列，是与蛋白质密码子一一对应的序列）</li><li><strong>ORF</strong>（DNA）是open reading frame的缩写，翻译成开放阅读框，基因的有意编码部分也就是开放阅读框（ORF）</li><li><strong>基因组DNA分为基因序列和非基因序列——基因序列就是一个完整的表达盒它包括ORF和ORF的调控序列——ORF转录后经加工，使得内含子被切除，外显子组成mRNA序列——mRNA包括了不能翻译的UTR序列和能翻译的CDS。</strong></li><li><p>CDS是ORF中不包含UTR的外显子部分</p><p><img src="https://github.com/soloistben/images/raw/master/protein/CDS.png" alt="CDS"></p></li></ul></li><li><p>氨基酸是蛋白质最小的结构单位</p></li><li>氨基酸+ … +氨基酸=肽</li><li>肽+ … +肽=蛋白质<ul><li>肽和氨基酸、蛋白质本就是同根生的物质，但在不同的类别中，各具特点和功能</li><li>蛋白质水解时，是相互缠绕、折叠和组合的结构首先遭到破坏，成为多肽；继续水解，肽键断裂，成为寡肽；彻底水解，就是氨基酸了</li></ul></li><li>人体吸收蛋白质主要形式是小分子活性多肽片段和游离氨基酸<ul><li>相对氨基酸的吸收，以多肽形式具有易吸收、主动吸收、优先吸收、完全吸收、可作为信使等特点</li><li>除了吸收更快以外，肽还具有的特殊的，甚至氨基酸和蛋白质都不具有的生理功能</li></ul></li><li><strong>10个以上氨基酸组成的肽被称为多肽，</strong></li><li><strong>2至9个氨基酸组成的就叫做寡肽</strong></li><li><strong>2至4个氨基酸组成的就叫做小分子肽或小肽</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Amino-acids-amp-proteins&quot;&gt;&lt;a href=&quot;#Amino-acids-amp-proteins&quot; class=&quot;headerlink&quot; title=&quot;Amino acids &amp;amp; proteins&quot;&gt;&lt;/a&gt;Amino acids 
      
    
    </summary>
    
    
    
      <category term="basic protein" scheme="http://yoursite.com/tags/basic-protein/"/>
    
  </entry>
  
  <entry>
    <title>Master_Eng</title>
    <link href="http://yoursite.com/2019/12/02/Master-Eng/"/>
    <id>http://yoursite.com/2019/12/02/Master-Eng/</id>
    <published>2019-12-02T04:51:06.000Z</published>
    <updated>2019-12-04T07:52:55.118Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Master-Eng-硕士英语"><a href="#Master-Eng-硕士英语" class="headerlink" title="Master Eng 硕士英语"></a>Master Eng 硕士英语</h4><hr><h5 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h5><ul><li>Audience 面向大部分受众</li><li>Visual Aids (PPT)</li><li>Presentor <ul><li>eye contact</li><li>story teller</li><li>body language</li><li>jargon 避免过多专业术语，用最简单的话让他人明白，必须要用时，给出解释</li><li>修辞, 自黑</li></ul></li></ul><hr><h5 id="Dissertation-硕士论文"><a href="#Dissertation-硕士论文" class="headerlink" title="Dissertation 硕士论文"></a>Dissertation 硕士论文</h5><ul><li>1.5w，40-50页</li><li>process 写论文是个过程，需要不断修改、校对（初稿改大方向，后面改小方向）</li><li>content structure<ul><li>Title Page 封面</li><li>Declaration 声明</li><li>Acknowledgement 致谢<ul><li>The acknowledgement for thesis is the section where you thank all peoplewho helped you complete the project successfully.</li></ul></li><li>Contents 目录</li><li>Abstract 摘要（350字）<ul><li>正文<ul><li>Background 背景</li><li>Methodology 方法</li><li>Result 结果</li><li>Conclusion 总结 </li></ul></li><li>Keywords (3-5 words)</li></ul></li><li>Chapter 1: Introduction 引言 (4-6 pages)</li><li>Chapter 2: Literature Review 文献综述 <ul><li>阅读大量文献，了解前沿知识，<strong>find gap (why gap? why you?)</strong></li><li>写论文可以先写这部分</li><li>占比：20/50 页</li></ul></li><li>Chapter 3: Methodology 方法论（定量Quantitative）</li><li>Chapter 4: Result 结果</li><li>Chapter 5: Discussion 讨论 <ul><li>开始先回顾Result， 后续要与Literature Revie呼应，表明填补了哪些gap</li></ul></li><li>Chapter 6: Conclusion 总结（不足之处：人力、物力、财力）</li><li><strong>Reference (APA style)</strong><ul><li>快速定位文献</li><li><strong>避免学术抄袭</strong><ul><li>paraphrasing 改写（字数一致）</li><li>summarizing 总结（缩减至1/4）</li><li>synthesizing 综合前两种</li><li>in-text 文内引用（引用多句，中间无关紧要部分可用省略号表示; 自己修改部分需要中括号括起来）</li></ul></li></ul></li></ul></li></ul><hr><h5 id="Essay-小论文"><a href="#Essay-小论文" class="headerlink" title="Essay 小论文"></a>Essay 小论文</h5><ul><li>unity: 强调一段要完整，主题鲜明</li><li>coherence 段句之间要求连贯</li><li>1段：Topic + support + conclusion</li><li>essay VS project<ul><li>same topic</li><li>same length</li><li>same structure</li><li>essay: literature review, project: do experiment</li></ul></li><li>An essay has three main parts: an introduction, a body, and a conclusion. Introduction consists of two part: general statements and <strong>thesis statement</strong> which plays specific role in the role in the essay, stating the specific topic, listing subtopics of the main topic, and indicating the pattern of organization of the essay, and writer’s position or point of view.<br>&emsp;So a good thesis statement of introduction is first step of a well-written essay, at the same time, main body follow subtopics to expand supports, and conclusion responds the main topic of thesis statement.</li></ul><hr><h5 id="CV-amp-Resume"><a href="#CV-amp-Resume" class="headerlink" title="CV &amp; Resume"></a>CV &amp; Resume</h5><ul><li>CV 个人简历-研究（学术圈），多少页都可以，详细研究经历（who r u）</li><li>Resume 个人简历-普通工作，一页，以读者角度（快速了解），内容：objective 求职意向明确（精确到职位）、Education 精炼、experience 工作经验（研究或项目）、Contact Information 个人信息</li><li>cover letter 封面信（用于申请学校，放在申请材料最上面，解释说明都有什么材料；用于求职信，3段：招聘信息来源、个人信息、很想加入）</li></ul><hr><h5 id="Interview"><a href="#Interview" class="headerlink" title="Interview"></a>Interview</h5><ul><li><strong>First impressions</strong>: we need to be punctual and neat in our appearance, greeting each person with smile.</li><li><strong>Preparation</strong>: we need to know ourselves and the company, preparing some based questions for interviewers.</li><li><strong>Ending</strong>: Asking for a business card or ensuring the information about interviewer’s name, and e-mail address, we can send a <strong>thank-you note</strong>.</li></ul><hr><h5 id="Email"><a href="#Email" class="headerlink" title="Email"></a>Email</h5><ul><li>“密送”：一般用于HR发给候选人（特例：两人之间的邮件信息，在密送中的人可以看到全部内容。遇到想否定的回复直接就不回，然后找上级汇报）</li><li>“自动转发”：工作邮箱邮件转发到私人邮箱，可防止撤回; 在私人邮箱是已读状态，工作邮箱仍是未读，因为撤回是可知道是否已读。</li><li>邮件名称就用姓名拼即可</li></ul><hr><h5 id="Documentation-or-Reference"><a href="#Documentation-or-Reference" class="headerlink" title="Documentation or Reference"></a>Documentation or Reference</h5><ul><li>APA (American Psycholopicaly Association) 用于硕士论文</li><li>MLA (Modern Language Association) 文学 （姓，年份，页码）</li><li>in-text citation（文内引用，每段引用后加括号（姓，年份））</li><li>Reference （文后引用）</li><li>姓. 名（年份）：书名（斜体）. 出版社：xxx. 页码（双写小写p）</li><li>book eg: <strong>Mills. S (2019): <em>Language</em>. London: Oup. pp16-17.</strong></li><li>journals eg: <strong>Mills. S(2019): Language. <em>Journal Name</em>. London:Oup. pp16-17. </strong></li><li>Internet eg: <strong>Mills (or Unknowner) (2019): <em>Language</em>. website. [date] </strong></li></ul><hr><h5 id="Academic-plagiarism"><a href="#Academic-plagiarism" class="headerlink" title="Academic plagiarism"></a>Academic plagiarism</h5><p>&emsp;Academics try to add original contributions to human knowledge by finding gaps in research and by studying very specific topics in detail. When you use information from an outside source without acknowledging that source, you are guilty of plagiarism. Academic plagiarism is using someone else’s words or ideas as if they were your own, which  is a serious offense, and it can result in highly negative consequences such as paper retractions and loss of author credibility and reputation.<br>&emsp;<strong>It is essential for researchers to raise their awareness of academic  plagiarism.</strong> To avoid plagiarism, you should always put quotation marks around words that you copy exactly. You do not need to use <font color="#FF0000">quotation marks</font> if you change the words. <strong>However</strong>, whether you copy the words exactly or state an idea in your own words, you must <font color="#FF0000">cite the source</font>. To cite a source means to tell where you got the information. You can insert a short reference called in-text citation in parentheses at the end of each piece of borrowed information and list describing all your sources completely in the last page. <strong>Finally</strong>, two skills to avoid  plagiarism are <font color="#FF0000">paraphrasing</font> and<font color="#FF0000">summarizing</font>. A paraphrase is typically the same length as the original text but written in your own words and not using the words from the original source. A summary is a condensed version of the original text that highlights the main or key ideas in your own words. So if you were going to summarize a chapter, it might be a page. If you were going to summarize a paragraph, it might be a couple of lines.<br>&emsp;To sum up, <strong>academic dishonesty devalues everyone else’s hard work, at the same time, critical analysis plays a significant role in academic writing.</strong> Your assignments will require you to analyze ideas from multiple sources, draw connections between them, and come to your own conclusions. Therefore, if you read the original work carefully, try to understand the main idea, take good notes, and then express it in academic writing by your own words. Being honesty!</p><hr><p><em>In school, many learning things depend on IQ, but in work, EQ + Money + Relarionship.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Master-Eng-硕士英语&quot;&gt;&lt;a href=&quot;#Master-Eng-硕士英语&quot; class=&quot;headerlink&quot; title=&quot;Master Eng 硕士英语&quot;&gt;&lt;/a&gt;Master Eng 硕士英语&lt;/h4&gt;&lt;hr&gt;
&lt;h5 id=&quot;Presenta
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>algorithm</title>
    <link href="http://yoursite.com/2019/11/14/algorithm/"/>
    <id>http://yoursite.com/2019/11/14/algorithm/</id>
    <published>2019-11-14T12:44:27.000Z</published>
    <updated>2020-01-06T07:36:20.708Z</updated>
    
    <content type="html"><![CDATA[<h4 id="LCS-Longest-Common-Subsequence"><a href="#LCS-Longest-Common-Subsequence" class="headerlink" title="LCS Longest Common Subsequence"></a>LCS Longest Common Subsequence</h4><ul><li>运用动态规划方法查找给定两个序列的最大公共子序列（子序列之间的字符可以不连续，若子串substring的字符必须连续）</li><li>由序列尾部开始：<ul><li>若两个序列尾部字符相等：LCS[i][j] = LSC[i-1][j-1] +1 </li><li>若两个序列尾部字符不相等：LCS[i][j] = max(LSC[i-1][j], LSC[i][j-1])</li></ul></li></ul><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/lcs.png" alt="LCS"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'GXTXAYB'</span></span><br><span class="line">str2 = <span class="string">'AGGTAB'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lcs</span><span class="params">(str1, str2)</span>:</span></span><br><span class="line">    L = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#print(L)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                L[i][j] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                L[i][j] = L[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                L[i][j] = max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    print(L)</span><br><span class="line">    <span class="keyword">return</span> L[<span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(lcs(str1, str2))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 0. 1. 1. 2. 2. 2.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 3.]</span><br><span class="line"> [0. 1. 1. 1. 2. 3. 4.]]</span><br><span class="line">4.0</span><br></pre></td></tr></table></figure><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/dynamic_lcs.png" alt="dynamic_lcs"></p><h6 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: LCS(A, B)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">L[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then L[i][j]=L[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span> </span><br><span class="line">        <span class="keyword">else</span> L[i][j]=max(L[i<span class="number">-1</span>][j], L[i][j<span class="number">-1</span>])</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> L</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure><hr><h4 id="SA-Sequence-Alignment"><a href="#SA-Sequence-Alignment" class="headerlink" title="SA Sequence Alignment"></a>SA Sequence Alignment</h4><ul><li>运用动态规划方法对给定两个序列做对比（原用于基因DNA链碱基的序列对比）</li><li>对比结果有三种情况：<ul><li>match 相等</li><li>unmatch 不想等</li><li>gap 缺额（两个序列长度不一致）</li></ul></li><li>使用打分原则（match不扣分，unmatch和gap扣不同的分数）</li></ul><p>eg.<br>    A G C T<br>    A U T<br>[第一A和最后T属于match，第二或第三U属于unmatch，第三或第二属于gap]<br>[若match不扣分，unmatch扣3分，gap扣2分，则本次对比=5分，扣分（惩罚分）越低越好]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">str1 = <span class="string">'AGGGCT'</span></span><br><span class="line">str2 = <span class="string">'SAGE'</span></span><br><span class="line">p_unmatch = <span class="number">3</span></span><br><span class="line">p_gap = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SA</span><span class="params">(str1, str2, p_unmatch, p_gap)</span>:</span></span><br><span class="line">    P = np.zeros((len(str1)+<span class="number">1</span>, len(str2)+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> j==<span class="number">0</span>:</span><br><span class="line">                P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">                P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> str1[i<span class="number">-1</span>] == str2[j<span class="number">-1</span>]:</span><br><span class="line">                P[i][j] = P[i<span class="number">-1</span>][j<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                P[i][j] = min(&#123;P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_unmatch, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap&#125;)</span><br><span class="line">            </span><br><span class="line">    print(P)</span><br><span class="line">    <span class="keyword">return</span> P[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">print(SA(str1, str2, p_unmatch, p_gap))</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[[ 0.  2.  4.  6.  8.]</span><br><span class="line"> [ 2.  3.  2.  4.  6.]</span><br><span class="line"> [ 4.  5.  4.  2.  4.]</span><br><span class="line"> [ 6.  7.  6.  4.  5.]</span><br><span class="line"> [ 8.  9.  8.  6.  7.]</span><br><span class="line"> [10. 11. 10.  8.  9.]</span><br><span class="line"> [12. 13. 12. 10. 11.]]</span><br><span class="line">11.0</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------</span><br><span class="line">Algorithm: SA(A, B, p_gap, p_xy)</span><br><span class="line">----------------------------------------------------</span><br><span class="line">P[<span class="number">0.</span>..|A|][<span class="number">0.</span>..|B|]</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        P[i][<span class="number">0</span>] = i*p_gap</span><br><span class="line">        P[<span class="number">0</span>][j] = j*p_gap</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">0</span> to |A|+<span class="number">1</span> do</span><br><span class="line">    <span class="keyword">for</span> j=<span class="number">0</span> to |B|+<span class="number">1</span> do </span><br><span class="line">        <span class="keyword">if</span> A[i<span class="number">-1</span>]=B[j<span class="number">-1</span>] then P[i][j]=P[i<span class="number">-1</span>][j<span class="number">-1</span>] </span><br><span class="line">        <span class="keyword">else</span> P[i][j]=min(P[i<span class="number">-1</span>][j<span class="number">-1</span>]+p_xy, P[i<span class="number">-1</span>][j]+p_gap, P[i][j<span class="number">-1</span>]+p_gap)</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">return</span> P</span><br><span class="line">----------------------------------------------------</span><br><span class="line">O(nm)</span><br></pre></td></tr></table></figure><hr><h4 id="Matroid-拟阵"><a href="#Matroid-拟阵" class="headerlink" title="Matroid 拟阵"></a>Matroid 拟阵</h4><ul><li>拟阵理论不能完全覆盖所有的贪心算法（如赫夫曼编码问题），但它可以覆盖大多数具有实际意义的情况。首先介绍拟阵的概念。（有些文献也称作矩阵胚）<br><a href="https://blog.csdn.net/ChiXueZhiHun/article/details/54808939" target="_blank" rel="noopener">Matroid</a></li></ul><hr><h4 id="Set-Cover"><a href="#Set-Cover" class="headerlink" title="Set Cover"></a>Set Cover</h4><ul><li>给定一个集合B，和一些子集sets，在子集中找到最好的覆盖原集合的子集（贪心算法）</li><li>在t时刻，选择子集设为St，当前还剩集合元素个数为nt，以选子集的元素集合为P</li><li>则在子集中，每次选择与原集合<strong>相交</strong>最多元素的子集，且符合公式|St|&gt;= nt/|P|</li></ul><h5 id="S-0-1-2-3-4-5-6-7-8-9"><a href="#S-0-1-2-3-4-5-6-7-8-9" class="headerlink" title="S = {0,1,2,3,4,5,6,7,8,9}"></a>S = {0,1,2,3,4,5,6,7,8,9}</h5><h5 id="Sets-0-1-2-3-1-4-2-2-3-5-1-7-4-6-3-6-8-7-9"><a href="#Sets-0-1-2-3-1-4-2-2-3-5-1-7-4-6-3-6-8-7-9" class="headerlink" title="Sets = {0,1}, {2,3}, {1,4,2}, {2,3,5}, {1,7}, {4,6}, {3,6,8}, {7,9}"></a>Sets = {0,1}, {2,3}, {1,4,2}, {2,3,5}, {1,7}, {4,6}, {3,6,8}, {7,9}</h5><p>Let nt be the number of uncovered elements after step t, P be optimal selection<br>Choose max (sets ∩ S) in each step:</p><ul><li>Step 1: n1 = 10, S1 = {1,4,2}, P = S1 = {1,2,4}</li><li>Step 2: n2 = 7, S2 = {3,6,8}, P = S1US2 = {1,2,3,4,6,8}</li><li>Step 3: n3 = 4, S3 = {7,9}, P = S1US2US3 = {1,2,3,4,6,7,8,9}</li><li>Step 4: n4 = 2, S4 = {0,1}, P = S1US2US3Us4 = {0,1,2,3,4,6,7,8,9}</li><li>Step 5: n5 = 1, S5 = {2,3,5}, P = S1US2US3US4US5 = {0,1,2,3,4,5,6,7,8,9} = S</li></ul><h5 id="so-cover-1-4-2-U-3-6-8-U-7-9-U-0-1-U-2-3-5-S"><a href="#so-cover-1-4-2-U-3-6-8-U-7-9-U-0-1-U-2-3-5-S" class="headerlink" title="so cover {1,4,2}U{3,6,8}U{7,9}U{0,1}U{2,3,5} = S"></a>so cover {1,4,2}U{3,6,8}U{7,9}U{0,1}U{2,3,5} = S</h5><p>set cover problem need to obey  <strong>|St+1| ≥ nt/|P| </strong></p><ul><li>if choose l S1| = 2 in step 1, n1 = 10, and n1/|P| = 10/2 = 5, so we need to choose a set containing 5 elements in step 2. That is contradictory.</li><li>if choose l S1| = 3 in step 1, n1 =10, and n1/|P| = 10/3 = 3, so we need to choose a set containing 3 elements in step 2. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">B = set([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">sets = &#123;&#125;</span><br><span class="line">sets[<span class="number">1</span>] = set([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">sets[<span class="number">2</span>] = set([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">sets[<span class="number">3</span>] = set([<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">sets[<span class="number">4</span>] = set([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">sets[<span class="number">5</span>] = set([<span class="number">1</span>,<span class="number">7</span>])</span><br><span class="line">sets[<span class="number">6</span>] = set([<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">sets[<span class="number">7</span>] = set([<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>])</span><br><span class="line">sets[<span class="number">8</span>] = set([<span class="number">7</span>,<span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">P = set()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> B:</span><br><span class="line">    best_sets = <span class="keyword">None</span></span><br><span class="line">    sets_covered = set()</span><br><span class="line">    <span class="keyword">for</span> sets_num, sets_value <span class="keyword">in</span> sets.items():</span><br><span class="line">        covered = B &amp; sets_value    <span class="comment"># 选择交集最大的set</span></span><br><span class="line">        <span class="keyword">if</span> len(covered) &gt; len(sets_covered):</span><br><span class="line">            sets_covered = covered</span><br><span class="line">            best_sets = sets_num </span><br><span class="line">    B -= sets_covered</span><br><span class="line">    print(best_sets,<span class="string">':'</span>,sets_covered)</span><br><span class="line">    P.add(best_sets)</span><br><span class="line">print(P)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3 : &#123;1, 2, 4&#125;</span><br><span class="line">7 : &#123;8, 3, 6&#125;</span><br><span class="line">8 : &#123;9, 7&#125;</span><br><span class="line">1 : &#123;0&#125;</span><br><span class="line">4 : &#123;5&#125;</span><br><span class="line">&#123;1, 3, 4, 7, 8&#125;</span><br></pre></td></tr></table></figure><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Dijkstra.png" alt="dijkstra_process"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bellman_ford.png" alt="bellman_ford"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bell.png" alt="bell"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal1.png" alt="kruskal_process"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal2.png" alt="kruskal_process"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Kruskal3.png" alt="kruskal_process"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow1.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow2.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow3.png" alt="Network_Flow"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Network_flow4.png" alt="Network_Flow"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching1.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching2.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching3.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching4.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching5.png" alt="bipartite_matching"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/bipartite_matching6.png" alt="bipartite_matching"></p><hr><p><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine1.png" alt="Turing_machine"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine2.png" alt="Turing_machine"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Turing_machine3.png" alt="Turing_machine"></p><hr><h4 id="Halting-Problem"><a href="#Halting-Problem" class="headerlink" title="Halting Problem"></a>Halting Problem</h4><ul><li><p>问题：是否存在一个过程能做这件事：该过程以一个计算机程序以及该程序的一个输入作为输入，并判断该过程在给定输入运行时是否<strong>最终</strong>能停止。</p></li><li><p>假设：存在一个这样的过程<strong>H(P,I)</strong>，P为计算机程序，I为该程序的一个输入,H可以根据P和I返回true（该过程在给定输入运行时能停止)，或者false（该过程在给定输入运行时不能停止）</p></li><li><p>结论：因此不存在这样的一个过程能解决停机问题。</p></li><li><p>停机问题的“现实”意义是说明了程序不是无所不能的</p><p><a href="https://blog.csdn.net/MyLinChi/article/details/79044156" target="_blank" rel="noopener">HP</a></p></li></ul><hr><h4 id="P-vs-NP"><a href="#P-vs-NP" class="headerlink" title="P vs NP"></a>P vs NP</h4><ul><li>P 问题：现阶段可以用计算机在多项式时间内解决的问题(<strong>P</strong>olynomial Time)</li><li>NP问题：现阶段无法在多项式时间内解决，但可以在多项式时间内验证的问题(<strong>N</strong>on-deterministic <strong>P</strong>olynomial Time)</li><li>若P=NP，则任何问题均可用计算机破解，则没有加密可言，任意生物、医疗、经济、科技问题全部可以解决。</li><li>NP-complete：很多很难的NP问题，本质上是同一个问题（卡在了相同问题上），可以用简单的<strong>多项式时间</strong>转换（数学：<strong>这些NP问题中真正困难的部分</strong>），能解决NP-complete问题就可以解决NP问题（数独游戏、蛋白质折叠（治疗癌症问题））</li><li>证明P是否等于NP，就是一个NP问题</li><li>NP-hard = NP-complete + 各种指数级别问题（下棋的计算或验证）(NP-hrad问题 是假定有无限时间和空间)<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/NP.png" alt="NP"></li><li>P-SAPCE：在无限时间，只使用多项式数量的空间，就可以求解（包括NP-complete）</li><li>BPP: 多项式时间内有几率求解问题 （BQP = BPP的量子计算）（包括P）</li><li>比NP-hrad更难的问题 = “没有任何电脑在任意时间或空间可解出来的问题”（指数层叠，多项式层叠）</li><li>所有问题 -&gt; 在给定空间、时间下，什么可以计算出来（已经不在是探讨计算的特性，而是寻找时间和空间的本身性质）<br><a href="https://www.bilibili.com/video/av19085452?from=search&amp;seid=7087981409362725035" target="_blank" rel="noopener">b站</a></li></ul><hr><h4 id="CNF-amp-SAT"><a href="#CNF-amp-SAT" class="headerlink" title="CNF &amp; SAT"></a>CNF &amp; SAT</h4><ul><li><strong>Conjunctive Normal Form (CNF)合取范式</strong> 是命题公式的一种标准形。一个命题公式的合取范式可以通过<strong>真值(TRUE)</strong>表得到，也可以通过等价变换得到。（合取范式主要用于解决命题公式的逻辑判断，一个命题的合取范式不是唯一的）</li><li><strong>Satisfaction Problem (SAT)</strong>,  decide if there is an assignment that satisfies the given CNF. SAT is NP<ul><li>TRUE, if CNF is satisfied with the certification</li><li>FALSE, if CNF is not satisfied with the certification</li></ul></li><li>给定一个布尔变量集合（只取0和1）组成SAT和子句集合CNF，是否存在一个真值赋值（SAT结果），<strong>使得CNF为真，即每个子句为真</strong><br><a href="https://blog.csdn.net/yxl564710062/article/details/79882746" target="_blank" rel="noopener">3-SAT</a></li><li><strong>CIRCUIT-SAT</strong> is NPC，若将CIRCUIT-SAT问题划分三个SAT问题，是否能证明SAT也是NPC?</li><li><strong>3-CNF Satisfaction Problem (3-SAT)</strong>: decide if there is an assignment that satisfies the given 3-CNF, in which each clause contains exactly 3 literals. </li><li><strong>NP -&gt; NPC -&gt; 3-SAT -&gt; Clique -&gt; Vertex Cover</strong>  (Using Clique To Set Variables of 3-SAT)<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/SAT.png" alt="SAT"><ul><li>only Variables disconnect its not Variables (x1 disconnect not x1)</li><li>Variables disconnect others in same group (make no sence)</li><li>connected Variables is same value<br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/SAT_Clique.png" alt="SAT_Clique"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Clique.png" alt="Clique"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Vertex_Cover.png" alt="Vertex_Cover"><br><img src="https://github.com/soloistben/images/raw/master/algorithm_image/Clique_VertexCover.png" alt="Clique_VertexCover"></li></ul></li></ul><hr><ol><li>算法中线性结构，数据元素之间存在一对一的线性关系。</li><li>算法符号，Θ：渐近紧确界，O：渐近上界，Ω：渐近下界；o：非渐近紧确的上界（O提供的渐近上界可能是也可能不是渐近紧确的），ω：非渐近紧确的下界</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;LCS-Longest-Common-Subsequence&quot;&gt;&lt;a href=&quot;#LCS-Longest-Common-Subsequence&quot; class=&quot;headerlink&quot; title=&quot;LCS Longest Common Subsequence&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>GNN</title>
    <link href="http://yoursite.com/2019/08/19/GNN/"/>
    <id>http://yoursite.com/2019/08/19/GNN/</id>
    <published>2019-08-19T05:39:28.000Z</published>
    <updated>2019-08-20T01:40:45.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications"><a href="#NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications" class="headerlink" title="NOTE of Graph Neural Networks: A Review of Methods and Applications"></a>NOTE of Graph Neural Networks: A Review of Methods and Applications</h4><ol><li>As a unique non-Euclidean data structure for machine learning, graph analysis focuses on node classification, link prediction, and clustering.</li><li>Why GNN?</li></ol><ul><li>Composed to CNN, GNN is highly expressive representations</li><li>shared weights reduce the computational cost compared with traditional spectral graph theory; multi-layer structure is the key to deal with hierarchical patterns, which captures the features of various sizes.</li><li>CNN can only operate on regular Euclidean data like images (2D grid) and text (1D sequence), GNN can.</li><li>graph embedding learns to represent graph nodes, edges or sub-graphs in low-dimensional vectors. </li><li>Deep-Walk, which is first graph embedding, is based on representation learning, word embedding and Skip-Gram model. Just like node2vec. However, <ul><li>no parameters are shared between nodes, so it means the number of parameters grows linearly with the number of nodes. </li><li>The direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs</li></ul></li></ul><ol><li>GNN model input and/or output consisting of elements and their dependency.</li><li>There isn’t a natural order of nodes in the graph. (CNN &amp; RNN can’t input node of no special order)</li><li>In the standard neural networks, the dependency information is just regarded as the feature of nodes, not edge which represents the information of dependency between two nodes in a graph!</li><li>GNN update the hidden state of nodes by a weighted sum of the states of their neighborhood.</li><li>Human brain is almost based on the graph which is extracted from daily experience. </li><li>GNN can learn the <em>reasoning</em> graph from large experimental data.</li><li><strong>Message Passing Neural Network (MPNN)</strong> could generalize several graph neural network and graph convolutional network approaches. </li><li><strong>Non-local Neural Network (NLNN)</strong> unifies several “self-attention”-style methods.</li><li>Both of MPNN&amp;NLNN focus on specific application domains and can’t provide a review over other graph attention models.</li><li><strong>Graph Network (GN) </strong>has strong capability to generalize other models. However, the graph network model is highly abstract and only gives a rough classification of the applications.</li><li>Graph neural networks suffer from over-smoothing and scaling problems. There are still no effective methods for dealing with dynamic graphs as well as modeling non-structural sensory data.</li><li>Original framework:</li></ol><ul><li>Notations<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/1.jpg" alt="chinese"><br><img src="https://github.com/soloistben/images/raw/master/gnn_image/2.png" alt="eng"></li><li>The target of GNN is to learn a state embedding <strong>h_v ∈ Rs </strong>which contains the information of neighborhood for each node.</li><li>Let <strong>f</strong> be a parametric function, called local transition function, that is shared among all nodes and updates the node state according to the input neighborhood.</li><li>Let <strong>g</strong> be the local output function that describes how the output is produced.</li><li><strong>h_v = f (x_v, x_co[v], h_ne[v], x_ne[v])  o_v = g (h_v, x_v)</strong>  (x is the features of v)</li><li><strong>Vectorize: H = F (H, X)  O = G (H, XN)</strong>  (F is global translation function, G is global output function)</li><li>The value of H is the fixed point of Eq.3 and is uniquely defined with the assumption that F is a contraction map.</li><li>GNN uses the following classic iterative scheme for computing the state.     <strong>Ht+1 = F(Ht, X)</strong></li><li>With the target information (t_v for a specific node) for the supervision. <strong>loss = ⅀ (t_i − o_i)</strong></li></ul><ol><li>Limitations:</li></ol><ul><li>GNN is inefficient to update the hidden states of nodes iteratively for the fixed point.</li><li>There are also some informative features on the edges which cannot be effectively modeled in the original GNN. </li><li>It’s unsuitable to use the fixed points if we focus on the representation of nodes instead of graphs because the distribution of representation in the fixed point will be much smooth in value and less informative for distinguishing each node.</li></ul><ol><li><strong>Variants of Graph Neural Networks</strong> to release the limitations:</li></ol><ul><li>different graph types:<ul><li>Directed Graphs. Directed edges can bring more information than undirected edges.</li><li>Heterogeneous Graphs. <ul><li>The simplest way to process heterogeneous graph is to convert the type of each node to a one-hot feature vector which is concatenated with the original feature.</li><li>For each neighbor group, GraphInception treats it as a sub-graph in a homogeneous graph to do propagation and concatenates the propagation results from different homogeneous graphs to do a collective node representation. (heterogeneous graph attention network, <strong>HAN</strong>)</li><li>[<a href="https://zhuanlan.zhihu.com/p/47040007" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47040007</a>]</li></ul></li><li>Graphs with Edge Information.<ul><li>Converting the graph to a bipartite graph where the original edges also become nodes and one original edge is split into two new edges which means there are two new edges between the edge node and begin/end nodes. (The encoder of <strong>G2S</strong> uses the following aggregation function for neighbors).</li><li>Adapting different weight matrices for the propagation on different kinds of edges. When the number of relations is very large, r-GCN introduces two kinds of regularization to reduce the number of parameters for modeling amounts of relations: basis and block diagonal-decomposition</li></ul></li><li>Dynamic Graphs. It has static graph structure and dynamic input signals.</li></ul></li><li>several modifications:<ul><li><strong>Graph Convolutional Network (GCN)</strong>: convolutions to the graph domain<ul><li>spectral approaches,使用谱分解的方法，应用图的拉普拉斯矩阵分解进行节点的信息收集     </li><li>non-spectral (spatial) approaches,直接使用图的拓扑结构，根据图的邻居信息进行信息收集</li></ul></li><li><strong>Gated graph neural network (GGNN)</strong>: using the gate mechanism like GRU or LSTM in the propagation step to diminish the restrictions in the former GNN models and improve the long-term propagation of information across the graph structure. Tree LSTM、Graph LSTM and Sentence LSTM</li><li><strong>Graph Attention Network (GAT)</strong> : incorporates the attention mechanism into the propagation step. <ul><li>GAT computes the hidden states of each node by attending over its neighbors, following a self-attention strategy.     </li><li>Gated Attention Network (GAAN) also uses the multi-head attention mechanism. However, it uses a self-attention mechanism to gather information from different heads to replace the average operation of GAT.</li></ul></li><li><strong>Residual connection</strong>: aiming to achieve better results as more layers make each node aggregate more information from neighbors, because more layers could also propagate the noisy information from an exponentially increasing number of expanded neighborhood members.<ul><li>Highway GCN</li><li>Jump Knowledge Network, selects from all of the intermediate representations (which ”jump” to the last layer) for each node at the last layer, which makes the model adapt the effective neighborhood size for each node as needed. uses three approaches of concatenation, max-pooling and LSTM-attention in the experiments to aggregate information. (The Jump Knowledge Network performs well on the experiments in social, bioinformatics and citation networks. It could also be combined with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks to improve their performance.)</li></ul></li><li><strong>Hierarchical Pooling</strong>, Complicated and large-scale graphs usually carry rich hierarchical structures which are of great importance for node-level and graph-level classification tasks</li></ul></li></ul><ol><li>Training Method:</li></ol><ul><li>Sampling<ul><li>GraphSAGE replaced full graph Laplacian in GCN with learnable aggregation functions, which are key to perform message passing and generalize to unseen nodes.  With learned aggregation and propagation functions, GraphSAGE could generate embeddings for unseen nodes.</li><li>PinSage, By simulating random walks starting from target nodes, this approach chooses the top T nodes with the highest normalized visit counts.</li><li>FastGCN, Instead of sampling neighbors for each node, FastGCN directly samples the receptive field for each layer.  </li><li>adaptive sampler could find optimal sampling importance and reduce variance simultaneously</li></ul></li><li>Receptive Field Control, a control-variate based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate. </li><li>Data Augmentation, To solve the limitations, the authors proposed Co-Training GCN and Self-Training GCN to enlarge the training dataset. </li><li>Unsupervised Training, Graph auto-encoders (GAE) aim at representing nodes into low-dimensional vectors by an unsupervised training manner.</li></ul><ol><li>Frameworks:</li></ol><ul><li><strong>Message Passing Neural Networks (MPNN)</strong> unified GNN &amp; GCN. It abstracts the commonalities between several of the most popular models for graph-structured data.<ul><li>message passing </li><li>Readout computes a feature vector for the whole graph</li></ul></li><li><strong>Non-local Neural Networks (NLNN)</strong> unified several self-attention for capturing long-range dependencies with deep neural networks. It computes the response at a position as a weighted sum of the features at all positions. List the choices for f function:<ul><li>Gaussian (natural choice) </li><li>Embedded Gaussian </li><li>Dot product </li><li>Concatenation.</li></ul></li><li><span style="border-bottom:2px dashed red;"><strong>Graph Networks (GN) </strong>unified MPNN &amp; NLNN and so on.</span><ul><li>Graph definition</li><li>GN block contains<ul><li>three “update” functions, φ_e, φ_h &amp; φ_u, </li><li>three “aggregation” functions, ρ ( The ρ functions must be invariant to permutations of their inputs and should take variable numbers of arguments)<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/3.png" alt="func"></li></ul></li><li>Computation steps<br><img src="https://github.com/soloistben/images/raw/master/gnn_image/4.png" alt="steps"></li><li>Design Principles<ul><li><strong>Flexible representations</strong><ul><li>One can simply tailor the output of a GN block according to specific demands of tasks </li><li>be applied to both structural scenarios where the graph structure is explicit and non structural scenarios where the relational structure should be inferred or assumed.</li></ul></li><li><strong>Configurable within-block structure</strong>. Based on different structure and functions settings, a variety of models (such as MPNN, NLNN and other variants) could be expressed by the GN framework.</li><li><strong>Composable multi-block architectures.</strong><ul><li>Arbitrary numbers of GN blocks could be composed in sequence with shared or unshared parameters.</li><li>utilizes GN blocks to construct an encode process decode architecture and a recurrent GN-based architecture.</li><li>Other techniques for building GN based architectures could also be useful, such as skip connections, LSTM- or GRU-style gating schemes and so on.</li></ul></li></ul></li></ul></li></ul><ol><li>Applications of GNN:</li></ol><ul><li>supervised, semi-supervised, unsupervised and reinforcement learning</li><li>Structural Scenarios<ul><li>Physics</li><li>Chemistry and Biology</li><li>Knowledge graph</li></ul></li><li>Non-Structural Scenarios<ul><li>Image<ul><li>Visual Reasoning</li><li>Semantic Segmentation</li></ul></li><li>Text<ul><li>Text classification</li><li>Sequence labeling</li><li>Neural machine translation</li><li>Relation extraction</li><li>Event extraction</li><li>Other applications</li></ul></li></ul></li><li>Other Scenarios<ul><li>Generative Models</li><li>Combinatorial Optimization</li></ul></li></ul><ol><li>Problems</li></ol><ul><li>Shallow Structure</li><li>Dynamic Graphs</li><li>Non-Structural Scenarios</li><li>Scalability </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-Applications&quot;&gt;&lt;a href=&quot;#NOTE-of-Graph-Neural-Networks-A-Review-of-Methods-and-
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
