<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Personal notes">
<meta property="og:type" content="website">
<meta property="og:title" content="MR.C">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Personal notes">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MR.C">
<meta name="twitter:description" content="Personal notes">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-EM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/EM/" class="article-date">
  <time datetime="2020-10-06T13:07:10.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/EM/">EM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/EM/" data-id="ckfy4zh1f0005zeeg7eh41dwg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hidden-Markov-Model" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/Hidden-Markov-Model/" class="article-date">
  <time datetime="2020-10-06T12:55:04.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><strong>Hidden Markov Model = Markov Random Field + Time</strong>
<ul>
<li>动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）</li>
<li><p>GMM高斯混合模型（样本之间独立同分布）；但是动态模型，样本之间不是独立同分布</p>
<p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM1.png" alt="HMM1" style="zoom: 67%;"></p></li>
<li>图中动态模型包括：横向的时间关系（Time），纵向的混合关系（Mixture，不同变量混合）</li>
<li><p><strong>若hidden variable是离散的，则动态模型为HMM</strong>；线性连续的，则是Kalmm Filter 卡尔曼滤波器；非线性连续的，则是Partide Filter</p></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM2.png" alt="HMM2" style="zoom:75%;"></p></li>
<li>HMM: param λ = (π, A, B)（π是初始概率分布、A转移矩阵、发射矩阵）
<ul>
<li><strong>observed variable O</strong>: o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>t</sub>,... → V = {v<sub>1</sub>, ...v<sub>M</sub>} （观察值，观测变量o的值域）</li>
<li><strong>hidden variable I</strong>: i<sub>1</sub>, i<sub>2</sub>, ..., i<sub>t</sub>,... → Q = {q<sub>1</sub>, ...q<sub>N</sub>} （隐变量i的值域）</li>
<li><span class="math inline">\(A = [a_{ij}], a_{ij} = P(i_{t+1}=q_{j}|i_{t}=q_{i})\)</span></li>
<li><span class="math inline">\(B = [b_{j(k)}], b_{j(k)} = P(o_{t}=v_{k}|i_{t}=q_{j})\)</span></li>
</ul></li>
<li>两个假设
<ul>
<li>齐次马尔可夫假设
<ul>
<li><span class="math inline">\(P(i_{t+1}|i_{t}, i_{t-1}, ..., i_{1}, o_{t}, o_{t-1}, ..., o_{1}) = P(i_{t+1}|i_{t})\)</span></li>
<li>隐状态i<sub>t+1</sub>只与隐状态i<sub>t</sub>有关</li>
</ul></li>
<li>观测独立假设
<ul>
<li><span class="math inline">\(P(o_{t}|i_{t}, i_{t-1}, ..., t_{1}, o_{t-1}, ..., o_{1}) = P(o_{t}|i_{t})\)</span></li>
<li>观测状态o<sub>t</sub>只与观测状态i<sub>t</sub>有关</li>
</ul></li>
</ul></li>
<li>HMM解决三个问题（已知参数 λ = (π, A, B)）
<ul>
<li>Evalution → P(O|λ) （已知参数下，求解O现象的概率）→ 前向后向算法</li>
<li>Learning → 求解参数，λ = argmax P(O|λ) → EM算法（以前是Baum Welch算法）</li>
<li>Decoding → 找到一个状态序列，使 P(I|O)达到最大（I～ = argmax P(I|O)）
<ul>
<li>预测问题：<span class="math inline">\(P(i_{t+1}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，预测下一时刻t+1的隐状态</li>
<li><strong>滤波问题</strong>：<span class="math inline">\(P(i_{t}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，求解当前时刻t的隐状态</li>
</ul></li>
</ul>
<p><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=82" target="_blank" rel="noopener">白板系列</a></p></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/Hidden-Markov-Model/" data-id="ckfy4zh1p000ezeegvjwluhrp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-PGM-Inference" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/PGM-Inference/" class="article-date">
  <time datetime="2020-10-06T07:09:49.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Inference 推断 → 求概率
<ul>
<li>P(X) = P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>)</li>
<li>边缘概率：$P(x_{i}) = Σ_{1}...Σ_{i-1} Σ_{i+1}...Σ_{p} P(X) $（除了i的都积分）</li>
<li>条件概率：<span class="math inline">\(P(x_{a}|x_{b}),(x=x_{a}\bigcup x_{b})\)</span></li>
<li>最大后验 MAP Inference：<span class="math inline">\(\hat{Z} = argmaxP(Z|X) \propto argmax P(Z,X)\)</span>（贝叶斯定理）（不一定直接求后验，只要得到最大即<span class="math inline">\(\hat{Z}\)</span>可）</li>
</ul></li>
<li>方法
<ul>
<li>精确推断
<ul>
<li>Variable Elimination (VE) 变量消除法</li>
<li><strong>Belief Propagation</strong> (BP) 信念传播（与反向传播不一样）（弥补VE缺点）
<ul>
<li>另外一个名称：Sum-Product Algorithm</li>
<li>针对树结构</li>
</ul></li>
<li>Junction Tree Algorithm
<ul>
<li>基于BP，由树结构扩展到普通图结构</li>
</ul></li>
</ul></li>
<li>近似推断
<ul>
<li>Loop Belief Propagation
<ul>
<li>基于BP，针对有环图</li>
</ul></li>
<li>Mente Carlo Inference
<ul>
<li>Importance Sampling</li>
<li>MCMC (Markov Chain Mente Carlo)</li>
<li>基于采样</li>
</ul></li>
<li>Variational Inference
<ul>
<li>确定性近似</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="variable-elimination">Variable Elimination</h5>
<p><img src="https://github.com/soloistben/images/raw/master/PGM_Inference/VE1.png" alt="VE1" style="zoom:75%;"></p>
<ul>
<li>假设a,b,c,d均为二值的随机变量{0,1}</li>
<li><span class="math inline">\(P(d) = Σ_{a,b,c} P(a,b,c,d) = Σ_{a,b,c} P(a)P(b|a)P(c|b)P(d|c)\)</span> （将各个条件概率看成因子）
<ul>
<li>→ P(a=0)P(b=0|a=0)P(c=0|b=0)P(d|c=0) + P(a=1)P(b=0|a=1)P(c=0|b=0)P(d|c=0) +...+ P(a=1)P(b=1|a=1)P(c=1|b=1)P(d|c=1) = 8*因子积</li>
<li>由于马尔可夫的性质，结点只与相邻的相关，与其他无关，可以优先将有关的先合并。</li>
<li>→ <span class="math inline">\(Σ_{b,c} P(c|b)P(d|c) Σ_{a} P(a)P(b|a)\)</span>（<span class="math inline">\(Σ_{a} P(a)P(b|a) = Σ_{a} P(a,b) = P(b)\)</span>， <strong>将P(a)看成一个函数Φ(a)，P(b|a)看成Φ(a,b)，则Σ<sub>a</sub> P(a)P(b|a)看成Φ<sub>a</sub>(b)</strong>）</li>
<li>→ <span class="math inline">\(Σ_{c} P(d|c) Σ_{b}P(c|b)Φ_{a}(b) = Σ_{c} P(d|c) Φ_{b}(c) = Φ_{c}(d)\)</span></li>
</ul></li>
<li>若是无向图，则为 <span class="math inline">\(P(a,b,c,d) = \frac{1}{Z}\prod Φ(x)\)</span></li>
<li>主要思想：乘法分配律（ab+ac=a(a+c)）</li>
<li>缺点：1、重复计算（求另外一个点时，则需要重新求（则没有存储中间结果，若是链很长，则计算量很大））；2、消去次序（一般相关最少的先消去，但在无向图找到最优消去次序是NP-Hard问题）</li>
</ul>
<h5 id="belief-propagation">Belief Propagation</h5>
<ul>
<li></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/PGM-Inference/" data-id="ckfy4zh1u000izeegc9pytjjm" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Exponential-Family-Distribution" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/05/Exponential-Family-Distribution/" class="article-date">
  <time datetime="2020-10-05T08:43:34.000Z" itemprop="datePublished">2020-10-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="指数族分布">指数族分布</h4>
<ul>
<li>Gaussian Distribution, Bernoulli Distribution (Categorical Distribution), Binomial Distribution (Multinomial Distribution), Poisson Distribution, Beta Distribution, Dirichlet Distribution, Gamma Distribution</li>
<li><font color="red">P(x|η) = h(x) exp(η^T Φ(x) - A(η))</font> （分为三部分）
<ul>
<li>η属于参数、p维向量；A(η): log partition function（配分函数）；h(x)与η无关，往往设置为1</li>
<li>partition function（源自于统计物理学）：P(x|θ) = 1/Z P~(x|θ) （Z为归一化因子，Z=∫P~(x|θ) dx）</li>
<li>P(x|η) = 1/exp(A(η)) h(x) exp(η^T Φ(x)) → exp(A(η)) = Z → A(η) = log Z，所以A(η)是log partition function</li>
</ul></li>
<li>特点
<ul>
<li><strong>充分统计量 sufficient statistics</strong>：Φ(x)
<ul>
<li>统计量：对样本的加工，关于样本的一个函数，均值、方差等</li>
<li>充分：统计量就可以完整表达样本特征信息了</li>
<li>Online Learning （可以仅存储样本统计量信息，就不需要存储大量样本，起到压缩数据的效果）</li>
</ul></li>
<li><strong>共轭</strong>
<ul>
<li>P(Y|X) = P(X|Y)P(Y)/∫P(X|Y)P(Y) dY（后验概率是求不出来，或者积分难问题，太复杂，难以求解）</li>
<li>计算f(Y)后验分布的期望：近似推断（变分推断、MCMC）</li>
<li>若似然P(X|Y)与先验P(Y)共轭（如二项分布和Beta分布），则后验与先验同分布，则仅需算后验分布的参数即可，就可以不用计算积分</li>
</ul></li>
<li><strong>最大熵</strong>（无信息先验）
<ul>
<li>没有先验，则认为是所有样本等概率，但无法定量分析，则引入最大熵</li>
<li>赋予先验：共轭（为了计算方便）；最大熵（无信息先验）</li>
</ul></li>
</ul></li>
<li>模型和推断
<ul>
<li><strong>广义线性模型</strong>
<ul>
<li>目标：解决分类、回归问题</li>
<li>线性组合（w^T x）</li>
<li>link function（激活函数的反函数）</li>
<li>指数族分布（y|x ~ 指数族分布）（如线性回归：y|x ~ N(μ,σ^2)；线性分类：y|x ~ 0/1分布（Bernoulli））</li>
</ul></li>
<li><strong>概率图模型</strong>
<ul>
<li>无向图：RBM 波尔兹曼机</li>
</ul></li>
<li><strong>变分推断</strong>
<ul>
<li>指数族分布可以简化变分推断</li>
</ul></li>
</ul></li>
<li>Gaussian Distribution
<ul>
<li>P(x|θ) = 1/√(2*pi)*σ exp{(x-μ)<sup>2/(-2σ</sup>2)} θ=(μ, σ^2) 一维高斯分布
<ul>
<li>η = η(θ)，A(η) = A(η(θ))，将θ映射成η</li>
<li>P(x|θ) = 1/√(2*pi<em>σ^2) exp(1/(-2σ^2) </em> (x<sup>2-2μx+μ</sup>2)) = exp log (2*pi*σ<sup>2)</sup>-1/2 exp{-1/(-2σ^2) [-2μ 1][x x<sup>2]</sup>T - μ/(-2σ^2)} = exp{[μ/(σ^2) -1/(-2σ^2) ][x x<sup>2]</sup>T - (μ/(-2σ^2) + 1/2 log(2*pi*σ^2))}</li>
<li>η^T = [μ/(σ^2) -1/(-2σ^2) ], Φ(x) = [x x<sup>2]</sup>T, A(η) = μ/(-2σ^2) + 1/2 log(2*pi*σ^2)</li>
<li>设定η=[η_1 η_2]^T，则A(η) = -η_1^2/4η_2 + 1/2 log (-pi/η_2)</li>
</ul></li>
</ul></li>
<li><strong>充分统计量Φ(x) 与 对数配分函数A(η)</strong>
<ul>
<li>P(x|η) 概率积分为1</li>
<li>P(x|η) = 1/exp(A(η)) h(x) exp(η^T Φ(x)) → exp(A(η)) = ∫h(x) exp(η^T Φ(x)) dx</li>
<li>两边对η求导：exp(A(η))*A'(η) = d(∫h(x) exp(η^T Φ(x)) dx)/dη = ∫h(x) Φ(x) exp(η^T Φ(x)) dx</li>
<li><strong>A'(η)</strong> = ∫h(x) Φ(x) exp(η^T Φ(x) - A(η)) dx = ∫Φ(x) P(x|η) dx = <strong>E_P(x|η)[Φ(x)]</strong></li>
<li><strong>A''(η) = Var[Φ(x)]</strong>，则A(η)是一个凸函数</li>
</ul></li>
<li><strong>充分统计量Φ(x) 与 极大似然估计</strong></li>
<li>Data: D = {x_1,x_2,...,x_N} N个样本</li>
<li>η_MLE = argmax log P(D|η) = argmax log Π P(x_i|η) = argmax Σ log P(x_i|η) = argmax Σ log [h(x_i) exp(η^T Φ(x_i) - A(η))] = argmax Σ [log h(x_i) + η^T Φ(x_i) - A(η)]
<ul>
<li>→ η_MLE ∝argmax Σ [η^T Φ(x_i) - A(η)] , 求导 → d(Σ [η^T Φ(x_i) - A(η)])/dη = Σ [d([η^T Φ(x_i) - A(η)])/dη] = Σ [Φ(x_i) - A'(η)] = Σ Φ(x_i) - N*A'(η) = 0 → A'(η_MLE) =1/N Σ Φ(x_i)</li>
<li>η_MLE = A'(η_MLE)反函数 → 则η_MLE是可求解的，只需要记录1/N Σ Φ(x_i) 即可，不需要记录所有样本</li>
</ul></li>
<li><strong>最大熵角度</strong>
<ul>
<li>若一个事件发生概率为p，其信息量为-log p （若p=1,信息量就为0；一个确定的事件没有信息量）</li>
<li>熵：E[-log p] = ∫-p(x)log p(x) dx = -Σp(x)log p(x) （对信息的衡量，对信息可能性的衡量）（熵只与x的分布有关，与取值无关）</li>
<li>最大熵&lt;=&gt;等可能（利用最大熵对等可能定量分析）（最大熵，对未知的分布进行猜测，因为不知道，所以认为都是等可能的）（<strong>要想熵最大，未知的分布必须是等可能</strong>）
<ul>
<li><strong>H[p] = -Σp(x)log p(x)</strong></li>
<li>假设x是离散的，P(x=1) = p_1，P(x=2) = p_2，...，P(x=k) = p_k，Σp_i = 1</li>
<li>则 max H[P] = max - Σp_i log p_i s.t. Σp_i = 1</li>
<li>p_i~ = argmax H[P] = argmin Σp_i log p_i （优化问题）</li>
<li>拉格朗日：L(p, λ) = Σp_i log p_i - λ(1- Σp_i)，dL/dp_i = log p_i + 1 - λ = 0 → p_i = exp(λ-1) 常数</li>
<li>则 p_1 = p_2 = ... p_k = 1/k，p(x)是了离散型均匀分布</li>
</ul></li>
<li>最大熵原理：<strong>在满足已知事实（约束条件）（已知数据）下，什么分布是具有最大熵的分布</strong>
<ul>
<li>Data = {x_1,x_2,...,x_N} 通过经验分布（概率分布P~(X=x) = p~(x) = count(x)/N）定量描述数据</li>
<li>需要P分布的E_p~[x], Var_p<sub>[x]，设定f(x)是任意关于x的向量函数（f=[f_1,f_2,...,f_Q]^T），则E_p</sub>[f(x)]=Δ （即是已知事实）</li>
<li>P(X) → p(x), H[p] = -Σp(x)log p(x) → min Σp(x) log p(x) s.t. Σp(x) = 1, E_p[f(x)]=Σp(x)f(x)=E_p~[f(x)]=Δ</li>
<li>拉格朗日：L(p, λ_0, λ) = Σp(x)log p(x) - λ_0(1- Σp(x)) - λ^T(Δ - Σp(x)f(x))</li>
<li>求导：dL/dp(x) = Σ(log p(x) + 1) - Σλ_0 - Σ λ^T f(x) = Σ[log p(x) + 1 - λ_0 - λ^T f(x)] = 0 → log p(x) + 1 - λ_0 - λ^T f(x) = 0 → <strong>p(x) = exp(λ^T f(x) - (1-λ_0))</strong>（令η=λ^T，Φ(x)=f(x)，A(η)=(1-λ_0)）</li>
<li>则用最大熵推出p(x)是指数分布</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/05/Exponential-Family-Distribution/" data-id="ckfy4zh1j0009zeeg8w2z604r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Dimensionality-Reduction" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Dimensionality-Reduction/" class="article-date">
  <time datetime="2020-10-02T13:10:36.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>过拟合
<ul>
<li>解决方法：增加数据、正则化、降维</li>
<li>原因：<strong>维度灾难</strong>
<ul>
<li>在没有很多数据集时，只能降维</li>
<li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li>
<li><p>从几何层面看：</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p>
<ul>
<li>2维正方形面积：1，圆形：pi*(0.5)^2</li>
<li>3维正方体体积：1，球体体积：4/3*pi*(0.5)^3 = K*(0.5)^3</li>
<li>D维超立方体体积：1，超球体体积： K*(0.5)^D</li>
<li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p></li>
<li>D维外超球体体积：K*1^D = K，环形体积：外超球体体积 - 内超球体体积 = K - K(1-e)^D
<ul>
<li>V外/V内 = 1-(1-e)^D =1（0&lt;e&lt;1，D趋向无穷大之后，(1-e)^D趋向于0）</li>
<li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Data
<ul>
<li>N个p维样本 X（维度N×p）（设I为N维全1列向量）</li>
<li>样本均值 X～=1/N Σ x_i = 1/N X^T I（维度p×1）</li>
<li>方差 S = 1/N Σ (x_i - X~)(x_i - X~)^T = 1/N X^T (I - 1/N I I^T) (1-1/N I I<sup>T)</sup>T X = 1/N X^T H H^T X = 1/N X^T H X
<ul>
<li>（维度p×p）</li>
<li>H = I-1/N I I^T centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li>
<li>H^T = H, H^2 = H H^T = (I-1/N I I^T) (I-1/N I I<sup>T)</sup>T = I-1/N I I^T = H</li>
<li>H^n = H</li>
</ul></li>
</ul></li>
<li>降维方法
<ul>
<li>直接降维 （特征选择：人工选取重要特征 ）</li>
<li>线性降维
<ul>
<li><strong>Principal Components Analysis PCA 主成分分析</strong>
<ul>
<li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p></li>
<li>最大投影方差
<ul>
<li>找到一个u_1平面，使投影间距达到最大（投影到u_2平面，距离太小，没有意义）</li>
<li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li>
<li>第1步：中心化：先将所有数据平移，利于计算，即 x_i - X～</li>
<li>第2步：投影到u_1平面：(x_i - X～)^T u_1（设定 |u_1|=1，即u_1^T u_1=1）</li>
<li>第3步：投影方差 J = 1/N Σ [(x_i - X~)^T u_1]^2 = 1/N Σ [u_1^T (x_i - X~) (x_i - X~)^T u_1] = u_1^T S u_1
<ul>
<li>u_1~ = argmax u_1^T S u_1 s.t. u_1^T u_1=1
<ul>
<li>使用拉格朗日求解</li>
<li>L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)</li>
<li>dL/du_1 = 2 S u_1 - 2λ u_1 = 0 ==&gt; <strong>S u_1 = λ u_1</strong></li>
<li><strong>u_1为eigen-vector特征向量，λ为eigen-value特征值</strong></li>
<li>解法1：对方差矩阵特征分解，即可求解PCA</li>
<li>解法2：直接对原始数据进行操作：中心化后的 HX = UΣV^T 进行奇异值分解（U和V均为正交矩阵），S = X^T H X = X^T H^T H X = (VΣU<sup>T)(UΣV</sup>T) = V Σ^2 V^T（可先忽略1/N常数）（维度p×p），因此直接求解HX奇异值分解，也就是求解了S的特征分解</li>
<li>假设 B = H X X^T H = (UΣV<sup>T)(VΣU</sup>T) = U Σ^2 U^T（维度N×N），则B与S有一样的eigenvalue，（S特征分解得到方向V（主成分）然后通过HX V得到新坐标）（B特征分解直接得到坐标U，称为主坐标分析 <strong>principal coordinate analysis PCoA</strong>）</li>
<li>HX V = UΣV^T V = UΣ（UΣ是坐标矩阵），BUΣ = U Σ^2 U^T UΣ = UΣ Σ^2（UΣ是特征向量组成的矩阵）</li>
</ul></li>
</ul></li>
</ul></li>
<li>最小重构代价
<ul>
<li>投影在u_1平面的点恢复到原来样子的代价</li>
<li>设从原p维降到q维（下面u代表特征）
<ul>
<li>x_i = Σ_k^p (x_i^T u_k) u_k, x_i ~ = Σ_k^q (x_i^T u_k) u_k (用特征u_k描述样本x_i，(x_i^T u_k)为距离大小，u_k为单位大小，则第k维描述为(x_i^T u_k) u_k)</li>
<li>代价函数 L = 1/N Σ||x_i - x_i ~||^2 = Σ_(k=q+1)^p u_k T S u_k = Σ_(k=q+1)^p λ_k (s.t. u_k^T u_k=1)</li>
<li>u_k = argmin L</li>
</ul></li>
</ul></li>
<li>概率角度
<ul>
<li>P-PCA
<ul>
<li>设定observed data X为p维（特征为连续型数据），latent variable Z为q维（q&lt;p）</li>
<li>设定Z～N(0, I)（服从高斯分布，q维）</li>
<li>X = WZ + u + ε（X是Z的一个线性变换加噪声），X|Z~N(WZ + u, σ^2 I)，X~N(u, WW<sup>T+σ</sup>2 I)</li>
<li>噪声ε~N(0, σ^2 I)（p维）</li>
<li>这也是一种Linear Gaussian Model（称该模型各向同性，对各方向影响是一样的）</li>
</ul></li>
<li><p>求P(Z), P(X|Z), P(X), 最后求后验P(Z|X)、用EM求参数W, u, σ</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR4.png" alt="DR4" style="zoom:75%;"></p></li>
<li>从服从高斯分布的Z中，投影点在方向W，进行线性变换得到X，也得到服从高斯分布的X|Z，方向W上有很多的高斯分布（各向同性）；X的分布不在方向W上，且中间很宽（因为高斯分布中间高两边低）<a href="https://www.bilibili.com/video/BV1aE411o7qd?p=27" target="_blank" rel="noopener">详情</a></li>
</ul></li>
</ul></li>
<li>MDS</li>
</ul></li>
<li>非线性降维
<ul>
<li>流型</li>
<li>Isomap</li>
<li>LLE</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Dimensionality-Reduction/" data-id="ckfy4zh1i0008zeegb5ethhx7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SVM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/SVM/" class="article-date">
  <time datetime="2020-10-02T08:50:04.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/SVM/">SVM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Data : N个p维样本 X（维度N×p），y_i = {-1,1}</li>
<li><p>SVM 三宝：间隔，对偶，核技巧</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p></li>
<li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</li>
<li><strong>hard-margin SVM</strong>（硬间隔）
<ul>
<li>最大间隔分类器 = max margin(w, b) s.t. y_i(w^T x_i+b) &gt; 0 for i = 1,...,N</li>
<li>点到直线距离，垂直线最短</li>
<li>margin(w, b) = min distance(w, b, x_i) = <strong>min 1/||w|| |w^T x_i + b|</strong></li>
<li>→ max_w,b min_x 1/||w|| |w^T x_i + b| (s.t. y_i(w^T x_i+b) &gt; 0) = max_w,b 1/||w|| min_x y_i(w^T x_i + b) (存在r&gt;0，使y_i(w^T x_i + b) =r，可以设置r=1)</li>
<li>→ max 1/||w|| s.t. min y_i(w^T x_i + b) = 1 → <strong>min 1/2 w^T w s.t. y_i(w^T x_i + b) &gt;=1</strong>(convex optimization 二次凸优化问题)(primal problem原问题)</li>
<li>拉格朗日：L(w, b, λ) = 1/2 w^T w + Σ λ_i(1-y_i(w^T x_i + b)) (λ_i &gt;= 0, (1-y_i(w^T x_i + b)) &lt;= 0；若(1-y_i(w^T x_i + b))&gt;0，L为正无穷，无解；仅在 λ_i=0， (1-y_i(w^T x_i + b)) =0，达到最大L)</li>
<li><strong>primal problem 原问题</strong>&lt;=&gt; <strong>min_w,b max_λ L(w, b, λ) s.t. λ_i &gt;= 0</strong> （对w，b没有限制）→ min 1/2 w^T w</li>
<li><strong>dual problem 对偶问题</strong>：<strong>max_λ min_w,b L(w, b, λ) s.t. λ_i &gt;= 0</strong>
<ul>
<li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li>
<li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li>
<li>dL/db = d[Σ λ_i(1-y_i(w^T x_i + b))]/db = d[- Σ λ_i y_i b]/db = -<strong>Σ λ_i y_i =0</strong>，带入原式L = 1/2 w^T w + Σ λ_i - Σ λ_i y_i w^T x_i</li>
<li>dL/dw = w - Σ λ_i y_i x_i = 0 → <strong>w = Σ λ_i y_i x_i</strong>，带入原式L = 1/2 w^T w + Σ λ_i - w^T w = Σ λ_i - 1/2 w^T w</li>
<li>→ <strong>min 1/2 w^T w - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</strong> （此处不能求λ偏导）</li>
<li>→ min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li><strong>KKT条件</strong>：参数偏导为0（dL/db=0，dL/dw=0，dL/dλ=0），λ_i(1-y_i(w^T x_i + b))=0， λ_i&gt;=0，(1-y_i(w^T x_i + b)&lt;=0</li>
<li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li>
<li>w~ = Σ λ_i y_i x_i, （存在x_k, y_k，1-y_k(w^T x_k + b)=0）b~ = y_k - w^T x_k = y_k - (Σ λ_i y_i x_i^T) x_k</li>
</ul></li>
<li><p><strong>f(x) = sign(w~^T x + b~)</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p>
<ul>
<li>落在虚线的样本点就是x_k（y_k(w^T x_k + b)=1），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li>
</ul></li>
</ul></li>
<li><strong>soft-margin SVM</strong>（软间隔）
<ul>
<li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li>
<li>soft-margin SVM在hard-margin SVM基础上允许一点点错误，min 1/2 w^T w + loss
<ul>
<li>分错点的个数：loss = Σ I{y_i(w^T x_i + b)&lt;1} （关于w是不连续的，无法求导，因此不采取）</li>
<li><p>hinge 距离：hinge loss = max{0, 1-y_i(w^T x_i + b)}</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p></li>
<li>min 1/2 w^T w + C Σ max{0, 1-y_i(w^T x_i + b)} s.t. y_i(w^T x_i + b)&gt;=1 （超参数C）</li>
<li><p>→ 设定ξ_i = y_i(w^T x_i + b)，<strong>min 1/2 w^T w + C Σ ξ_i</strong> s.t. y_i(w^T x_i + b)&gt;=1-ξ_i, ξ_i&gt;=0（同样用对偶问题方式来求解）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p></li>
</ul></li>
</ul></li>
<li>约束优化问题
<ul>
<li>primal problem 原问题：<strong>min f(x) s.t. m_i(x)&lt;=0, n_j(x)=0 (i=1,...,M, j=1,...,N)</strong></li>
<li>原问题的无约束形式（关于x的函数）：拉格朗日：L(x, λ, η) = f(x) + Σ λ_i m_i(x) + Σ η_i n_i(x) → <strong>min_x max_λ,η L(x, λ, η) s.t. λ_i&gt;=0</strong></li>
<li>证明两者等价：如果违法约束m_i(x)&gt;0，max_λ L → ∞；反之，max_λ L 必有最大值（λ_i=0时）（即排除了m_i(x)&gt;0情况，过滤掉违反约束的情况）</li>
<li>dual problem 对偶问题（关于λ,η的函数）：<strong>max_λ,η min_x L(x, λ, η) s.t. λ_i&gt;=0</strong>
<ul>
<li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （max_λ,η min_x L(x, λ, η) &lt;= min_x max_λ,η L(x, λ, η)）</li>
<li>证明：min_x L &lt;= L &lt;= max_λ,η L → A(λ,η) &lt;= L &lt;= B(x) → A(λ,η) &lt;= B(x) → max A(λ,η) &lt;= min B(x)<br>
</li>
<li>→ max_λ,η min_x L(x, λ, η) &lt;= min_x max_λ,η L(x, λ, η)</li>
<li>（在min_x L已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li>
<li>强对偶性：对偶问题=原问题</li>
</ul></li>
<li>几何解释
<ul>
<li>primal problem: min f(x) s.t. m_1(x)&lt;=0 (D定义域，D=dom_f ∩ dom_m_1) <strong>原问题最优解：p* = min f(x)</strong></li>
<li>L(x, λ) = f(x) + λ m_1(x) s.t. λ&gt;=0 <strong>对偶最优解：d* = max_λ min_x L(x, λ)</strong></li>
<li>将问题投影入二维空间：引入集合(区域) G = {(m_1(x), f(x))|x∈D}
<ul>
<li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li>
<li>凸集是指集合内任意两点的连线都在集合内</li>
<li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li>
<li>目标函数f如果不是凸函数，则不是凸优化问题</li>
<li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li>
<li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li>
<li>结论：凸函数的局部最优解就是全局最优解</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p>
<ul>
<li><p><strong>p* = inf {f(x)|(m_1(x), f(x))∈G, m_1(x)&lt;=0}</strong>（集合中没有最小值概念，对应的是下确界）</p>
<ul>
<li>P* 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li>
</ul></li>
<li>d* = max_λ g(λ) , g(λ) = min_x f(x) + λ m_1(x) , <strong>g(λ) = inf {f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G}</strong>
<ul>
<li>一条过原点直线 f(x) + λ m_1(x) = 0 (斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）g(λ) &lt;= p*</li>
<li>当调整斜率λ*，得到一个 g(λ*) = f(x) + λ* m_1(x) 同时与G的俩角相切，此时，直线与纵轴的交点为d*（绿线）</li>
<li><p>d* &lt;= p* （<strong>凸优化+slater条件 → d* = p*</strong>）（SVM是二次规划问题，符合slater条件）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li>slater条件
<ul>
<li>Convex + Slater → Strong Duality （充分不必要条件）</li>
<li>定义：存在x~在relint，使m_i(x)&lt;0 (i=1,...,M)
<ul>
<li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li>
<li>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = Ax + b（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li>
</ul></li>
<li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li>
<li>放松的slater条件：在m_i(x)中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足m_i(x)&lt;0 （凸二次规划问题：目标函数f是凸的，不等式约束m_i是仿射函数，等式约束n_j也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li>
</ul></li>
<li>KKT条件
<ul>
<li>KKT &lt;=&gt; Strong Duality (d* = p*)（充要条件）</li>
<li>从p*得到最优x*，从d*得到λ*、η*</li>
<li><strong>可行域（可行条件）：m_i(x*)&lt;=0, n_j(x*)=0, λ*&gt;=0</strong></li>
<li>互补松弛
<ul>
<li>d* = max_λ,η g(λ,η) = g(λ*, η*) = min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*) = f(x*) + Σ λ_i* m_i(x*) + Σ η_i* n_i(x*) = f(x*) + Σ λ_i* m_i(x*) &lt;= f(x*) = p*</li>
<li>（λ_i&gt;=0，m_i&lt;=0，则 (λ_i m_i) &lt;= 0）</li>
<li><strong>互补松弛条件 ：Σ λ_i* m_i(x*) = 0 → λ_i* m_i(x*)</strong></li>
</ul></li>
<li>梯度为0
<ul>
<li>min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*)</li>
<li>x*是对应x最小值，则 <strong>dL/dx = 0</strong></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>kernel SVM</strong>
<ul>
<li>Kernel Method（思想角度）</li>
<li>Kernel Trick（计算角度）</li>
<li><p>Kernel function</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png" alt="SVM9"></p>
<ul>
<li><strong>非线性带来高维转换（从模型角度）</strong>
<ul>
<li>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</li>
<li>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p>
<ul>
<li>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</li>
<li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）<br>
</li>
<li><font color="red">非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</font></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p>
<ul>
<li>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</li>
<li>三种方法转高维：1、类似MLP直接转高维；2、Kernel方法转高维；3、深度学习运用与或非构建有向无环图（神经网络）（与或非（三种基础运算均可用PLA表示）解决异或问题（复合运算）），神经网络：复合表达式、复合函数、MLP（FeedForward Neural Network）
<ul>
<li><p>XOR：x_1⊕x_2 = (¬x_1∧x_2)∨(x_1∧¬x_2)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png" alt="FNN1" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>对偶表示带来内积（从优化角度）</strong>
<ul>
<li>从频率视角归化到优化问题</li>
<li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题
<ul>
<li>Doul Problem: min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li>内积：x_i^T x_j</li>
<li>非线性转换：Φ(x_i)^T Φ(x_j) （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此Φ(x_i)^T Φ(x_j) 很难i求解和计算量很大）</li>
<li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到Φ(x_i)^T Φ(x_j) 结果</strong>（不需要先求Φ(x)再求内积）</li>
</ul></li>
<li><strong>Kernel function : K(x, x') = Φ(x)^T Φ(x') = &lt;Φ(x), Φ(x')&gt;</strong>
<ul>
<li>存在x, x'∈X，使K(x, x') = Φ(x)^T Φ(x')，则K就是一个核函数（如K(x, x')=exp(-(x-x')<sup>2/(2σ</sup>2))）</li>
<li>蕴含了非线性转换+内积</li>
</ul></li>
</ul></li>
<li>一般核函数指<strong>正定核函数</strong> <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a>
<ul>
<li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x')为核函数</li>
<li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x')，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使K(x, x') = &lt;Φ(x), Φ(x')&gt;，则K(x, x')为正定核函数</li>
<li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x')，若满足两个条件（对称性、正定性）则为正定和函数
<ul>
<li>对称性：K(x, x') = K(x', x)</li>
<li>正定性：任取N个元素，x_1,x_2,...,x_N∈X，对应的Gram矩阵是半正定的（K=[K(x_i, x_j)]）（两个定义等价，即证明：<strong>K(x, x') = &lt;Φ(x), Φ(x')&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li>
<li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（&lt;f, g&gt; = &lt;g, f&gt;）、正定性（内积大于等于0，&lt;f, f&gt; &gt;= 0）、线性性（&lt;r_1 f_1 + r_2 f_2 , g&gt; = r_1 &lt;f_1, g&gt; + r_2 &lt;f_2, g&gt;））<br>
</li>
</ul></li>
<li>必要性证明
<ul>
<li>在Hilbert Space中的Φ(x)具有对称性性质，K(x, x') = &lt;Φ(x), Φ(x')&gt; = &lt;Φ(x'), Φ(x)&gt; = K(x', x)</li>
<li>K=[K(x_i, x_j)]（维度N×N）（半正定：任意a列向量，a^T K a &gt;=0）</li>
<li>a^T K a = Σ Σ a_i a_j K_ij = Σ Σ a_i a_j K(x_i, x_j) = Σ Σ a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; =线性性= Σ Σ a_i a_j Φ(x_i)^T Φ(x_j) = Σ a_i Φ(x_i)^T Σ a_j Φ(x_j) = [Σ a_i Φ(x_i)]^T Σ a_j Φ(x_j) = &lt;Σ a_i Φ(x_i), Σ a_j Φ(x_j)&gt; = ||Σ a_i Φ(x_i),||^2 &gt;= 0，半正定性</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/SVM/" data-id="ckfy4zh22000qzeegum7ibocn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Decision-Tree" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Decision-Tree/" class="article-date">
  <time datetime="2020-10-02T08:49:49.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Decision-Tree/">Decision_Tree</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>基于数据特征构造决策树
<ul>
<li>有向边</li>
<li>结点
<ul>
<li>内部结点(internal node)-&gt;表示特征</li>
<li><p>叶子结点(leaf node)-&gt;表示类别</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p></li>
<li>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</li>
</ul></li>
<li>决策树表示给定特征条件下类的条件概率分布。
<ul>
<li>一个条概率分布定义特征空间的一个划分上</li>
<li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li>
</ul></li>
<li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li>
</ul></li>
<li>决策树学习过程：特征选择、决策树生成、剪枝
<ul>
<li><strong>ID3算法</strong>（分类、多叉树）
<ul>
<li>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）
<ul>
<li>Information Gain 信息增益。信息增益越大对应的特征越重要</li>
<li>Entropy 熵，表示随机变量不确定的度量</li>
<li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"><figcaption>DT2</figcaption>
</figure></li>
</ul></li>
<li>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</li>
<li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li>
</ul></li>
<li><strong>C4.5算法</strong>（分类、多叉树）
<ul>
<li>特征选择
<ul>
<li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p></li>
</ul></li>
<li>生成：与ID3算法类似</li>
<li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li>
<li>二叉树模型会比多叉树运算效率高</li>
<li>无剪枝</li>
</ul></li>
<li><strong>CART</strong> classification and regression tree（分类、回归、二叉树）
<ul>
<li>分类
<ul>
<li>特征选择
<ul>
<li>Gini基尼指数</li>
<li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li>
<li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li>
</ul>
<figure>
<img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"><figcaption>DT4</figcaption>
</figure></li>
<li>生成
<ul>
<li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li>
<li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li>
</ul></li>
<li><p>损失函数</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p>
<ul>
<li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p></li>
</ul></li>
<li>回归</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Decision-Tree/" data-id="ckfy4zh1e0004zeegqt04gw54" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/11/Statistics/">Statistics</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="statistics-for-machine-learning">Statistics for Machine Learning</h4>
<h5 id="one.-两大派系">One. 两大派系</h5>
<ul>
<li><strong>频率派：统计机器学习</strong>（model (f(x)=w^T x+b), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）
<ul>
<li>正则化（L1,L2）</li>
<li>核化（Kernel SVM）</li>
<li>集成化（AdaBoost，RandForest）</li>
<li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li>
</ul></li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））
<ul>
<li>有向图：Bayesian Network (Deep Directed Network)
<ul>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder (VAE)</li>
<li>GAN</li>
</ul></li>
<li>无向图：Markov Network (Deep Boltzmann Network )</li>
<li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li>
<li>统称 Deep Generative Model（但深层很难计算）</li>
<li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li>
</ul></li>
<li>Data
<ul>
<li>X: data, X={x_1, x_2, ..., x_n}^T Dimension(N, P)</li>
<li>θ: parameter, X~p(X|θ)</li>
</ul></li>
<li><strong>频率派</strong>
<ul>
<li>θ为未知常数；X为随机变量</li>
<li>loss = P(X|θ) = Π P(x_i|θ)
<ul>
<li>x_1, x_2, ..., x_n之间独立同分布</li>
</ul></li>
<li>Maximum Likelihood Estimation 极大似然估计
<ul>
<li>θ_MLE = argmax_θ log P(X|θ)</li>
</ul></li>
</ul></li>
<li><strong>贝叶斯派</strong>
<ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率
<ul>
<li>P(θ|X) = P(X, θ)/P(X) = P(X|θ)P(θ)/P(X) = likelihood*prior / ∫_θ P(X|θ) dθ</li>
</ul></li>
<li>Maximum A Posterior Probability 最大后验概率
<ul>
<li>找到θ在分布中最大值点</li>
<li>θ_MLE = argmax_θ P(X|θ) = argmax_θ P(X|θ)P(θ)</li>
<li>P(θ|X)其分母是不变的，则θ_MLE与分子成正比，只需要算分子</li>
</ul></li>
<li>贝叶斯估计
<ul>
<li>P(θ|X) = P(X|θ)P(θ) / ∫_θ P(X|θ) dθ</li>
<li>必须完整计算整个分子式</li>
</ul></li>
<li>贝叶斯预测
<ul>
<li>X (train), X~ (test), X -&gt; θ -&gt; X~</li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li>P(X~|X) = ∫_θ P(X~, θ|X) dθ = ∫_θ P(X~|X)P(θ|X) dθ</li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-linear-regression">Two. Linear Regression</h5>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线f(w) = w^T x + b（偏置b可先忽略）</p>
<ul>
<li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）
<ul>
<li>线性（属性线性、全局线性、系数线性）
<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul></li>
<li>全局性
<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul></li>
<li>数据未加工
<ul>
<li>预处理：PCA，流行</li>
</ul></li>
</ul></li>
<li><strong>矩阵表达</strong>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）
<ul>
<li><strong>L(w) = Σ||w^T x_i - y_i||^2</strong> = Σ(w^T x_i - y_i)^2 = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y</li>
</ul></li>
<li><strong>w~ = argmin L(w)</strong></li>
<li>求导 dL/dw = 2 X^T X w - 2 X^T Y = 0 ----&gt; X^T X w = X^T Y
<ul>
<li><strong>w~ = (X^T X)^-1 X^T Y</strong> (伪逆：(X^T X)^-1 X^T)</li>
</ul></li>
<li>x_3的误差为(w^T x_3 - y_3)，即所有误差分成一小段一小段</li>
</ul></li>
<li><strong>几何意义</strong>
<ul>
<li>f(w) = w^T x &lt;=&gt; f(β) = x^T β</li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li>目标：在p维空间中找到一条直线f(β)离Y最近，即Y在p维空间的投影
<ul>
<li><p>若向量a与向量b垂直，则 a^T b = 0</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li>
<li>虚线为(Y - Xβ) 与X的p维空间垂直，X^T (Y-Xβ) = 0 ----&gt; β = (X^T X)^-1 X^T Y</li>
<li>误差分散在p个维度上</li>
</ul></li>
</ul></li>
<li><strong>概率角度</strong>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ^2)</li>
<li>y = f(w) + ε = w^T x + ε
<ul>
<li>y|x,w ~ N(w^T x, σ^2) &lt;=&gt; <strong>P(y|x,w)</strong> =1/(sqrt(2*pi)*σ) exp(-(y - w^T x)<sup>2/(2*σ</sup>2))</li>
</ul></li>
<li>定义log-likelihood：
<ul>
<li>L_MLE (w) = log P(y|x,w) = log Π P(y_i|x_i,w) = Σ log P(y_i|x_i,w) = Σ[log(1/(sqrt(2*pi)*σ)) + log(exp(-(y_i - w^T x_i)<sup>2/(2*σ</sup>2)))] = Σ[log(1/(sqrt(2*pi)*σ)) -(y_i - w^T x_i)<sup>2/(2*σ</sup>2)]</li>
<li>样本之间独立同分布</li>
<li>w~ = argmax L_MLE (w) = argmax -(y_i - w^T x_i)<sup>2/(2*σ</sup>2) = argmin (y_i - w^T x_i)^2</li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul></li>
</ul></li>
<li><strong>Regularization 正则化</strong>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong>
<ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：argmin [L(w)+λP(w)]</li>
</ul></li>
<li>L1 -&gt; Lasso
<ul>
<li>P(w) = ||w||_1</li>
</ul></li>
<li>L2 -&gt; Ridge 岭回归
<ul>
<li>P(w) = ||w||_2 = w^T w</li>
<li>权值衰减</li>
<li>J(w) = Σ||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w = w<sup>T(X</sup>T X + λ I) w - 2 w^T X^T Y + Y^T Y
<ul>
<li>w~ = argmin J(w)</li>
<li>dJ/dw = 2 (X^T X + λ I)w - 2 X^T Y = 0, w~ = (X^T X + λ I)^-1 X^T Y</li>
<li>X^T X 是半正定矩阵+对角矩阵=(X^T X + λ I)正定矩阵，必然<strong>可逆</strong></li>
</ul></li>
</ul></li>
<li>贝叶斯的角度
<ul>
<li>参数w服从分布，w~N(0,σ_0^2) ---&gt; <strong>P(w)</strong> = 1/(sqrt(2*pi)*σ_0) exp(||w||<sup>2/(2*σ_0</sup>2))</li>
<li>P(w|y) = P(y|w)P(w) / P(y)</li>
<li>MAP: w~ = argmax P(w|y) = argmax P(y|w)P(w) = argmax log(P(y|w)P(w)) = argmax log[1/(2*pi*σ_0*σ) exp(-(y_i - w^T x_i)<sup>2/(2*σ</sup>2) -||w||<sup>2/(2*σ_0</sup>2))] = argmin [(y_i - w^T x_i)^2 + σ<sup>2/σ_0</sup>2||w||^2] = argmin [L(w)+λP(w)]</li>
<li>λ = σ<sup>2/σ_0</sup>2</li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-linear-classification">Three. Linear Classification</h5>
<ul>
<li>线性回归------&gt;激活函数，降维-------&gt;线性分类</li>
<li>硬分类：0/1
<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul></li>
<li>软分类：[0,1]区间内概率
<ul>
<li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li>
<li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li>
</ul></li>
<li><p><strong>Perceptron 感知机</strong> (1958年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>判别模型</li>
<li>样本：{(x_i, y_i)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：f(x) = sign(w^T x + b) （w^T x大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）
<ul>
<li>L(w) = Σ I{y_i * (w^T x_i) &lt; 0} （非连续函数，不可导）</li>
<li>L(w) = Σ -y_i w^T x_i, dL = -y_i x_i</li>
</ul></li>
<li>若是非线性可分，可是使用pocket algorithm</li>
<li>从感知机到深度学习（发展历程）
<ul>
<li>1958年提出PLA</li>
<li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li>
<li>1981年提出MLP（多层感知机），FeedForward Neural Network</li>
<li>1986年BP+MLP，RNN</li>
<li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li>
<li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li>
<li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li>
<li>2009年GPU发展，2011年speech，2012年ImageNet</li>
<li>2013年Variational Autoencoder (VAE)</li>
<li>2014年GAN</li>
<li>2016年Alpha Go</li>
<li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li>
<li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li>
</ul></li>
</ul></li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数N_1，均值X_c1，方差S_c1，负样本个数N_2，均值X_c2，方差S_c2，（S_c1 = 1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T）</li>
<li>思想：类内小，类间大
<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 w^T x（因为垂直，则Z平面即w向量）
<ul>
<li>（前提设置||w||=1）</li>
<li>则样本点投影到Z平面为：|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i</li>
</ul></li>
</ul></li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong>
<ul>
<li>N_1 = 1/N_1 Σ w^T x_i</li>
<li>S_1 = 1/N_1 Σ (w^T x_i - Z_1) (w^T x_i - Z_1)^T</li>
<li>类间：(Z_1-Z_2)^2</li>
<li>类内：S_1+S_2</li>
</ul></li>
<li>策略：L(w) = (Z_1-Z_2)^2 / (S_1+S_2) = [w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w] / [ w^T (S_c1+ S_c2) w ]
<ul>
<li>分子 = [w^T (1/N_1 Σ x_i - 1/N_2 Σ x_i)]^2 = [w^T (X_c1 - X_c2)]^2 = w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w</li>
<li>分母 = w^T S_c1 w + w^T S_c2 w = w^T (S_c1+ S_c2) w
<ul>
<li>S_1 = 1/N_1 Σ (w^T x_i - 1/N_1 Σ w^T x_j)(w^T x_i - 1/N_1 Σ w^T x_j)<sup>T=w</sup>T [1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T] w = w^T S_c1 w</li>
</ul></li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li>L(w) = w^T S_b w / w^T S_w w</li>
<li>w~ = argmax L(w)
<ul>
<li>dL/dw = 2*S_b w (w^T S_w w)^-1 + (w^T S_b w) * (-1) (w^T S_w w)^-2 * 2 * S_w w = 0</li>
<li>S_b w (w^T S_w w) = (w^T S_b w) S_w w （(w^T S_w w) 最终计算得一个实数，一维，没有方向）（求解w～关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li>w = (w^T S_w w)/(w^T S_b w) * S_w^-1 * S_b w，正比于(S_w^-1 * S_b w) ，正比于(S_w^-1 *(X_c1 - X_c2))</li>
<li>（S_b w = (X_c1 - X_c2)(X_c1 - X_c2)^T w，(X_c1 - X_c2)^T w为实数）</li>
<li>（若S_w是对角矩阵，各向同性，S_w正比于单位矩阵，则w正比于(X_c1 - tX_c2)</li>
</ul></li>
</ul></li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul></li>
<li><strong>Logistic Regression</strong>
<ul>
<li>线性回归------&gt;sigmoid-------&gt;线性分类</li>
<li>判别模型</li>
<li>model
<ul>
<li>sigmoid(x) = 1/(1+e<sup>-x)，将w</sup>T x映射到处于[0,1]区间的概率值p</li>
<li>p_1 = P(y=1|x) = sigmoid(w^T x) = 1/(1+e<sup>(w</sup>T x))</li>
<li>p_0 = P(y=0|x) = sigmoid(w^T x) = e<sup>(w</sup>T x)/(1+e<sup>(w</sup>T x))</li>
<li>综合表达：P(y|x) = p_1^y * p_0^(1-y)</li>
</ul></li>
<li>w~ = argmax P(Y|X) = argmax log Π P(y_i|x_i) = argmax Σ log P(y_i|x_i) = argmax Σ [y_i*log p_1 + (1-y_i)*log p_0] （-cross entropy）
<ul>
<li>MLE &lt;=&gt; loss function (min cross entropy)</li>
</ul></li>
</ul></li>
<li><strong>Gaussian Discriminant Analysis</strong>
<ul>
<li>生成模型、连续
<ul>
<li>y~ = argmax P(y|x) = argmax P(x|y)P(y)</li>
<li>分类：最终比较P(y=0|x)，P(y=1|x)大小</li>
<li>P(y|x)正比于P(x|y)P(y)，即联合概率P(x, y)</li>
</ul></li>
<li>Data：N个d维样本，二分类(0,1)，正样本个数N_1，方差S_1，负样本个数N_2，方差S_2</li>
<li><strong>prior probability</strong>
<ul>
<li>先验概率服从伯努利分布</li>
<li>y ~ Bernoulli，P(y=1) = p，P(y=0) = 1-p</li>
<li>P(y) = p<sup>y*(1-p)</sup>(1-y)</li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li>
<li>x|y=1 ~ N(u_1, σ)</li>
<li>x|y=0 ~ N(u_2, σ)</li>
<li>方差一样（权值共享），均值不一样</li>
<li>P(x|y) = N(u_1, σ)^y * N(u_2, σ)^(1-y)</li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>log MLE =&gt; L(θ) = log Π P(x_i, y_i) = Σ log [P(x_i|y_i)P(y_i)] = Σ[log P(x_i|y_i) + log P(y_i)] = Σ[log N(u_1, σ)^y_i * N(u_2, σ)^(1-y_i) + log p<sup>y_i*(1-p)</sup>(1-y_i)] = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ) + y_i*log p + (1-y_i)*log (1-p)]</li>
<li>θ = (u_1, u_2, σ, p)</li>
<li>θ ~ = argmax L(θ)</li>
<li>求解4个参数
<ul>
<li>p
<ul>
<li>相关部分 L = Σ [log p^y_i + log (1-p)^(1-y_i)]</li>
<li>dL/dp = Σ [y_i/p - (1-y_i)/(1-p)] = 0 =&gt; Σ [y_i*(1-p)- (1-y_i)*p] = Σ (y_i - p) = 0</li>
<li>p~ = 1/N Σ y_i = N_1/N（二分类（0,1），Σ y_i = N_1）</li>
</ul></li>
<li>u_1 （同理 u_2）
<ul>
<li>相关部分 L = Σ y_i*log N(u_1, σ) = Σ y_i*log [1/((2*pi)<sup>(d/2)*σ</sup>(1/2)) exp((x_i-u_1)^T(x_i-u_1)/-2*σ)</li>
<li>u_1 = argmax L = argmax Σ y_i * [(x_i-u_1)^T(x_i-u_1)/-2*σ] = argmax -1/2 Σ y_i * [(x_i-u_1)<sup>T(x_i-u_1)σ</sup>-1] = argmax -1/2 Σ y_i * [x_i^T σ^-1 x_i - 2*u_1^T σ^-1 x_i + u_1^T σ^-1 u_1]</li>
<li>dL/du_1 = -1/2 Σ y_i * [-2*σ^-1 x_i + 2*σ^-1 u_1] = 0 =&gt; Σ y_i * (u_1 - x_i) = 0</li>
<li>u_1 = Σ y_i x_i / Σ y_i = Σ y_i x_i / N_1</li>
</ul></li>
<li>σ
<ul>
<li>相关部分 L = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ)] = Σlog N(u_1, σ) + Σlog N(u_2, σ)
<ul>
<li>（二分类，非0即1，可以拆分算，可以省去y_i）</li>
</ul></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li>
<li>σ = 1/N (N_1*S_1 + N_2*S_2)</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Naive Bayes</strong>
<ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong>
<ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
<li>最简单概率图模型</li>
</ul></li>
<li>生成模型、离散</li>
<li>Data
<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c_1, c_2, ...,c_k}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul></li>
<li><strong>prior probability</strong>
<ul>
<li>P(Y=c_k)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>P(X=x|Y=c_k) = P(X<sup>(1)=x</sup>(1), ..., X<sup>(d)=x</sup>(d)|Y=c_k) = ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>特征条件独立</li>
<li>^(1) 上标表示第1维度</li>
</ul></li>
<li><strong>joint probability distributions</strong>
<ul>
<li>联合概率分布</li>
<li>P(X, Y) = P(X|Y)P(Y)</li>
</ul></li>
<li><strong>posterior probability</strong>
<ul>
<li>P(Y=c_k|X=x) = P(X=x|Y=c_k)P(Y=c_k)/ΣP(X=x|Y=c_k)P(Y=c_k) = P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)/Σ P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>则分类器为
<ul>
<li>y=f(x) = argmax P(Y=c_k|X=x) = argmax P(Y=c_k)ΠP(X<sup>(j)=x</sup>(j)|Y=c_k)</li>
<li>分母不变，则仅于分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
<li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li>
</ul></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li>L(Y, f(x)) = 1 if Y!=f(x) or 0 if Y==f(x)
<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul></li>
</ul></li>
<li>期望风险函数：R_exp(f) = E[L(Y, f(x))]
<ul>
<li>根据联合概率分布：R_exp(f) = E_x Σ[L(c_k|f(x))]P(c_k|X)</li>
</ul></li>
<li>f(x) = argmin Σ[L(c_k|f(x))]P(c_k|X)
<ul>
<li>根据L(Y, f(x))函数展开，消去Y==f(x)项</li>
<li>f(x) = <strong>argmin ΣP(y!=c_k|X=x)</strong> = argmin (1-P(y=c_k|X=x)) = <strong>argmax P(y=c_k|X=x)</strong></li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c_k类别的最大概率为多少）</li>
</ul></li>
<li>详情案例见统计学习方法(第二版)63页</li>
<li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckfy4zh2d0010zeegbomuex9n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Bayes-MRF" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/12/Bayes-MRF/" class="article-date">
  <time datetime="2020-08-12T12:10:46.000Z" itemprop="datePublished">2020-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/12/Bayes-MRF/">Bayes_MRF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="bayes-network贝叶斯网络-markov-random-fields-马尔可夫随机场">Bayes Network贝叶斯网络 &amp; Markov Random Fields 马尔可夫随机场</h4>
<h5 id="one.-前提">One. 前提</h5>
<ul>
<li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）
<ul>
<li>Representation 表示
<ul>
<li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li>
<li>无向图 Markov Network (Markov Random Fields) (无向，则结点的概率仅取决于1阶邻居)</li>
<li>高斯图 （连续）
<ul>
<li>Gassian Bayes Network</li>
<li>Gassian Markov Network</li>
</ul></li>
</ul></li>
<li>Inference 推断
<ul>
<li>精确推断
<ul>
<li>Variable Elimination, Belief Propagation, Junction Tree Algorithm</li>
</ul></li>
<li>近似推断
<ul>
<li>确定性近似（变分推断）</li>
<li>随机性近似（蒙特卡洛，MCMC）</li>
</ul></li>
</ul></li>
<li>Learning 学习
<ul>
<li>参数学习
<ul>
<li>完备数据（非隐变量）（有向，无向）</li>
<li>隐变量（EM）</li>
</ul></li>
<li>结构学习 (学习更好的图结构，参数)</li>
</ul></li>
</ul></li>
<li><strong>高维随机变量</strong> P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大
<ul>
<li>边缘概率 P(x<sub>i</sub>)</li>
<li>条件概率 P(x<sub>j</sub>|x<sub>i</sub>)</li>
</ul></li>
<li><strong>运算原则</strong>
<ul>
<li>sum rule: <span class="math inline">\(P(x_{1}) = \int_{x_{2}} P(x_{1}, x_{2}) d_{x_{2}}\)</span> (边缘概率)</li>
<li>poduct rule: <span class="math inline">\(P(x_{1}, x_{2}) = P(x_{1})*P(x_{2}|x_{1}) = P(x_{2})*P(x_{1}|x_{2})\)</span></li>
<li>chain rule: <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li>
<li>bayesian rule: <span class="math inline">\(P(x_{2}|x_{1}) = P(x_{1}, x_{2}) / P(x_{1}) = P(x_{2})*P(x_{1}|x_{2}) / ∫ P(x_{1}, x_{2}) d{x_{2}}\)</span></li>
</ul></li>
<li><strong>缺点</strong>
<ul>
<li><font color="red">高维复杂 P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大</font></li>
<li>简化
<ul>
<li>每个<strong>维度之间相互独立</strong> (特性太强了)
<ul>
<li><span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i})\)</span></li>
<li>Naive Bayes 朴素贝叶斯 <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y)\)</span></li>
</ul></li>
<li>Markov Property 马尔可夫特性
<ul>
<li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li>
<li>x<sub>i+1</sub> 只与 x<sub>i</sub>相关，与其他 x<sub>i-1</sub>,...,x<sub>1</sub>无关</li>
</ul></li>
<li><strong>条件独立性</strong>（可降低计算复杂度）
<ul>
<li>给定x<sub>B</sub>情况下，集合x<sub>A</sub>与集合x<sub>C</sub>无关 （x<sub>A</sub>，x<sub>B</sub>，x<sub>C</sub>无交集）</li>
<li>x<sub>B</sub> 只与x<sub>C</sub>相关 <a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a> <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-bayes">Two. Bayes</h5>
<ul>
<li>链式法则 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li>
<li><p>因子分解 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{p(i)})\)</span>（x<sub>p(i)</sub>为x<sub>i</sub>父亲集合，即指向x<sub>i</sub>的结点）（条件独立性） <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p></li>
<li><p><strong>tail to tail</strong></p>
<ul>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A)\)</span></li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span>
<ul>
<li>则 <span class="math inline">\(P(C|A) = P(C|A,B)\)</span>（A，B同时发生时，不影响C），则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，B,C相互独立，“倒V路径”</font>）</li>
</ul></li>
<li>条件独立性：<span class="math inline">\(P(B|A)P(C|A) = P(B|A)P(C|A,B) = P(B,C|A)\)</span></li>
</ul></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p>
<ul>
<li><p><strong>head to tail</strong></p>
<ul>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|B)\)</span></li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span></li>
<li>发生B时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞，A,C相互独立</font>）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p></li>
<li><strong>head to head</strong>
<ul>
<li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A、B有关系，不独立则难以分解</font>）</li>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B)P(C|A,B)\)</span> （父亲结点先于子结点）</li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span>
<ul>
<li><span class="math inline">\(P(B) = P(B|A)\)</span>，则默认情况下，C还没被观察，A,B相互独立</li>
</ul></li>
<li>这个模式是想判断 <span class="math inline">\(P(A|C) == P(A|C,B)\)</span>，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p>
<ul>
<li>（<font color="red">若D被观测，路径也是是连通，A、B有关系</font>）</li>
</ul></li>
<li>有向图是的条件独立性（证明发生x<sub>B</sub>, x<sub>A</sub>, x<sub>C</sub>相互独立）
<ul>
<li>D-separation
<ul>
<li><p>x<sub>A</sub>, x<sub>B</sub>, x<sub>C</sub> 三个集合两两无交集 <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p></li>
<li>若A与C，存在B<sub>1</sub>，B<sub>2</sub>关系，且属于x<sub>B</sub>集合；存在B<sub>3</sub>，B<sub>4</sub>关系，且不属于x<sub>B</sub>集合</li>
<li>则符合：发生x<sub>B</sub>时，存在上述情况， x<sub>A</sub>, x<sub>C</sub>相互独立 （<strong>全局马尔可夫性</strong>）</li>
<li><span class="math inline">\(P(x_{i}|x_{-i}) = P(x_{i}, x_{-i}) / P(x_{-i}) = p(x) / \int P(x_{i}) d{x_{i}} = \prod_{j} P(x_{j}|x_{p(j)}) / \int \prod_{j} P(x_{j}|x_{p(j)})d_{x_{i}}\)</span>
<ul>
<li>x<sub>-i</sub>表示集合{x<sub>1</sub>,...x<sub>p</sub>}中去除x<sub>i</sub>, <strong>x/x<sub>i</sub></strong></li>
<li><span class="math inline">\(\prod_{j} P(x_{j}|x_{p(j)})\)</span> 分为与x<sub>i</sub>有关和无关两部分</li>
<li><p>则 <span class="math inline">\(\prod_{j} P(x_{j}|x_{p(j)}) / \int \prod_{j} P(x_{j}|x_{p(j)}) d_{x_{i}}\)</span>无关部分则可以约去</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"><figcaption>bayes_6</figcaption>
</figure></li>
<li><span class="math inline">\(P(x_{i}, x_{-i}) = \prod_{j} P(x_{j}|x_{p(j)})\)</span>，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket</li>
<li>一个人与全世界的关系=一个人与身边人的关系</li>
</ul></li>
</ul></li>
</ul></li>
<li>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）
<ul>
<li>离散
<ul>
<li>单一
<ul>
<li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y=1)\)</span> (x 是p维， 当y被观测时，x各维度相互独立)</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p></li>
</ul></li>
<li><p>混合</p>
<ul>
<li><p><strong>GMM</strong> 高斯混合模型（多个高斯分布） -&gt; 做聚类</p>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/GMM.png" alt="GMM" style="zoom:75%;"></li>
</ul></li>
<li>时间
<ul>
<li><strong>Markov Chain</strong> 马尔可夫链</li>
<li><strong>Gaussian Process</strong> 无限维高斯分布</li>
</ul></li>
<li>动态模型 = 混合 + 时间
<ul>
<li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li>
<li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li>
<li><strong>Partide Filter</strong> （非连续，非高斯）</li>
</ul></li>
</ul></li>
<li><p>连续</p>
<ul>
<li><strong>Gaussian Bayes Network</strong> 高斯图</li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-mrf">Three. MRF</h5>
<ul>
<li>无向图</li>
<li><p>条件独立性，发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关 （<strong>global markov</strong> 全局马尔可夫）</p>
<ul>
<li>存在集合x<sub>A</sub>, x<sub>C</sub>被x<sub>B</sub>分割（对应 bayes D-separation）， 那么发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关</li>
</ul></li>
<li><strong>local markov</strong> 局部马尔可夫
<ul>
<li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p></li>
</ul></li>
<li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p>
<ul>
<li>在集合x<sub>-i,-j</sub>(集合没有i，j结点), 对于任意两个点x<sub>i</sub>, x<sub>j</sub>没有直接连接，则相互独立, (i ≠ j)</li>
</ul></li>
<li>条件独立性体现的三个方面，并且相互等价，可以互推 global markov &lt;=&gt; local markov &lt;=&gt; pair markov &lt;=&gt; 基于最大团的因子分解
<ul>
<li>clique团，最大团
<ul>
<li>集合间的结点相互联通</li>
<li>在一个团无法添加结点，则是最大团</li>
<li>(c<sub>1</sub>, c<sub>2</sub>,...表示团)</li>
</ul></li>
</ul></li>
<li>因子分解 <span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}),Z = Σ_{x_{1}}...Σ_{x_{p}}\prod_{i}^{p}Φ(x_{c_{i}})\)</span> （用因子分解证明条件独立性）
<ul>
<li>Φ 势函数， 必须为正（大于0）
<ul>
<li><span class="math inline">\(Φ (x_{c_{i}}) = e^{-E(x_{c_{i}})}\)</span> (E为能量函数，也叫势函数)</li>
<li>Φ (x<sub>ci</sub>) &lt;-&gt; P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</li>
<li><span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}) = \frac{1}{Z} \prod_{i}^{p} e^{-E(x_{c_{i}})} = \frac{1}{Z} e^{-\sum E(x_{c_{i}})}\)</span></li>
<li>Gibbs Distribution是统计物理的名称，与指数族分布形式一样（俩个等价）</li>
<li>最大熵原理：在满足已知事实，最终推出分布属于指数族分布</li>
<li>结论：Gibbs Distribution &lt;=&gt; Markov Random Field</li>
</ul></li>
<li>c<sub>i</sub>最大团，x<sub>ci</sub>最大团随机变量集合</li>
<li>Z 为联合概率分布的归一化因子</li>
<li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li>
<li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li>
<li>Pair-MRF 因子分解：<span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i} Φ(x_{i}) \prod_{j} Φ(x_{i}, x_{j})\)</span> (考虑边)</li>
<li>最大后验概率推理（图像分割问题）：max_x P(x)
<ul>
<li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"><figcaption>pair-markov</figcaption>
</figure></li>
<li>假设<span class="math inline">\(θ(x_{i}) = -logΦ(x_{i}),θ(x_{i}, x_{j}) = -logΦ(x_{i},x_{j})\)</span></li>
<li><p><span class="math inline">\(max_{x} P(x)\)</span> -&gt; 能量最小化 -&gt; <span class="math inline">\(min_{x} E(x) = \sum θ(x_{i}) + \sum θ(x_{i}, x_{j})\)</span></p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"><figcaption>pair-markov_2</figcaption>
</figure>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p></li>
<li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"><figcaption>pair-markov_4</figcaption>
</figure></li>
<li><p>结点的设置势函数，一个前景一个背景</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"><figcaption>pair-markov_5</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/12/Bayes-MRF/" data-id="ckfy4zh170001zeegv0n23ulp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-HMM-CRF" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/09/HMM-CRF/" class="article-date">
  <time datetime="2020-08-09T12:29:28.000Z" itemprop="datePublished">2020-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/09/HMM-CRF/">HMM_CRF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="hidden-markov-model-隐马尔可夫-conditional-random-field-条件随机场">Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场</h4>
<h5 id="one.-time-sequence-series">One. Time Sequence / Series</h5>
<ul>
<li>在时间序列中，起伏状态可以被观测到（股票走势就是一种时间序列，股票中的涨跌）</li>
<li>无法观测到的是时间序列的<strong>隐状态</strong>（股票中是否处于牛市状态，这类的就是隐状态）
<ul>
<li>隐状态会有很多</li>
<li>当知道隐状态存在时，每个隐状态被观测时，都是相互独立（互不干扰）</li>
<li>隐状态之间是离散的</li>
</ul></li>
</ul>
<h5 id="two.-hmm">Two. HMM</h5>
<ul>
<li>在概率中，隐马尔可夫模型对应所有状态是相互独立的</li>
<li><strong>P(q_t|q_t-1)</strong> Transition Probability 转移概率
<ul>
<li>P(q_t|q_t-1, q_t-2,..., q_1) = P(q_t|q_t-1)
<ul>
<li>每个状态只取决于前面一个状态，而非前面所有状态</li>
<li>即当前隐状态到下一个隐状态的概率</li>
</ul></li>
<li>Discrete 离散的（用矩阵表示 A，维度(k, k)，k是开个隐状态）</li>
</ul></li>
<li><strong>P(y_t|q_t)</strong> Emmission/Measurement Probability 发射概率 / 观测概率
<ul>
<li>P(y_t|q_1,..., q_t-1, q_t, y_1,..., y_t-1) = P(y_t|q_t)
<ul>
<li>已知当前状态 q_t，得到事实变化 y_t 的概率</li>
</ul></li>
<li>Discrete or Continuous</li>
<li>若是离散时，可以用矩阵表示 B，维度(k, L)，L是y的取值范围（也是离散的）</li>
<li>若是连续的，无法使用矩阵表示，y可能是连续的一个分布</li>
</ul></li>
<li>两个概率决定了整个隐马尔可夫模型</li>
<li><p>在信号中用HMM也很多，时间轴每段范围的信号就相当与一个y_t <img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm.png" alt="hmm" style="zoom:67%;"></p></li>
<li>每个隐状态的概率和为1，则矩阵行和为1</li>
<li>P(X) = ∫_y P(X, Y) dy</li>
<li><strong>P(y_1, y_2, y_3)</strong> = Σq_1 Σq_2 Σq_3 P(y_1, y_2, y_3, q_1, q_2, q_3) = Σq_1 Σq_2 Σq_3 <strong>P(y_3|q_3)P(q_3|q_2)*P(y_2|q_2)P(q_2|q_1)*P(y_1|q_1)P(q_1)</strong>
<ul>
<li>P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|y_1, y_2, q_1, q_2, q_3)*P(y_1, y_2, q_1, q_2, q_3)</li>
<li>利用马尔可夫性质：P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|q_3)*P(q_3|y_1, y_2, q_1, q_2)*P(y_1, y_2, q_1, q_2) = P(y_3|q_3)P(q_3|q_2)*P(y_1, y_2, q_1, q_2)</li>
<li>P(y_3|q_3), P(q_3|q_2), P(y_2|q_2), P(q_2|q_1), P(y_1|q_1) 在马尔可夫参数A, B矩阵中可得</li>
<li><strong>P(q_1) 是初始状态</strong>，这需要问题给出</li>
<li>则马尔可夫模型需要三个参数，λ={A, B, P(q_1)} (假设y是离散的)</li>
</ul></li>
<li>马尔可夫模型有什么用？
<ul>
<li>找50人讲10个单词（动物名字）</li>
<li><strong>λ_cat = argmax_λ log P(y_1, ..., y_50|λ)</strong>
<ul>
<li>当初始条件为λ，最大为说cat的情况</li>
<li>记录所有单词的λ</li>
<li>用高斯或者高斯混合模型计算λ</li>
</ul></li>
<li>当来了个新人，说其中一个单词，那么他说哪个单词概率最大？
<ul>
<li><strong>P(y_new|λ_cat)</strong>, ... 概率最大为结果</li>
<li>若所有该概率计算结果很小时，说明新人说的不是原10个单词</li>
</ul></li>
</ul></li>
<li>公式范化（计算量较大） <img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm_function.png" alt="hmm_function" style="zoom: 67%;">
<ul>
<li><p>一个定义 <img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB.png" alt="FB" style="zoom:67%;"></p></li>
<li>优化计算量
<ul>
<li>alpha_i(t) = P(y_1, ..., y_t, q_t=i)</li>
<li>alpha_i(1) = P(y_1, q_1=i) = P(y_i|q_1=i)P(q1) = b_i(y_1) P(q_1)</li>
<li>alpha_j(2) = P(y_1, y_2, q_2=j) = Σq_1P(y_1, y_2, q_1=i, q_2=j) = Σq_1 P(y_2|q_2=j)P(q_2=j|q_1=i)*P(y_1, q_1=i) = P(y_2|q_2=j) Σq_1 P(q_2=j|q_1=i)*alpha_i(1) = b_j(y_1) Σq_1 a_ij*alpha_i(1)</li>
<li>开始递归 alpha_j(t) = b_j(y_t) Σq_1 a_ij*alpha_i(t-1) = P(y_1, ..., y_t, q_t=j)
<ul>
<li><strong>P(y_1, ..., y_t) = Σj P(y_1, ..., y_t, q_t=j) = Σj alpha_j(t)</strong></li>
</ul></li>
<li>通过贝叶斯公式 <img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB_2.png" alt="FB_2" style="zoom:67%;">
<ul>
<li>P(Y, q_t=i|λ) = P(Y, q_t=i)P(q_t=i) = P(y_1, ..., y_t|q_t=i)P(y_t+1, ..., y_T|q_t=i)P(q_t=i) = P(y_1, ..., y_t, q_t=i)P(y_t+1, ..., y_T|q_t=i) = alpha_i(t) beta_i(t)</li>
</ul></li>
</ul></li>
</ul></li>
<li>如何学习HMM的参数
<ul>
<li>λ = argmax_λ log P(Y|λ) （极大似然估计）</li>
<li>用EM学习
<ul>
<li>E-step: Σq_1... Σq_t log(P(Y, Q))*P(Q,Y|θ^g) = Σq_1... Σq_t log(P(q_1)*ΠP(q_t|q_t-1)*ΠP(y_t|q_t)) * P(Q,Y|θ^g) = Σq_1... Σq_t [log(P(q_1)+Σlog(a_q_t-1,q_t)+Σlog(b_q_t(y_t))] * P(Q,Y|θ^g)</li>
<li>part 5,6还没看完(需要EM基础)</li>
</ul></li>
</ul></li>
</ul>
<p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLFze15KrfxbGPEHyjxddbbxVvLa5kilFf" target="_blank" rel="noopener">徐亦达系列</a> <a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/dynamic_model.pdf" target="_blank" rel="noopener">pdf</a></p>
<h5 id="three.-crf">Three. CRF</h5>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/09/HMM-CRF/" data-id="ckfy4zh1r000gzeegwvku3i8z" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
          <li>
            <a href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
          </li>
        
          <li>
            <a href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
          </li>
        
          <li>
            <a href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>