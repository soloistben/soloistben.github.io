<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Personal notes">
<meta property="og:type" content="website">
<meta property="og:title" content="MR.C">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Personal notes">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MR.C">
<meta name="twitter:description" content="Personal notes">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Dimensionality-Reduction" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Dimensionality-Reduction/" class="article-date">
  <time datetime="2020-10-02T13:10:36.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>过拟合</p>
<ul>
<li>解决方法：增加数据、正则化、降维</li>
<li><p>原因：<strong>维度灾难</strong></p>
<ul>
<li>在没有很多数据集时，只能降维</li>
<li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li>
<li><p>从几何层面看：</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p>
<ul>
<li>2维正方形面积：1，圆形：pi*(0.5)^2</li>
<li>3维正方体体积：1，球体体积：4/3*pi*(0.5)^3 = K*(0.5)^3</li>
<li>D维超立方体体积：1，超球体体积： K*(0.5)^D</li>
<li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p>
</li>
<li><p>D维外超球体体积：K*1^D = K，环形体积：外超球体体积 - 内超球体体积 = K - K(1-e)^D</p>
<ul>
<li>V外/V内 = 1-(1-e)^D  =1（0&lt;e&lt;1，D趋向无穷大之后，(1-e)^D趋向于0）</li>
<li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Data<ul>
<li>N个p维样本 X（维度N×p）（设I为N维全1列向量）</li>
<li>样本均值 X～=1/N Σ x_i = 1/N X^T I（维度p×1）</li>
<li>方差 S = 1/N Σ (x_i - X~)(x_i - X~)^T = 1/N X^T (I - 1/N I I^T) (1-1/N I I^T)^T X = 1/N X^T H H^T X = 1/N X^T H X<ul>
<li>（维度p×p）</li>
<li>H =  I-1/N I I^T  centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li>
<li>H^T = H, H^2 = H H^T = (I-1/N I I^T) (I-1/N I I^T)^T = I-1/N I I^T = H</li>
<li>H^n = H</li>
</ul>
</li>
</ul>
</li>
<li><p>降维方法</p>
<ul>
<li>直接降维 （特征选择：人工选取重要特征 ）</li>
<li><p>线性降维</p>
<ul>
<li><p><strong>Principal Components Analysis PCA 主成分分析</strong></p>
<ul>
<li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p>
</li>
<li><p>最大投影方差</p>
<ul>
<li>找到一个u_1平面，使投影间距达到最大（投影到u_2平面，距离太小，没有意义）<ul>
<li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li>
<li>第1步：中心化：先将所有数据平移，利于计算，即 x_i - X～</li>
<li>第2步：投影到u_1平面：(x_i - X～)^T u_1（设定 |u_1|=1，即u_1^T u_1=1）</li>
<li>第3步：投影方差 J = 1/N Σ [(x_i - X~)^T u_1]^2 = 1/N Σ [u_1^T (x_i - X~) (x_i - X~)^T u_1] = u_1^T S u_1</li>
</ul>
</li>
<li>u_1~ = argmax u_1^T S u_1  s.t. u_1^T u_1=1<ul>
<li>使用拉格朗日求解</li>
<li>L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)</li>
<li>dL/du_1 = 2 S u_1 - 2λ u_1 = 0  ==&gt; <strong>S u_1 = λ u_1</strong> </li>
<li><strong>u_1为eigen-vector特征向量，λ为eigen-value特征值</strong></li>
<li>解法1：对方差矩阵特征分解，即可求解PCA</li>
<li>解法2：直接对原始数据进行操作：中心化后的 HX = UΣV^T 进行奇异值分解（U和V均为正交矩阵），S = X^T H X = X^T H^T H X = (VΣU^T)(UΣV^T) = V Σ^2 V^T（可先忽略1/N常数）（维度p×p），因此直接求解HX奇异值分解，也就是求解了S的特征分解</li>
<li>假设 B = H X X^T H = (UΣV^T)(VΣU^T) = U Σ^2 U^T（维度N×N），则B与S有一样的eigenvalue，（S特征分解得到方向V（主成分）然后通过HX V得到新坐标）（B特征分解直接得到坐标U，称为主坐标分析 <strong>principal coordinate analysis PCoA</strong>）</li>
<li>HX V = UΣV^T V = UΣ（UΣ是坐标矩阵），BUΣ = U Σ^2 U^T UΣ = UΣ Σ^2（UΣ是特征向量组成的矩阵）</li>
</ul>
</li>
</ul>
</li>
<li>最小重构代价<ul>
<li>投影在u_1平面的点恢复到原来样子的代价</li>
<li>设从原p维降到q维（下面u代表特征）<ul>
<li>x_i = Σ_k^p (x_i^T u_k) u_k, x_i ~ = Σ_k^q (x_i^T u_k) u_k  (用特征u_k描述样本x_i，(x_i^T u_k)为距离大小，u_k为单位大小，则第k维描述为(x_i^T u_k) u_k)</li>
<li>代价函数 L = 1/N Σ||x_i - x_i ~||^2 = Σ_(k=q+1)^p u_k T S u_k  = Σ_(k=q+1)^p λ_k (s.t. u_k^T u_k=1)</li>
<li>u_k = argmin L </li>
</ul>
</li>
</ul>
</li>
<li><p>概率角度</p>
<ul>
<li>P-PCA<ul>
<li>设定observed data X为p维（特征为连续型数据），latent variable Z为q维（q&lt;p）</li>
<li>设定Z～N(0, I)（服从高斯分布，q维）</li>
<li>X = WZ + u + ε（X是Z的一个线性变换加噪声），X|Z~N(WZ + u, σ^2 I)，X~N(u, WW^T+σ^2 I)</li>
<li>噪声ε~N(0, σ^2 I)（p维）</li>
<li>这也是一种Linear Gaussian Model（称该模型各向同性，对各方向影响是一样的）</li>
</ul>
</li>
<li><p>求P(Z), P(X|Z), P(X), 最后求后验P(Z|X)、用EM求参数W, u, σ</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR4.png" alt="DR4" style="zoom:75%;"></p>
</li>
<li><p>从服从高斯分布的Z中，投影点在方向W，进行线性变换得到X，也得到服从高斯分布的X|Z，方向W上有很多的高斯分布（各向同性）；X的分布不在方向W上，且中间很宽（因为高斯分布中间高两边低）<a href="https://www.bilibili.com/video/BV1aE411o7qd?p=27" target="_blank" rel="noopener">详情</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>MDS</li>
</ul>
</li>
<li>非线性降维<ul>
<li>流型</li>
<li>Isomap</li>
<li>LLE</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Dimensionality-Reduction/" data-id="ckfs9msyv0005g6egauqwztfk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SVM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/SVM/" class="article-date">
  <time datetime="2020-10-02T08:50:04.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/SVM/">SVM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Data : N个p维样本 X（维度N×p），y_i = {-1,1}</li>
<li><p>SVM 三宝：间隔，对偶，核技巧</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p>
</li>
<li><p>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</p>
</li>
<li><p><strong>hard-margin SVM</strong>（硬间隔）</p>
<ul>
<li>最大间隔分类器 = max margin(w, b)   s.t. y_i(w^T x_i+b) &gt; 0 for i = 1,…,N</li>
<li>点到直线距离，垂直线最短</li>
<li>margin(w, b) = min distance(w, b, x_i) = <strong>min 1/||w|| |w^T x_i + b|</strong></li>
<li>→ max_w,b min_x 1/||w|| |w^T x_i + b| (s.t. y_i(w^T x_i+b) &gt; 0) = max_w,b 1/||w|| min_x y_i(w^T x_i + b) (存在r&gt;0，使y_i(w^T x_i + b) =r，可以设置r=1)</li>
<li>→ max 1/||w|| s.t. min y_i(w^T x_i + b) = 1 → <strong>min 1/2 w^T w  s.t. y_i(w^T x_i + b) &gt;=1</strong>(convex optimization 二次凸优化问题)(primal problem原问题)</li>
<li>拉格朗日：L(w, b, λ) = 1/2 w^T w + Σ λ_i(1-y_i(w^T x_i + b)) (λ_i &gt;= 0, (1-y_i(w^T x_i + b)) &lt;= 0；若(1-y_i(w^T x_i + b))&gt;0，L为正无穷，无解；仅在 λ_i=0， (1-y_i(w^T x_i + b)) =0，达到最大L)</li>
<li><strong>primal problem 原问题</strong>&lt;=&gt; <strong>min_w,b max_λ L(w, b, λ)  s.t. λ_i &gt;= 0</strong> （对w，b没有限制）→ min 1/2 w^T w</li>
<li><strong>dual problem 对偶问题</strong>：<strong>max_λ min_w,b L(w, b, λ)  s.t. λ_i &gt;= 0</strong>  <ul>
<li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li>
<li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li>
<li>dL/db = d[Σ λ_i(1-y_i(w^T x_i + b))]/db = d[- Σ λ_i y_i b]/db = -<strong>Σ λ_i y_i =0</strong>，带入原式L = 1/2 w^T w + Σ λ_i - Σ λ_i y_i w^T x_i</li>
<li>dL/dw = w - Σ λ_i y_i x_i = 0  → <strong>w = Σ λ_i y_i x_i</strong>，带入原式L = 1/2 w^T w + Σ λ_i - w^T w  = Σ λ_i - 1/2 w^T w </li>
<li>→ <strong>min 1/2 w^T w - Σ λ_i  s.t. λ_i&gt;=0, Σ λ_i y_i =0</strong>  （此处不能求λ偏导）</li>
<li>→ min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i  s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li><strong>KKT条件</strong>：参数偏导为0（dL/db=0，dL/dw=0，dL/dλ=0），λ_i(1-y_i(w^T x_i + b))=0， λ_i&gt;=0，(1-y_i(w^T x_i + b)&lt;=0</li>
<li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li>
<li>w~ = Σ λ_i y_i x_i, （存在x_k, y_k，1-y_k(w^T x_k + b)=0）b~ = y_k - w^T x_k = y_k - (Σ λ_i y_i x_i^T) x_k</li>
</ul>
</li>
<li><p><strong>f(x) = sign(w~^T x + b~)</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p>
<ul>
<li>落在虚线的样本点就是x_k（y_k(w^T x_k + b)=1），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>soft-margin SVM</strong>（软间隔）</p>
<ul>
<li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li>
<li><p>soft-margin SVM在hard-margin SVM基础上允许一点点错误，min 1/2 w^T w + loss </p>
<ul>
<li>分错点的个数：loss = Σ I{y_i(w^T x_i + b)&lt;1} （关于w是不连续的，无法求导，因此不采取）</li>
<li><p>hinge 距离：hinge loss = max{0, 1-y_i(w^T x_i + b)}</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p>
</li>
<li><p>min 1/2 w^T w + C Σ max{0, 1-y_i(w^T x_i + b)}  s.t. y_i(w^T x_i + b)&gt;=1 （超参数C）</p>
</li>
<li><p>→ 设定ξ_i = y_i(w^T x_i + b)，<strong>min 1/2 w^T w + C Σ ξ_i</strong>  s.t. y_i(w^T x_i + b)&gt;=1-ξ_i, ξ_i&gt;=0（同样用对偶问题方式来求解）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>约束优化问题</p>
<ul>
<li>primal problem 原问题：<strong>min f(x)  s.t. m_i(x)&lt;=0, n_j(x)=0 (i=1,…,M, j=1,…,N)</strong></li>
<li>原问题的无约束形式（关于x的函数）：拉格朗日：L(x, λ, η) = f(x) + Σ λ_i m_i(x) + Σ η_i n_i(x)  → <strong>min_x max_λ,η L(x, λ, η)  s.t. λ_i&gt;=0</strong> </li>
<li>证明两者等价：如果违法约束m_i(x)&gt;0，max_λ L → ∞；反之，max_λ L 必有最大值（λ_i=0时）（即排除了m_i(x)&gt;0情况，过滤掉违反约束的情况）</li>
<li>dual problem 对偶问题（关于λ,η的函数）：<strong>max_λ,η min_x L(x, λ, η)  s.t. λ_i&gt;=0</strong><ul>
<li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （max_λ,η min_x L(x, λ, η)  &lt;= min_x max_λ,η L(x, λ, η)）</li>
<li>证明：min_x L &lt;= L &lt;= max_λ,η L  →  A(λ,η) &lt;= L &lt;= B(x)  →  A(λ,η) &lt;= B(x)  →  max A(λ,η) &lt;= min B(x)  </li>
<li>→ max_λ,η min_x L(x, λ, η)  &lt;= min_x max_λ,η L(x, λ, η) </li>
<li>（在min_x L已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li>
<li>强对偶性：对偶问题=原问题</li>
</ul>
</li>
<li><p>几何解释</p>
<ul>
<li>primal problem: min f(x)  s.t. m_1(x)&lt;=0 (D定义域，D=dom_f ∩ dom_m_1) <strong>原问题最优解：p* = min f(x)</strong></li>
<li>L(x, λ) = f(x) + λ m_1(x) s.t. λ&gt;=0 <strong>对偶最优解：d* = max_λ min_x L(x, λ)</strong></li>
<li><p>将问题投影入二维空间：引入集合(区域) G = {(m_1(x), f(x))|x∈D}</p>
<ul>
<li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li>
<li>凸集是指集合内任意两点的连线都在集合内</li>
<li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li>
<li>目标函数f如果不是凸函数，则不是凸优化问题</li>
<li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li>
<li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li>
<li>结论：凸函数的局部最优解就是全局最优解</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p>
<ul>
<li><p><strong>p* = inf {f(x)|(m_1(x), f(x))∈G, m_1(x)&lt;=0}</strong>（集合中没有最小值概念，对应的是下确界）</p>
<ul>
<li>P* 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li>
</ul>
</li>
<li><p>d* = max_λ g(λ) , g(λ) = min_x  f(x) + λ m_1(x) , <strong>g(λ) = inf {f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G}</strong></p>
<ul>
<li>一条过原点直线 f(x) + λ m_1(x) = 0 (斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）g(λ) &lt;= p* </li>
<li>当调整斜率λ*，得到一个 g(λ*) = f(x) + λ* m_1(x) 同时与G的俩角相切，此时，直线与纵轴的交点为d*（绿线）</li>
<li><p>d* &lt;= p* （<strong>凸优化+slater条件 → d* = p*</strong>）（SVM是二次规划问题，符合slater条件）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>slater条件</p>
<ul>
<li>Convex + Slater → Strong Duality （充分不必要条件）</li>
<li>定义：存在x~在relint，使m_i(x)&lt;0 (i=1,…,M)<ul>
<li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li>
<li>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = Ax + b（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li>
</ul>
</li>
<li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li>
<li>放松的slater条件：在m_i(x)中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足m_i(x)&lt;0 （凸二次规划问题：目标函数f是凸的，不等式约束m_i是仿射函数，等式约束n_j也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li>
</ul>
</li>
<li>KKT条件<ul>
<li>KKT  &lt;=&gt; Strong Duality (d* = p*)（充要条件）</li>
<li>从p*得到最优x*，从d*得到λ*、η*</li>
<li><strong>可行域（可行条件）：m_i(x*)&lt;=0, n_j(x*)=0, λ*&gt;=0</strong></li>
<li>互补松弛<ul>
<li>d* = max_λ,η g(λ,η) = g(λ*, η*) = min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*) = f(x*) + Σ λ_i* m_i(x*) + Σ η_i* n_i(x*) = f(x*) + Σ λ_i* m_i(x*) &lt;= f(x*) = p*</li>
<li>（λ_i&gt;=0，m_i&lt;=0，则 (λ_i m_i) &lt;= 0）</li>
<li><strong>互补松弛条件 ：Σ λ_i* m_i(x*)  = 0 → λ_i* m_i(x*)</strong> </li>
</ul>
</li>
<li>梯度为0<ul>
<li>min_x L(x, λ*, η*) &lt;= L(x*, λ*, η*)</li>
<li>x*是对应x最小值，则 <strong>dL/dx = 0</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>kernel SVM</strong></p>
<ul>
<li>Kernel Method（思想角度）</li>
<li>Kernel Trick（计算角度）</li>
<li><p>Kernel function</p>
<ul>
<li><strong>非线性带来高维转换（从模型角度）</strong><br>|  线性可分  | 一点点错误 |  严格非线性  |<br>|  :—-  | :—-  | :—-  |<br>| PLA（感知机算法） | Pocket Algorthm | Φ(x)+PLA |<br>| Hard-Margin SVM | Soft-Margin SVM | Φ(x)+Hard-Margin SVM = Kernel SVM |</li>
<li><p>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</p>
<ul>
<li><p>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p>
</li>
<li><p>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</p>
</li>
<li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）</li>
<li><p><strong>非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p>
</li>
<li><p>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</p>
</li>
</ul>
</li>
<li><strong>对偶表示带来内积（从优化角度）</strong><ul>
<li>从频率视角归化到优化问题</li>
<li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题<ul>
<li>Doul Problem: min 1/2 Σ Σ λ_i λ_j y_i y_j x_i^T x_j - Σ λ_i  s.t. λ_i&gt;=0, Σ λ_i y_i =0</li>
<li>内积：x_i^T x_j</li>
<li>非线性转换：Φ(x_i)^T Φ(x_j) （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此Φ(x_i)^T Φ(x_j) 很难i求解和计算量很大）</li>
<li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到Φ(x_i)^T Φ(x_j) 结果</strong>（不需要先求Φ(x)再求内积）</li>
</ul>
</li>
<li><strong>Kernel function : K(x, x’) =  Φ(x)^T Φ(x’) = &lt;Φ(x), Φ(x’)&gt;</strong> <ul>
<li>存在x, x’∈X，使K(x, x’) =  Φ(x)^T Φ(x’)，则K就是一个核函数（如K(x, x’)=exp(-(x-x’)^2/(2σ^2))）</li>
<li>蕴含了非线性转换+内积</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>一般核函数指正定核函数 <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a><ul>
<li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x’)为核函数</li>
<li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x’)，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使K(x, x’) = &lt;Φ(x), Φ(x’)&gt;，则K(x, x’)为正定核函数</li>
<li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x’)，若满足两个条件（对称性、正定性）则为正定和函数<ul>
<li>对称性：K(x, x’) = K(x’, x)</li>
<li>正定性：任取N个元素，x_1,x_2,…,x_N∈X，对应的Gram矩阵是半正定的（K=[K(x_i, x_j)]）（两个定义等价，即证明：<strong>K(x, x’) = &lt;Φ(x), Φ(x’)&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li>
<li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（&lt;f, g&gt; = &lt;g, f&gt;）、正定性（内积大于等于0，&lt;f, f&gt; &gt;= 0）、线性性（&lt;r_1 f_1 + r_2 f_2 , g&gt; = r_1 &lt;f_1, g&gt; + r_2 &lt;f_2, g&gt;））  </li>
</ul>
</li>
<li>必要性证明<ul>
<li>在Hilbert Space中的Φ(x)具有对称性性质，K(x, x’) = &lt;Φ(x), Φ(x’)&gt; = &lt;Φ(x’), Φ(x)&gt; = K(x’, x)</li>
<li>K=[K(x_i, x_j)]（维度N×N）（半正定：任意a列向量，a^T K a &gt;=0）</li>
<li>a^T K a = Σ Σ a_i a_j K_ij = Σ Σ a_i a_j K(x_i, x_j) = Σ Σ a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; =线性性= Σ Σ a_i a_j Φ(x_i)^T Φ(x_j) = Σ a_i Φ(x_i)^T Σ a_j Φ(x_j) = [Σ a_i Φ(x_i)]^T Σ a_j Φ(x_j) = &lt;Σ a_i Φ(x_i), Σ a_j Φ(x_j)&gt; = ||Σ a_i Φ(x_i),||^2 &gt;= 0，半正定性</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/SVM/" data-id="ckfs9msz8000fg6egvcjb5p2i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Decision-Tree" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Decision-Tree/" class="article-date">
  <time datetime="2020-10-02T08:49:49.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Decision-Tree/">Decision_Tree</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>基于数据特征构造决策树</p>
<ul>
<li>有向边</li>
<li><p>结点</p>
<ul>
<li>内部结点(internal node)-&gt;表示特征</li>
<li><p>叶子结点(leaf node)-&gt;表示类别</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p>
</li>
<li><p>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</p>
</li>
</ul>
</li>
<li>决策树表示给定特征条件下类的条件概率分布。<ul>
<li>一个条概率分布定义特征空间的一个划分上</li>
<li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li>
</ul>
</li>
<li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果<br><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li>
</ul>
</li>
<li><p>决策树学习过程：特征选择、决策树生成、剪枝</p>
<ul>
<li><p><strong>ID3算法</strong>（分类、多叉树）</p>
<ul>
<li><p>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）</p>
<ul>
<li>Information Gain 信息增益。信息增益越大对应的特征越重要</li>
<li>Entropy 熵，表示随机变量不确定的度量</li>
<li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"></p>
</li>
</ul>
</li>
<li><p>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</p>
</li>
<li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li>
</ul>
</li>
<li><p><strong>C4.5算法</strong>（分类、多叉树）</p>
<ul>
<li><p>特征选择</p>
<ul>
<li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p>
</li>
</ul>
</li>
<li><p>生成：与ID3算法类似</p>
</li>
<li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li>
<li>二叉树模型会比多叉树运算效率高</li>
<li>无剪枝</li>
</ul>
</li>
<li><p><strong>CART</strong> classification and regression tree（分类、回归、二叉树）</p>
<ul>
<li><p>分类</p>
<ul>
<li><p>特征选择</p>
<ul>
<li>Gini基尼指数</li>
<li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li>
<li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"></p>
</li>
<li><p>生成</p>
<ul>
<li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li>
<li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li>
</ul>
</li>
<li><p>损失函数</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p>
<ul>
<li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p>
</li>
</ul>
</li>
<li><p>回归</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Decision-Tree/" data-id="ckfs9msyr0003g6eg78s9qf07" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/11/Statistics/">Statistics</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Statistics-for-Machine-Learning"><a href="#Statistics-for-Machine-Learning" class="headerlink" title="Statistics for Machine Learning"></a>Statistics for Machine Learning</h4><h5 id="One-两大派系"><a href="#One-两大派系" class="headerlink" title="One. 两大派系"></a>One. 两大派系</h5><ul>
<li><strong>频率派：统计机器学习</strong>（设计模型，找到loss function，设计算法。本质为优化问题）</li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为求积分问题(Monte Carlo method, MCMC)，直接求解过于复杂，则衍生出概率图模型）</li>
<li>Data<ul>
<li>X: data, X={x_1, x_2, …, x_n}^T  Dimension(N, P)</li>
<li>θ: parameter,   X~p(X|θ)</li>
</ul>
</li>
<li><strong>频率派</strong><ul>
<li>θ为未知常数；X为随机变量</li>
<li>loss = P(X|θ) = Π P(x_i|θ)<ul>
<li>x_1, x_2, …, x_n之间独立同分布</li>
</ul>
</li>
<li>Maximum Likelihood Estimation 极大似然估计<ul>
<li>θ_MLE = argmax_θ log P(X|θ)</li>
</ul>
</li>
</ul>
</li>
<li><strong>贝叶斯派</strong><ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率<ul>
<li>P(θ|X) = P(X, θ)/P(X) = P(X|θ)P(θ)/P(X) = likelihood*prior / ∫_θ P(X|θ) dθ</li>
</ul>
</li>
<li>Maximum A Posterior Probability 最大后验概率<ul>
<li>找到θ在分布中最大值点</li>
<li>θ_MLE = argmax_θ P(X|θ) = argmax_θ P(X|θ)P(θ)</li>
<li>P(θ|X)其分母是不变的，则θ_MLE与分子成正比，只需要算分子</li>
</ul>
</li>
<li>贝叶斯估计<ul>
<li>P(θ|X) = P(X|θ)P(θ) / ∫_θ P(X|θ) dθ</li>
<li>必须完整计算整个分子式</li>
</ul>
</li>
<li>贝叶斯预测<ul>
<li>X (train), X~ (test),  X -&gt; θ -&gt; X~</li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li>P(X~|X) = ∫_θ P(X~, θ|X) dθ = ∫_θ P(X~|X)P(θ|X) dθ</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Two-Linear-Regression"><a href="#Two-Linear-Regression" class="headerlink" title="Two. Linear Regression"></a>Two. Linear Regression</h5><p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线f(w) = w^T x + b（偏置b可先忽略）</p>
<ul>
<li><p>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）</p>
<ul>
<li>线性（属性线性、全局线性、系数线性）<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul>
</li>
<li>全局性<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul>
</li>
<li>数据未加工<ul>
<li>预处理：PCA，流行</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>矩阵表达</strong></p>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）<ul>
<li><strong>L(w) = Σ||w^T x_i - y_i||^2</strong> = Σ(w^T x_i - y_i)^2 = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y</li>
</ul>
</li>
<li><strong>w~ = argmin L(w)</strong></li>
<li>求导 dL/dw =  2 X^T X w - 2 X^T Y = 0  —-&gt; X^T X w = X^T Y<ul>
<li><strong>w~ = (X^T X)^-1 X^T Y</strong> (伪逆：(X^T X)^-1 X^T)</li>
</ul>
</li>
<li>x_3的误差为(w^T x_3 - y_3)，即所有误差分成一小段一小段</li>
</ul>
</li>
<li><p><strong>几何意义</strong></p>
<ul>
<li>f(w) = w^T x &lt;=&gt; f(β) = x^T β</li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li><p>目标：在p维空间中找到一条直线f(β)离Y最近，即Y在p维空间的投影</p>
<ul>
<li><p>若向量a与向量b垂直，则 a^T b = 0</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p>
</li>
<li><p>虚线为(Y - Xβ) 与X的p维空间垂直，X^T (Y-Xβ) = 0 —-&gt; β = (X^T X)^-1 X^T Y</p>
</li>
<li>误差分散在p个维度上</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>概率角度</strong></p>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ^2)</li>
<li>y = f(w) + ε = w^T x + ε<ul>
<li>y|x,w ~ N(w^T x, σ^2)  &lt;=&gt; <strong>P(y|x,w)</strong> =1/(sqrt(2*pi)*σ) exp(-(y - w^T x)^2/(2*σ^2))</li>
</ul>
</li>
<li>定义log-likelihood： <ul>
<li>L_MLE (w) = log P(y|x,w) = log Π P(y_i|x_i,w) = Σ log P(y_i|x_i,w) = Σ[log(1/(sqrt(2*pi)*σ)) + log(exp(-(y_i - w^T x_i)^2/(2*σ^2)))] = Σ[log(1/(sqrt(2*pi)*σ)) -(y_i - w^T x_i)^2/(2*σ^2)]</li>
<li>样本之间独立同分布 </li>
<li>w~ = argmax L_MLE (w) = argmax -(y_i - w^T x_i)^2/(2*σ^2) = argmin (y_i - w^T x_i)^2</li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Regularization 正则化</strong> </p>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong><ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：argmin [L(w)+λP(w)]</li>
</ul>
</li>
<li>L1 -&gt; Lasso<ul>
<li>P(w) = ||w||_1</li>
</ul>
</li>
<li>L2 -&gt; Ridge 岭回归<ul>
<li>P(w) = ||w||_2 = w^T w</li>
<li>权值衰减</li>
<li>J(w) = Σ||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y<ul>
<li>w~ = argmin J(w)</li>
<li>dJ/dw = 2 (X^T X + λ I)w - 2 X^T Y = 0, w~ = (X^T X + λ I)^-1 X^T Y</li>
<li>X^T X 是半正定矩阵+对角矩阵=(X^T X + λ I)正定矩阵，必然<strong>可逆</strong></li>
</ul>
</li>
</ul>
</li>
<li>贝叶斯的角度<ul>
<li>参数w服从分布，w~N(0,σ_0^2) —&gt; <strong>P(w)</strong> = 1/(sqrt(2*pi)*σ_0) exp(||w||^2/(2*σ_0^2))</li>
<li>P(w|y) = P(y|w)P(w) / P(y)</li>
<li>MAP: w~ = argmax P(w|y) = argmax P(y|w)P(w) = argmax log(P(y|w)P(w)) = argmax log[1/(2*pi*σ_0*σ) exp(-(y_i - w^T x_i)^2/(2*σ^2) -||w||^2/(2*σ_0^2))] = argmin [(y_i - w^T x_i)^2 + σ^2/σ_0^2||w||^2] = argmin [L(w)+λP(w)]</li>
<li>λ = σ^2/σ_0^2</li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Three-Linear-Classification"><a href="#Three-Linear-Classification" class="headerlink" title="Three. Linear Classification"></a>Three. Linear Classification</h5><ul>
<li><p>线性回归——&gt;激活函数，降维——-&gt;线性分类</p>
</li>
<li><p>硬分类：0/1</p>
<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul>
</li>
<li><p>软分类：[0,1]区间内概率</p>
<ul>
<li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li>
<li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li>
</ul>
</li>
<li><p><strong>Perceptron 感知机</strong> (1957年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>判别模型</li>
<li>样本：{(x_i, y_i)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：f(x) = sign(w^T x + b) （w^T x大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）<ul>
<li>L(w) = Σ I{y_i * (w^T x_i) &lt; 0} （非连续函数，不可导）</li>
<li>L(w) = Σ -y_i w^T x_i, dL = -y_i x_i</li>
</ul>
</li>
<li>若是非线性可分，可是使用pocket algorithm</li>
</ul>
</li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数N_1，均值X_c1，方差S_c1，负样本个数N_2，均值X_c2，方差S_c2，（S_c1 = 1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T）</li>
<li>思想：类内小，类间大<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 w^T x（因为垂直，则Z平面即w向量）<ul>
<li>（前提设置||w||=1）</li>
<li>则样本点投影到Z平面为：|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i</li>
</ul>
</li>
</ul>
</li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong> <ul>
<li>N_1 = 1/N_1 Σ w^T x_i</li>
<li>S_1 = 1/N_1 Σ (w^T x_i - Z_1) (w^T x_i - Z_1)^T</li>
<li>类间：(Z_1-Z_2)^2</li>
<li>类内：S_1+S_2</li>
</ul>
</li>
<li>策略：L(w) = (Z_1-Z_2)^2 / (S_1+S_2) = [w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w] / [ w^T (S_c1+ S_c2) w ]<ul>
<li>分子 = [w^T (1/N_1 Σ x_i - 1/N_2 Σ x_i)]^2 = [w^T (X_c1 - X_c2)]^2 = w^T (X_c1 - X_c2)(X_c1 - X_c2)^T w</li>
<li>分母 =  w^T S_c1 w +  w^T S_c2 w =  w^T (S_c1+ S_c2) w<ul>
<li>S_1 =  1/N_1 Σ (w^T x_i - 1/N_1 Σ w^T x_j)(w^T x_i - 1/N_1 Σ w^T x_j)^T=w^T [1/N_1 Σ (x_i - X_c1)(x_i - X_c1)^T] w = w^T S_c1 w</li>
</ul>
</li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li>L(w) = w^T S_b w / w^T S_w w</li>
<li>w~ = argmax L(w)<ul>
<li>dL/dw = 2*S_b w (w^T S_w w)^-1 + (w^T S_b w) * (-1) (w^T S_w w)^-2 * 2 * S_w w = 0</li>
<li>S_b w (w^T S_w w) = (w^T S_b w) S_w w （(w^T S_w w) 最终计算得一个实数，一维，没有方向）（求解w～关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li>w = (w^T S_w w)/(w^T S_b w) * S_w^-1 * S_b w，正比于(S_w^-1 * S_b w) ，正比于(S_w^-1 *(X_c1 - X_c2))</li>
<li>（S_b w = (X_c1 - X_c2)(X_c1 - X_c2)^T w，(X_c1 - X_c2)^T w为实数）</li>
<li>（若S_w是对角矩阵，各向同性，S_w正比于单位矩阵，则w正比于(X_c1 - tX_c2)</li>
</ul>
</li>
</ul>
</li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul>
</li>
<li><p><strong>Logistic Regression</strong></p>
<ul>
<li>线性回归——&gt;sigmoid——-&gt;线性分类</li>
<li>判别模型</li>
<li>model<ul>
<li>sigmoid(x) = 1/(1+e^-x)，将w^T x映射到处于[0,1]区间的概率值p</li>
<li>p_1 = P(y=1|x) = sigmoid(w^T x) = 1/(1+e^(w^T x))</li>
<li>p_0 = P(y=0|x) = sigmoid(w^T x) = e^(w^T x)/(1+e^(w^T x))</li>
<li>综合表达：P(y|x) = p_1^y * p_0^(1-y)</li>
</ul>
</li>
<li>w~ = argmax P(Y|X) = argmax log Π P(y_i|x_i) = argmax Σ log P(y_i|x_i) = argmax Σ [y_i*log p_1 + (1-y_i)*log p_0] （-cross entropy）<ul>
<li>MLE &lt;=&gt; loss function (min cross entropy)</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Gaussian Discriminant Analysis</strong></p>
<ul>
<li>生成模型、连续<ul>
<li>y~ = argmax P(y|x) = argmax P(x|y)P(y)</li>
<li>分类：最终比较P(y=0|x)，P(y=1|x)大小</li>
<li>P(y|x)正比于P(x|y)P(y)，即联合概率P(x, y)</li>
</ul>
</li>
<li>Data：N个d维样本，二分类(0,1)，正样本个数N_1，方差S_1，负样本个数N_2，方差S_2</li>
<li><strong>prior probability</strong> <ul>
<li>先验概率服从伯努利分布</li>
<li>y ~ Bernoulli，P(y=1) = p，P(y=0) = 1-p</li>
<li>P(y) = p^y*(1-p)^(1-y)</li>
</ul>
</li>
<li><strong>conditional probability</strong><ul>
<li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li>
<li>x|y=1 ~ N(u_1, σ)</li>
<li>x|y=0 ~ N(u_2, σ)</li>
<li>方差一样（权值共享），均值不一样</li>
<li>P(x|y) = N(u_1, σ)^y * N(u_2, σ)^(1-y)</li>
</ul>
</li>
<li><strong>loss function</strong><ul>
<li>log MLE =&gt; L(θ) =  log Π P(x_i, y_i) = Σ log [P(x_i|y_i)P(y_i)] = Σ[log P(x_i|y_i) + log P(y_i)] =  Σ[log N(u_1, σ)^y_i * N(u_2, σ)^(1-y_i) + log p^y_i*(1-p)^(1-y_i)] = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ) + y_i*log p + (1-y_i)*log (1-p)]</li>
<li>θ = (u_1, u_2, σ, p)</li>
<li>θ ~ = argmax L(θ)</li>
<li>求解4个参数<ul>
<li>p<ul>
<li>相关部分 L = Σ [log p^y_i + log (1-p)^(1-y_i)]</li>
<li>dL/dp = Σ [y_i/p - (1-y_i)/(1-p)] = 0  =&gt;  Σ [y_i*(1-p)- (1-y_i)*p] = Σ (y_i - p) = 0</li>
<li>p~ = 1/N Σ y_i  = N_1/N（二分类（0,1），Σ y_i = N_1）</li>
</ul>
</li>
<li>u_1 （同理 u_2）<ul>
<li>相关部分 L = Σ y_i*log N(u_1, σ) = Σ y_i*log [1/((2*pi)^(d/2)*σ^(1/2)) exp((x_i-u_1)^T(x_i-u_1)/-2*σ)</li>
<li>u_1 = argmax L = argmax Σ y_i * [(x_i-u_1)^T(x_i-u_1)/-2*σ] =  argmax -1/2 Σ y_i * [(x_i-u_1)^T(x_i-u_1)σ^-1] = argmax -1/2 Σ y_i * [x_i^T σ^-1 x_i - 2*u_1^T σ^-1 x_i + u_1^T σ^-1 u_1] </li>
<li>dL/du_1 = -1/2 Σ y_i * [-2*σ^-1 x_i + 2*σ^-1 u_1] = 0  =&gt;   Σ y_i * (u_1 - x_i) = 0</li>
<li>u_1 = Σ y_i x_i / Σ y_i = Σ y_i x_i / N_1</li>
</ul>
</li>
<li>σ<ul>
<li>相关部分 L = Σ[y_i*log N(u_1, σ) + (1-y_i)*log N(u_2, σ)] = Σlog N(u_1, σ) +  Σlog N(u_2, σ)<ul>
<li>（二分类，非0即1，可以拆分算，可以省去y_i）</li>
</ul>
</li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li>
<li>σ = 1/N (N_1*S_1 + N_2*S_2)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Naive Bayes</strong></p>
<ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong><ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
<li>最简单概率图模型</li>
</ul>
</li>
<li>生成模型、离散</li>
<li>Data<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c_1, c_2, …,c_k}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul>
</li>
<li><strong>prior probability</strong> <ul>
<li>P(Y=c_k)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul>
</li>
<li><strong>conditional probability</strong><ul>
<li>P(X=x|Y=c_k) = P(X^(1)=x^(1), …, X^(d)=x^(d)|Y=c_k) = ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>特征条件独立</li>
<li>^(1) 上标表示第1维度</li>
</ul>
</li>
<li><strong>joint probability distributions</strong><ul>
<li>联合概率分布</li>
<li>P(X, Y) = P(X|Y)P(Y)</li>
</ul>
</li>
<li><strong>posterior probability</strong><ul>
<li>P(Y=c_k|X=x) = P(X=x|Y=c_k)P(Y=c_k)/ΣP(X=x|Y=c_k)P(Y=c_k) = P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)/Σ P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>则分类器为<ul>
<li>y=f(x) = argmax P(Y=c_k|X=x) = argmax P(Y=c_k)ΠP(X^(j)=x^(j)|Y=c_k)</li>
<li>分母不变，则仅于分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>loss function</strong><ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li>L(Y, f(x)) = 1 if Y!=f(x) or 0 if Y==f(x)<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul>
</li>
<li>期望风险函数：R_exp(f) = E[L(Y, f(x))]<ul>
<li>根据联合概率分布：R_exp(f)  = E_x Σ[L(c_k|f(x))]P(c_k|X)</li>
</ul>
</li>
<li>f(x) = argmin Σ[L(c_k|f(x))]P(c_k|X) <ul>
<li>根据L(Y, f(x))函数展开，消去Y==f(x)项</li>
<li>f(x) = <strong>argmin ΣP(y!=c_k|X=x)</strong> = argmin (1-P(y=c_k|X=x)) = <strong>argmax P(y=c_k|X=x)</strong></li>
</ul>
</li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c_k类别的最大概率为多少）</li>
</ul>
</li>
<li>详情案例见统计学习方法(第二版)63页</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckfs9mszi000mg6egofqz84a0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Bayes-MRF" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/12/Bayes-MRF/" class="article-date">
  <time datetime="2020-08-12T12:10:46.000Z" itemprop="datePublished">2020-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/12/Bayes-MRF/">Bayes_MRF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场"><a href="#Bayes-贝叶斯-amp-Markov-Random-Fields-马尔可夫随机场" class="headerlink" title="Bayes 贝叶斯 &amp; Markov Random Fields 马尔可夫随机场"></a>Bayes 贝叶斯 &amp; Markov Random Fields 马尔可夫随机场</h4><h5 id="One-前提"><a href="#One-前提" class="headerlink" title="One. 前提"></a>One. 前提</h5><ul>
<li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）<ul>
<li>Representation 表示<ul>
<li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li>
<li>无向图 Markov Network (无向，则结点的概率仅取决于1阶邻居)</li>
<li>高斯图 <ul>
<li>Gassian Bayes</li>
<li>Gassian Markov</li>
</ul>
</li>
</ul>
</li>
<li>Inference 推断<ul>
<li>精确推断</li>
<li>近似推断<ul>
<li>确定性近似（变分推断）</li>
<li>随机性近似（MCMC）</li>
</ul>
</li>
</ul>
</li>
<li>Learning 学习<ul>
<li>参数学习<ul>
<li>完备数据（非隐变量）（有向，无向）</li>
<li>隐变量（EM）</li>
</ul>
</li>
<li>结构学习 (学习更好的图结构，参数)</li>
</ul>
</li>
</ul>
</li>
<li><strong>高维随机变量</strong> P(x_1, x_2, …, x_p) 计算量太大<ul>
<li>边缘概率 P(x_i)</li>
<li>条件概率 P(x_j|x_i)</li>
</ul>
</li>
<li><strong>运算原则</strong><ul>
<li>sum rule: P(x_1) =  ∫ P(x_1, x_2) dx_2 (边缘概率)</li>
<li>poduct rule: P(x_1, x_2) = P(x_1)*P(x_2|X_1) = P(x_2)*P(x_1|X_2)</li>
<li>chain rule: P(x_1, x_2, …, x_p) = Π P(x_i|x_1, x_2, …, x_i-1)</li>
<li>bayesian rule: P(x_2|x_1) = P(x_1, x_2) / P(x_1) = P(x_1, x_2) / ∫ P(x_1, x_2) dx_2 =  P(x_2)*P(x_1|x_2) /  ∫ P(x_1, x_2) dx_2</li>
</ul>
</li>
<li><strong>缺点</strong><ul>
<li><font color="red">高维复杂 P(x_1, x_2, …, x_p) 计算量太大</font></li>
<li>简化 <ul>
<li>每个<strong>维度之间相互独立</strong>    (特性太强了)<ul>
<li>P(x_1, x_2, …, x_p) = Π P(x_i)</li>
<li>Naive Bayes 朴素贝叶斯 P(x|y) = Π P(x_i|y)</li>
</ul>
</li>
<li>Markov Property 马尔可夫特性<ul>
<li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li>
<li>x_i+1 只与 x_i相关，与其他 x_i-1,…,x_1无关</li>
</ul>
</li>
<li><strong>条件独立性</strong>（可降低计算复杂度）<ul>
<li>给定x_B情况下，集合x_A与集合x_C无关 （x_A，x_B，x_C无交集）</li>
<li>x_B 只与x_C相关<br><a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a>    <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Two-Bayes"><a href="#Two-Bayes" class="headerlink" title="Two. Bayes"></a>Two. Bayes</h5><ul>
<li>链式法则 P(x_1, x_2, …, x_p) = Π P(x_i|x_1, x_2, …, x_i-1)</li>
<li><p>因子分解 P(x_1, x_2,…, x_p) = Π P(x_i|x_p(i)),  x_p(i)为x_i父亲集合 （条件独立性）<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p>
</li>
<li><p><strong>tail to tail</strong></p>
<ul>
<li>因子分解 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A)</li>
<li>链式法则 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A,B)<ul>
<li>则 P(C|A) = P(C|A,B)，则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，“倒V路径”</font>）</li>
<li>P(B|A)P(C|A,B) = P(B|A)P(C|A) = P(B,C|A)</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p>
</li>
<li><p><strong>head to tail</strong></p>
<ul>
<li>发生A时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞</font>）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p>
</li>
<li><p><strong>head to head</strong></p>
<ul>
<li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A,B有关系，不独立则难以分解</font>）</li>
<li>因子分解 -&gt; P(A,B,C) = P(A)P(B)P(C|A,B) （父亲结点先于子结点）</li>
<li>链式法则 -&gt; P(A,B,C) = P(A)P(B|A)P(C|A,B)<ul>
<li>P(B) = P(B|A)，则C还没被观察，A,B相互独立</li>
</ul>
</li>
<li>这个模式是想判断 P(A|C) == P(A|C,B)，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p>
<ul>
<li>（<font color="red">若D被观测，路径也是是连通</font>）</li>
</ul>
</li>
<li><p>有向图是的条件独立性（证明发生x_B, x_A, x_C相互独立）</p>
<ul>
<li><p>D-separation</p>
<ul>
<li><p>x_A, x_B, x_C 三个集合两两无交集<br><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p>
</li>
<li><p>若A与C，存在B1，B2关系，且属于x_B集合；存在B3，B4关系，且不属于x_B集合</p>
</li>
<li>则符合：发生x_B, x_A, x_C相互独立 （<strong>全局马尔可夫性</strong>）</li>
<li><p>P(x_i|x_-i) = P(x_i, x_-i) / P(x_i) = p(x) / ∫ P(x_i) dx_i = Π P(x_j|x_p(j)) / ∫ Π P(x_j|x_p(j)) dx_i</p>
<ul>
<li>x_-i表示集合{x_1,…x_p}中去除x_i, <strong>x/x_i</strong></li>
<li>Π P(x_j|x_p(j)) 分成，与x_i有关和无关两部分</li>
<li><p>则 Π P(x_j|x_p(j)) / ∫ Π P(x_j|x_p(j)) dx_i无关部分则可以约去</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"></p>
</li>
<li><p>P(x_i|x_-i)  = P(x_i|x_p(i))，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket马尔可夫态</p>
</li>
<li>一个人与全世界的关系=一个人与身边人的关系</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）</p>
<ul>
<li><p>离散</p>
<ul>
<li><p>单一</p>
<ul>
<li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; P(x|y) = ΠP(x_i|y=1)  (x 是p维)</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p>
</li>
</ul>
</li>
<li><p>混合</p>
<ul>
<li><strong>GMM</strong> 高斯混合模型 -&gt; 做聚类</li>
</ul>
</li>
<li>时间<ul>
<li><strong>Markov Chain</strong> 马尔可夫链</li>
<li><strong>Gaussian Process</strong> 无限维高斯分布</li>
</ul>
</li>
<li>动态模型 = 混合 + 时间<ul>
<li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li>
<li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li>
<li><strong>Partide Filter</strong> （非连续，非高斯）</li>
</ul>
</li>
</ul>
</li>
<li>连续<ul>
<li><strong>Gaussian Bayes Network</strong> 高斯图</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Three-MRF"><a href="#Three-MRF" class="headerlink" title="Three. MRF"></a>Three. MRF</h5><ul>
<li>无向图</li>
<li>条件独立性，发生x_B时，x_A与x_C无关 （<strong>global markov</strong> 全局马尔可夫）<ul>
<li>存在x_A, x_C被x_B分割（对应 bayes D-separation）， 那么发生x_B时，x_A与x_C无关 </li>
</ul>
</li>
<li><p><strong>local markov</strong> 局部马尔可夫</p>
<ul>
<li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p>
</li>
</ul>
</li>
<li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p>
<ul>
<li>对于集合x_-i,-j(集合没有i，j结点), 任意两个点x_i, x_j相互独立, (i != j)</li>
</ul>
</li>
<li>相互等价，可以互推 global markov <-> local markov <-> pair markov <->  基于最大团的因子分解</-></-></-></li>
<li>clique团，最大团<ul>
<li>集合间的结点相互联通</li>
<li>在一个团无法添加结点，则是最大团</li>
<li>(c1, c2,…表示团)</li>
</ul>
</li>
<li><p>因子分解 <strong>P(x) = 1/Z Π Φ(x_ci)</strong> ， Z = Σ_x1…Σ_xp Π Φ(x_ci) </p>
<ul>
<li>Φ 势函数， 必须为正（大于0）<ul>
<li><strong>Φ (x_ci) = exp {-E(x_ci)}</strong>   (E为能量函数，也叫势函数)</li>
<li>Φ (x_ci)  <-> P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</-></li>
<li>P(x) = 1/Z Π Φ(x_ci) = 1/Z Π exp{-E(x_ci)} = 1/Z exp{-ΣE(x_ci)} </li>
<li>最大熵原理 =&gt; 指数族分布 (Gibbs Distribution &lt;=&gt; Markov Random Field)</li>
<li>统计物理</li>
</ul>
</li>
<li>ci最大团</li>
<li>Z 为联合概率分布的归一化因子</li>
<li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li>
<li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li>
<li>Pair-MRF 因子分解：<strong>P(x) = 1/Z Π Φ(x_i) Π Φ(x_i, x_j)</strong>   (考虑边)</li>
<li><p>最大后验概率推理（图像分割问题）：max_x P(x)</p>
<ul>
<li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"></p>
</li>
<li><p>假设θ(x_i) = -logΦ(x_i); θ(x_i, x_j) = -logΦ(x_i, x_j)</p>
</li>
<li><p>max_x P(x) -&gt; 能量最小化 -&gt; min_x E(x) = Σθ(x_i)  + Σθ(x_i, x_j)</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"></p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p>
</li>
<li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"></p>
</li>
<li><p>结点的设置势函数，一个前景一个背景</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/12/Bayes-MRF/" data-id="ckfs9msyf0000g6egzvhzn82i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-HMM-CRF" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/09/HMM-CRF/" class="article-date">
  <time datetime="2020-08-09T12:29:28.000Z" itemprop="datePublished">2020-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/09/HMM-CRF/">HMM_CRF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场"><a href="#Hidden-Markov-Model-隐马尔可夫-amp-Conditional-Random-Field-条件随机场" class="headerlink" title="Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场"></a>Hidden Markov Model 隐马尔可夫 &amp; Conditional Random Field 条件随机场</h4><h5 id="One-Time-Sequence-Series"><a href="#One-Time-Sequence-Series" class="headerlink" title="One. Time Sequence / Series"></a>One. Time Sequence / Series</h5><ul>
<li>在时间序列中，起伏状态可以被观测到（股票走势就是一种时间序列，股票中的涨跌）</li>
<li>无法观测到的是时间序列的<strong>隐状态</strong>（股票中是否处于牛市状态，这类的就是隐状态）<ul>
<li>隐状态会有很多</li>
<li>当知道隐状态存在时，每个隐状态被观测时，都是相互独立（互不干扰）</li>
<li>隐状态之间是离散的</li>
</ul>
</li>
</ul>
<h5 id="Two-HMM"><a href="#Two-HMM" class="headerlink" title="Two. HMM"></a>Two. HMM</h5><ul>
<li>在概率中，隐马尔可夫模型对应所有状态是相互独立的</li>
<li><p><strong>P(q_t|q_t-1)</strong>  Transition Probability 转移概率</p>
<ul>
<li>P(q_t|q_t-1, q_t-2,…, q_1) = P(q_t|q_t-1)<ul>
<li>每个状态只取决于前面一个状态，而非前面所有状态</li>
<li>即当前隐状态到下一个隐状态的概率</li>
</ul>
</li>
<li>Discrete 离散的（用矩阵表示 A，维度(k, k)，k是开个隐状态）</li>
</ul>
</li>
<li><p><strong>P(y_t|q_t)</strong>  Emmission/Measurement Probability 发射概率 / 观测概率</p>
<ul>
<li>P(y_t|q_1,…, q_t-1, q_t, y_1,…, y_t-1) = P(y_t|q_t)<ul>
<li>已知当前状态 q_t，得到事实变化 y_t 的概率</li>
</ul>
</li>
<li>Discrete or Continuous</li>
<li>若是离散时，可以用矩阵表示 B，维度(k, L)，L是y的取值范围（也是离散的）</li>
<li>若是连续的，无法使用矩阵表示，y可能是连续的一个分布</li>
</ul>
</li>
<li><p>两个概率决定了整个隐马尔可夫模型 </p>
</li>
<li>在信号中用HMM也很多，时间轴每段范围的信号就相当与一个y_t<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm.png" alt="hmm" style="zoom:67%;"></li>
<li>每个隐状态的概率和为1，则矩阵行和为1</li>
<li>P(X) = ∫_y P(X, Y) dy </li>
<li><p><strong>P(y_1, y_2, y_3)</strong> = Σq_1 Σq_2 Σq_3 P(y_1, y_2, y_3, q_1, q_2, q_3) = Σq_1 Σq_2 Σq_3 <strong>P(y_3|q_3)P(q_3|q_2)*P(y_2|q_2)P(q_2|q_1)*P(y_1|q_1)P(q_1)</strong> </p>
<ul>
<li>P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|y_1, y_2, q_1, q_2, q_3)*P(y_1, y_2, q_1, q_2, q_3)</li>
<li>利用马尔可夫性质：P(y_1, y_2, y_3, q_1, q_2, q_3) = P(y_3|q_3)*P(q_3|y_1, y_2, q_1, q_2)*P(y_1, y_2, q_1, q_2) = P(y_3|q_3)P(q_3|q_2)*P(y_1, y_2, q_1, q_2)</li>
<li>P(y_3|q_3), P(q_3|q_2), P(y_2|q_2), P(q_2|q_1), P(y_1|q_1) 在马尔可夫参数A, B矩阵中可得</li>
<li><strong>P(q_1) 是初始状态</strong>，这需要问题给出</li>
<li>则马尔可夫模型需要三个参数，λ={A, B, P(q_1)} (假设y是离散的)</li>
</ul>
</li>
<li><p>马尔可夫模型有什么用？</p>
<ul>
<li>找50人讲10个单词（动物名字）</li>
<li><strong>λ_cat = argmax_λ log P(y_1, …, y_50|λ)</strong> <ul>
<li>当初始条件为λ，最大为说cat的情况</li>
<li>记录所有单词的λ</li>
<li>用高斯或者高斯混合模型计算λ</li>
</ul>
</li>
<li>当来了个新人，说其中一个单词，那么他说哪个单词概率最大？<ul>
<li><strong>P(y_new|λ_cat)</strong>, … 概率最大为结果</li>
<li>若所有该概率计算结果很小时，说明新人说的不是原10个单词</li>
</ul>
</li>
</ul>
</li>
<li><p>公式范化（计算量较大）<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/hmm_function.png" alt="hmm_function" style="zoom: 67%;"></p>
<ul>
<li><p>一个定义<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB.png" alt="FB" style="zoom:67%;"></p>
</li>
<li><p>优化计算量</p>
<ul>
<li>alpha_i(t) = P(y_1, …, y_t, q_t=i)</li>
<li>alpha_i(1) = P(y_1, q_1=i) = P(y_i|q_1=i)P(q1) = b_i(y_1) P(q_1) </li>
<li>alpha_j(2) = P(y_1, y_2, q_2=j) = Σq_1P(y_1, y_2, q_1=i, q_2=j) = Σq_1 P(y_2|q_2=j)P(q_2=j|q_1=i)*P(y_1, q_1=i) = P(y_2|q_2=j) Σq_1 P(q_2=j|q_1=i)*alpha_i(1) = b_j(y_1) Σq_1 a_ij*alpha_i(1)</li>
<li>开始递归 alpha_j(t) = b_j(y_t) Σq_1 a_ij*alpha_i(t-1) = P(y_1, …, y_t, q_t=j) <ul>
<li><strong>P(y_1, …, y_t) = Σj P(y_1, …, y_t, q_t=j) = Σj alpha_j(t)</strong></li>
</ul>
</li>
<li>通过贝叶斯公式<br><img src="https://github.com/soloistben/images/raw/master/hmm_crf/FB_2.png" alt="FB_2" style="zoom:67%;"><ul>
<li>P(Y, q_t=i|λ) = P(Y, q_t=i)P(q_t=i) = P(y_1, …, y_t|q_t=i)P(y_t+1, …, y_T|q_t=i)P(q_t=i) =  P(y_1, …, y_t, q_t=i)P(y_t+1, …, y_T|q_t=i) = alpha_i(t) beta_i(t)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如何学习HMM的参数</p>
<ul>
<li>λ = argmax_λ log P(Y|λ) （极大似然估计）</li>
<li>用EM学习<ul>
<li>E-step: Σq_1… Σq_t log(P(Y, Q))*P(Q,Y|θ^g) = Σq_1… Σq_t log(P(q_1)*ΠP(q_t|q_t-1)*ΠP(y_t|q_t)) <em> P(Q,Y|θ^g) = Σq_1… Σq_t [log(P(q_1)+Σlog(a_q_t-1,q_t)+Σlog(b_q_t(y_t))] </em> P(Q,Y|θ^g)</li>
<li>part 5,6还没看完(需要EM基础)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLFze15KrfxbGPEHyjxddbbxVvLa5kilFf" target="_blank" rel="noopener">source video</a> <a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/dynamic_model.pdf" target="_blank" rel="noopener">pdf</a></p>
<h5 id="Three-CRF"><a href="#Three-CRF" class="headerlink" title="Three. CRF"></a>Three. CRF</h5><p>+ </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/09/HMM-CRF/" data-id="ckfs9msyx0008g6egrn8ovzkl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Spectral-Cluster" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/25/Spectral-Cluster/" class="article-date">
  <time datetime="2020-07-25T05:19:54.000Z" itemprop="datePublished">2020-07-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/25/Spectral-Cluster/">Spectral_Cluster</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Spectral-Cluster-谱聚类"><a href="#Spectral-Cluster-谱聚类" class="headerlink" title="Spectral Cluster 谱聚类"></a>Spectral Cluster 谱聚类</h4><h5 id="一、分类与聚类"><a href="#一、分类与聚类" class="headerlink" title="一、分类与聚类"></a>一、分类与聚类</h5><p>1、分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类别标号y中。</p>
<p>2、聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。目的是个簇内的元素之间越相似，簇间的相似度越小。</p>
<h5 id="二、k-means-与-spectral-cluster"><a href="#二、k-means-与-spectral-cluster" class="headerlink" title="二、k-means 与 spectral cluster"></a>二、k-means 与 spectral cluster</h5><p>k-means 每次选择k个中心点，将每个数据点归类到离它最近的那个中心点所代表的簇中，迭代多次，一直到迭代了最大的步数或者前后 <strong>J</strong> 的值相差小于一个阈值为止。(u_k为中心点，r_nk 为数据点 x_n 被归类到 cluster k 的时候为 1 ，否则为 0 )<br>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/kmean.png" alt="kmean"></p>
<ul>
<li>结点与附近的结点相似度会更高，距离远的结点相似度更低；因此，对角线上颜色更亮，右上角左下角的区域更暗 </li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/affinity_matrix.png" alt="Similarity Matrix" style="zoom: 25%;"></p>
<p>然而，k-means初始的中心点是随机选的，每次选择结果不同，大部分情况结果还是令人满意，偶尔也会陷入局部最优。传统 k-means的x_n输入是每个结点的所有信息（即完整的N维度信息）。</p>
<p>spectral cluster 谱聚类只需要结点之间的相似度矩阵即可，并不需要结点的完整信息，因此不必像k-means那样要求N维的欧氏空间的向量。即抓住了结点之间的主要信息，排除了冗余信息，计算复杂度也更小，所以比传统聚类方法会更加健壮一些。</p>
<h5 id="三、谱的涵义"><a href="#三、谱的涵义" class="headerlink" title="三、谱的涵义"></a>三、谱的涵义</h5><p>对于谱的概念，简而言之，可以把谱认定为对一个信号（视频，音频，图像，图）分解成为一些简单的元素线性组合（小波基，图基）。为了使得这种分解更加有意义，可以使得这些分解的元素之间是线性无关的（正交的），也就是说这些分解的简单元素可以看作是信号的基。面对研究的东西，往往都会细分的更细小层面才能挖掘更主要的信息，犹如研究生物，则要细分到细胞、基因层面；研究物理，则要细分到质子、夸克层面；目前深度学习研究图片也是细分到像素层级，音频则细分到音频、音位层级。因此针对图也是如此。</p>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/spectrum.png" alt="spectrum" style="zoom: 50%;"></p>
<p>在信号处理中，谱就是傅立叶变换，它提供了不同频率下的正弦和余弦波作为基，将信号在这些基进行分解。</p>
<p>​        <img src="https://github.com/soloistben/images/raw/master/spectral_cluster/fourier.png" alt="fourier" style="zoom: 67%;"></p>
<p>在图中，“谱”则是指对图的拉普拉斯矩阵的特征分解，特征分解后正交化的特征向量就是对应的正交基，所有的特征值的全体统称为拉普拉斯矩阵的谱，最后则可以用特征向量和特征值来表示图的信息。</p>
<h5 id="四、Graph-与-Laplacian"><a href="#四、Graph-与-Laplacian" class="headerlink" title="四、Graph 与 Laplacian"></a>四、Graph 与 Laplacian</h5><ul>
<li><p>Graph G = (V, E, X)</p>
<p>结构信息：V 表示结点集，E 表示结点之间的边集，A 表示N维的邻接矩阵（若是无向图，则邻接矩阵内元素是0/1; 若是有向图，则邻接矩阵内元素的值是具体权重值w），D表示N维的度矩阵（对应结点的拥有邻居结点数，放置在矩阵对角线上）。</p>
</li>
</ul>
<p>​        特征信息：X表示结点的N维度信息。</p>
<ul>
<li><p><strong>为什么特征分解最终选择拉普拉斯矩阵而不是相似矩阵？</strong></p>
<p>因为拉普拉斯矩阵的独有属性，它是半正定矩阵，则特征值都是大于或等于0, 可以有多个0特征值，每个0特征值对应特征向量上的值大于0对应的节点之间具有连通性，对应一个子图。拉普拉斯矩阵拥有相似矩阵（邻接矩阵）的特性，又拥有独有特性，则更好表达图的信息。</p>
</li>
<li><p><strong>为什么需要使用归一化后的拉普拉斯矩阵？</strong></p>
<p>聚类的目的为两点：第一点是最小化簇间的相似度，第二点是最大化簇内相似度。未归一化的拉普拉斯矩阵仅能达到第一点，归一化的拉普拉斯矩阵    能达到两点要求。(具体如何达到两点的推导，见文章<strong><code>A Tutorial on Spectral Clustering</code></strong>)</p>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/laplacian.png" alt="Laplacain Matrix" style="zoom: 25%;"></p>
</li>
<li><p>特征分解</p>
<p>求解得到特征值、特征向量，小到大排序特征值，对应特征向量也排序，则最终得到选择前k个最小特征值对应的特征向量。</p>
<ul>
<li>在Eigenvector Matrix，第0列是特征值为0对应的特征向量，是个全1列向量（颜色相同）（因为第二步构造的是全连接矩阵，则仅有一个特征值为0）</li>
<li>第1列对应较亮色块是值大于0的情况，暗色块值小于0，因此较亮色块中对应结点属于一个簇；第2列则是黄色块部分属于一个簇，等。依次类推即可。</li>
<li>中间图为Eigenvector Matrix 通过t-SNE降到2维的结果，能将不同簇的结点都明显区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接。</li>
<li>右间图为Eigenvector Matrix 通过t-SNE降到3维的结果，在3D空间领域，也能将不同簇的结点区分，同簇的结点分布呈线状，荧光绿色与紫色部分仍有少许连接，黄色与棕色有一两个点连接。</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector.png" alt="Eigenvector Matrix" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_2d.png" alt="Eigenvector Matrix 2D" style="zoom: 25%;"><br><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/eigenvector_3d.png" alt="Eigenvector Matrix 3D" style="zoom: 50%;"></p>
<p>  即spectral cluster可以看作为node feature高维matrix data (n,d) 经过一个<a href="http://blog.pluskid.org/?p=290" target="_blank" rel="noopener">laplace mapping</a>降维得到一个低维embedding  (n, k)，embedding中融入更主要的信息，舍弃冗余信息。        </p>
<h5 id="五、Spectral-Clustering"><a href="#五、Spectral-Clustering" class="headerlink" title="五、Spectral Clustering"></a>五、Spectral Clustering</h5><p>​    获得embedding即执行k-means</p>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/orgin.png" alt="原数据图" style="zoom: 25%;"><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/k_means.png" alt="直接 k-means 的结果" style="zoom:25%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/spectral_cluster/SpectralClustering.png" alt="谱聚类结果" style="zoom:25%;"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/25/Spectral-Cluster/" data-id="ckfs9mszb000hg6egs9dv460c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-machine-learning" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/06/machine-learning/" class="article-date">
  <time datetime="2020-05-06T12:20:41.000Z" itemprop="datePublished">2020-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/06/machine-learning/">machine_learning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="机器学习-machine-learning-from-TJU"><a href="#机器学习-machine-learning-from-TJU" class="headerlink" title="机器学习 machine learning from TJU"></a>机器学习 machine learning from TJU</h3><h4 id="One-绪论"><a href="#One-绪论" class="headerlink" title="One. 绪论"></a>One. 绪论</h4><ol>
<li><p>什么是智能？</p>
<ul>
<li><strong>Self-adaption 自适应</strong> （迁移学习，通用AI模型 ( Artificial General Intelligence, 即strong AI)）</li>
<li><strong>Self-consciousness 自我意识</strong> （模糊决策）</li>
<li>运算智能：快速计算，存储</li>
<li>感知智能：人类五官的能力（视觉、听觉、触觉等）【已解决】</li>
<li>认知智能：大脑的能力（逻辑推理、知识理解、决策思考）<strong>（语言处理）</strong>（概念、意识、观念）（理解、思考、决策）【正在解决】</li>
</ul>
</li>
<li><p><strong>Turning Test</strong> 图灵测试：正常人分别和正常人、AI聊天，是否能分清人与AI（是否有用是哲学问题，AI是否伪装，从而不通过Turning Test）</p>
<p><strong>Behaviorism 行为主义</strong>，仅看行为是否符合智能，不管内部部分（有漏洞）</p>
<p><strong>Connectionism联结主义</strong>，只看内部构造（用神经网络模拟），符合大脑构造，则认为有智能（婴儿无法通过Turning test，但他结构是符合的）（但无法知道大脑构造，如何产生意识？）</p>
<p>模拟鸟的飞行，制造飞机（虽然达不到鸟内部的全部飞行系统，但能模拟飞行）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/turning test.png" alt="turning test" style="zoom: 33%;"></p>
</li>
<li><p>如何制造 AI？</p>
<ul>
<li><strong>“Thinking” by “Searching”</strong>：“思考”即“搜索” (Behaviorism)，类似搜索引擎，信息检索，在已有知识寻找最佳答案（大脑积累知识，面对问题，就是搜索大脑已有知识（但无法确定大脑是如何搜索的））<ul>
<li>Knowledge Graph 知识图谱</li>
</ul>
</li>
<li><strong>“Learning”</strong>：学习知识，发现新的知识<ul>
<li>什么是新的知识?（知识-&gt;模式-&gt;稳定的关联关系）</li>
<li><strong>pattern</strong> 模式识别（机器学习的前身）（语言的语法是一种模式，物理规律也是模式）</li>
<li>模式识别和机器学习的区别在于：前者喂给机器的是各种特征描述，从而让机器对未知的事物进行判断；后者喂给机器的是某一事物的海量样本，让机器通过样本来自己发现特征，最后去判断某些未知的事物。（机器学习在挖掘数据最终找到模式） （模式识别=数据挖掘）</li>
</ul>
</li>
<li><strong>“Thinking” by “Learning”</strong>：“思考”即“学习”，Known Data-&gt;Model-&gt;Unknown Data<ul>
<li>Model 模型就是对模式的大概猜测</li>
<li>y=f(x), f()就是模式</li>
<li>由于模式很多种，模型则用 y = ax+b 去猜测，算法则调整a和b参数</li>
<li>机器学习就是科学研究的自动化（确定变量-&gt;做实验-&gt;找到变量之间规律）</li>
</ul>
</li>
</ul>
</li>
<li><p>机器学习的基本框架</p>
<ul>
<li>一系列可能函数 &amp; 训练数据 -&gt; 通过算法 -&gt; 选出最好的函数</li>
<li><strong>Supervise Learning</strong> 监督学习，模型只需要找到输入和标记之间的关联关系（标记是人工的，不是自己手标的，就是已标好数据（这种要成本））</li>
<li><strong>Semi-Supervised Learning</strong> 半监督学习，少量部分样本标记的监督学习</li>
<li><strong>UnSupervise Learning</strong> 无监督学习，无标记，模型自己总结出类别（聚类）</li>
<li><strong>Reinforcement Learning</strong> 强化学习，利用间接”标记“来学习<ul>
<li>围棋的”输赢“，样本是棋局，直接标记是棋子在哪个位置是最好的（但没有这种标记，没有人知道哪里是最好的），间接标记是这个棋局是黑白输赢结果</li>
<li>online learning or 反复学习</li>
<li>输出是有反馈，对模型进行奖励机制</li>
</ul>
</li>
<li>基于规则的模型：人定义”特征“，人定义特征和输出之间的关系</li>
<li>基于统计的模型：人定义”特征“，模型确定特征和输出之间的关系（特征工程）<ul>
<li>cat？= 0.1*毛色+0.2*耳朵形状+0.3*眼睛形状 …</li>
</ul>
</li>
<li>深度学习模型：人不定义”特征“，模型确定原始信息和输出之间的关系（可以达到 end-to-end model）（人类选择的特征未必是最好的）<ul>
<li>深度学习可以发现特征（通过是神经网络学习原始信息获得高阶特征，一些人类未必发现的特征）</li>
<li>越深越能发现复杂特征</li>
</ul>
</li>
</ul>
</li>
<li><p>AI的一些重要问题</p>
<ul>
<li><p>标记、model、feature</p>
</li>
<li><p>什么是好模型？</p>
<ul>
<li><p>（泛化能力）描述性，但难以具体化，不可计算（欠拟合Underfitting）</p>
</li>
<li><p>（性能）具体可计算，适合范围小，描述性差（过拟合<strong>Overfitting</strong>）（模型复杂性越高容易过拟合）（难以避免）</p>
</li>
<li><p>两者折中比较难</p>
</li>
<li><p>机器学习的最终目标是在未知数据上效果最好</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/overfitting.png" alt="overfitting" style="zoom: 50%;"></p>
</li>
<li><p>严格上，数据需要分 训练集，开发集，测试集（避免虚假结果）</p>
</li>
<li><p>”成功就是最大的失败“（越成功会越保守，而事物是发展的，会在新事物上会越失败）</p>
</li>
<li><p>面对企业：</p>
<ul>
<li>基于规则的模型：问题简单，大量已知知识（可解释性好；基于人归纳，会比较抽象，越抽象越鲁棒性）</li>
<li>基于统计的模型：数据量不大，有一些明确的特征（可解释性一般，但都能猜到；基于数据归纳，不够抽象）</li>
<li>深度学习模型：数据量<strong>大</strong>，算力高，没有明确特征，先验知识缺乏（黑箱子，可解释性<strong>差</strong>）</li>
</ul>
</li>
<li><p>面对科研：越复杂越好</p>
</li>
</ul>
</li>
<li><p><strong>Global Knownledge 世界知识</strong>（人工智能选特征选模型，仍需要辅助的知识（经验知识，常识））</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/global_knownledge.png" alt="global knownledge" style="zoom: 50%;"></p>
<ul>
<li>人的学习不需要太多数据样本（小样本学习）</li>
<li>人可以小样本学习，也是有经验知识（先验知识），但儿童学习语言无法解释（儿童无先验知识，”大脑有普遍语法存在“）</li>
<li>有了先验知识，就可以对训练样本要求少一些（小样本学习），只学习特殊的知识即可。</li>
</ul>
</li>
<li><p><strong>Explainable</strong> 可解释性</p>
<ul>
<li><p>为什么模型会这么决策？</p>
</li>
<li><p>有了可解释性，可追溯源头，从根本上改进它。</p>
</li>
<li><p>自动驾驶事故率低于人类司机，为什么我们却不信任它？</p>
<p>（因为自动驾驶出事故的原因不可解释，出事故是概率性的）</p>
</li>
<li><p>刷脸支付，出错也是不可解释的</p>
</li>
</ul>
</li>
<li><p><strong>Ethics</strong> 伦理问题</p>
<ul>
<li><p>若有意识的机器人是否拥有人权？</p>
</li>
<li><p>AI通过用户的非隐私数据获得隐私数据</p>
<p>（识别用户的性格或者需求，左右用户做出选择（尤其在选举或者个性化推荐））</p>
<p>（搜索引擎（<strong>主动获取信息</strong>）是用户信息入口，会影响国家整体发展）</p>
<p>（现在约50%信息是依照个性推荐（<strong>被动获取信息</strong>），会导致个性分化、信息茧房，会加大偏见和隔阂，局限在自己圈子，最终导致社会撕裂，不接受他人，无法全面认识世界）</p>
</li>
<li><p>若AI能预测一个人的犯罪概率，是否在犯罪前先控制他？</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="Two-Machine-Learning-Foundations-from-台大林軒田"><a href="#Two-Machine-Learning-Foundations-from-台大林軒田" class="headerlink" title="Two. Machine Learning Foundations from 台大林軒田"></a>Two. Machine Learning Foundations from 台大林軒田</h4><ol>
<li><p><strong>machine learning = sample data + blurry pattern + not easily programmable definition</strong></p>
</li>
<li><p>Data Mining -&gt; feature -&gt; Machine Learning (在机器学习选择特征时，尽量选取特征之间相关性小的特征，相关性越大，则越冗余，就没意义了)</p>
</li>
<li><p>Machine Learning use data to compute hypothesis g that approximates target f. (Machine Learning ∈ Statitics)</p>
</li>
<li><p><strong>Perceptron 感知器</strong>，h(x) = sign(Σwi xi - threshold) (i =1,2,…)</p>
<ul>
<li><p>模拟神经细胞，接收信号，整合起来（<strong>加权求和</strong>），接收整体的信号超过某个阈值，则激活神经细胞</p>
</li>
<li><p>将threshold融入权重w，作为w0，h(x) = sign(Σwi xi)  (i =0,1,2,…) = sign(w^T x)，大于0为正例，小于0为负例。（线性感知器）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Perceptron.png" alt="Perceptron" style="zoom:33%;"></p>
<ul>
<li><p>令 h(x)=0， x2为纵轴，x1为横轴，x2 = -w1/w2 * x1 - w0/w2，右图分类效果较好（w1比w2大，即x1特征比x2特征更重要）</p>
</li>
<li><p>PLA  Perceptron Learning Algorithm 寻找最优划分的线性函数</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_1.png" alt="update weight" style="zoom:43%;"></p>
</li>
<li><p>前提是线性可分的，则可以在有限步内停止，每次调整都会更接近完美分类面；若非线性，则无法停止。(大多数情况是非线性的，有noise；可用pocket算法，在非线性情况下，在一定调整步数下，选择错误率最低的结果)</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_3.png" alt="weight update" style="zoom: 50%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/preceptron_2.png" alt="PLA" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>perceptrons <-> linear (binary) classifiers</-></p>
</li>
</ul>
</li>
<li><p><strong>Types of Learning</strong> 机器学习的类型</p>
<ul>
<li>output space<ul>
<li>binary or more binary <strong>Classification</strong> （预测类别，划分样本）</li>
<li><strong>Regression</strong> （预测一个实数，样本的拟合问题，连接样本）<ul>
<li><strong>可以用回归任务做分类</strong></li>
<li>先算出一个数，设定阈值，然后可以分类</li>
</ul>
</li>
<li><strong>Structured</strong> learning (一维（序列结构，语法结构学习），二维（图结构），三维（蛋白质结构，分子结构)）</li>
</ul>
</li>
<li>data label<ul>
<li>supervised, semi-superivised, unsupervised, reinforcement</li>
</ul>
</li>
<li>protocol f =&gt; (x,y)<ul>
<li><strong>Batch</strong> Learning 批量学习（大量样本）</li>
<li><strong>Online</strong> Learning 在线学习（在线 =&gt; 持续学习，不断接收数据（少量），更新模型）<ul>
<li>Online + Batch，先大批量数据学习一个模型，再持续接收少量数据，更新模型（更新模型部分，并不是完全重新学习，否则就是多次批量学习了）</li>
<li>垃圾邮件分类，先训练通用的模型，根据用户个性再调整，形成个性化（每次再垃圾箱找到需要的邮件，即分错样本，就会为模型产生少量数据）（是否垃圾邮件，对每个人的意义不一样）</li>
<li>PLA，Reinforcement Learning</li>
</ul>
</li>
<li><strong>Active</strong> Learing 主动学习<ul>
<li>属于一种 Online Learning</li>
<li>同样基于少量样本调整模型，但Active Learning 模型主动向用户获取数据，Online Learning 是被动获得数据</li>
<li>垃圾邮件分类，若删除多个同用户的邮件，Active Learning会提问是否标记其邮件为垃圾邮件，若是，立即标记该用户为重要特征；而Online Learning则是等待用户标记垃圾邮件，需要多次标记才可以。</li>
</ul>
</li>
</ul>
</li>
<li>input space<ul>
<li><strong>concrete</strong> features 具体特征（物理意义明确）</li>
<li><strong>Raw</strong> features 原始特征（图片的像素，亮度，黑白）</li>
<li><strong>Abstract</strong> feature 抽象特征（没用任何物理意义） </li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Feasibilityof Learning</strong> 学习的可行性</p>
<ul>
<li><p>是否可以学习知识？</p>
</li>
<li><p>训练样本是有限的，无法保证能学习到最好的 f()，只能逼近</p>
</li>
<li><p>Hoeffding’s Inequality，模型在训练样本的错误率v，模型在整体样本错误率u</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/hoeffding.png" alt="hoeffding" style="zoom:43%;"></p>
<ul>
<li>“v = u” is probably approximately correct (PAC)</li>
</ul>
</li>
<li><p>在训练集效果好，在测试集的效果也会好的概率？</p>
<ul>
<li><p>有M个候选函数，则错误率就很大，则过拟合。（前提是数据在候选函数之间相互独立（线性无关））</p>
</li>
<li><p>若M个候选函数中存在线性相关的候选函数，即不是<strong>相互独立</strong>，则M不是无穷大，则有希望减少过拟合</p>
</li>
<li><p>Ein 测试集错误率，Eout未知数据错误率（机器学习做两件事，模型在训练集使Ein变小，再使Ein和Eout尽可能相等）</p>
</li>
<li><p>样本N越大，则结果越可靠</p>
</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pac.png" alt="PAC" style="zoom:43%;"></p>
<ul>
<li><p>相同数据（x1 x2）喂入两个候选函数（两条红线），得到结果一样，则两个候选函数<strong>相关</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/error.png" alt="error" style="zoom:43%;"></p>
</li>
<li><p>对PLA而言，分类是候选函数将样本一分为2（则分类相同的候选函数相关，则两个函数视为等价），n个样本，最多也是2^n个<strong>类别</strong>，即<strong>M=2^n</strong>，则<strong>M不是无穷大</strong>的（对所有问题，都不是无穷大的）</p>
</li>
<li><p>实际情况是 <strong>M&lt;&lt;2^n</strong></p>
</li>
<li><p>growth function 成长函数，给定n个样本，返回实际可分类别数</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/growth_funtion.png" alt="growth_funtion" style="zoom:43%;"></p>
</li>
<li><p>问题不同，成长函数不一样（成长函数上限则为break point突破点）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different.png" alt="different" style="zoom:43%;"></p>
</li>
<li><p>机器学习要达到的目标</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Ein_Eout.png" alt="Ein_Eout" style="zoom:43%;"></p>
</li>
<li><p>参数的个数是决定模型复杂度的核心指标</p>
</li>
<li><p>自由度=这个模型的有多少参数，一个参数是一个维度，参数越多，自由度越高</p>
<ul>
<li><p>VC维=参数个数</p>
</li>
<li><p>VC维 the formal name of maximum non-break point</p>
</li>
<li><p>break point是成长函数的上限，k决定了成长函数的最多参数数量，从而决定了vc维</p>
</li>
<li><p>dvc = min_k-1</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/VC_dimension.png" alt="VC_dimension" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<ol start="7">
<li><p><strong>Regression</strong> 回归</p>
<ul>
<li><p>Noise: 样本标记错误（正例标记成反例）</p>
</li>
<li><p>Probabilistic 概率函数：对输出不是确定性的，都是概率性的（容忍存在Noise）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different_error.png" alt="different_error" style="zoom:43%;"></p>
</li>
<li><p>分类：判断sample是否符合目标f()</p>
</li>
<li><p>回归：让sample离目标f()越近（error用平方，是在最低点是可微的，用绝对值是不可微的）</p>
<ul>
<li>用回归无法直接做分类，但可以缩小分类的范围，err_0/1 &lt;= err_sqr</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/logistics_regression.png" alt="logistics_regression" style="zoom:43%;"></p>
</li>
<li><p><strong>logistic regression</strong> （非线性回归）</p>
<ul>
<li>err 需要计算两个概率分布的差值（KL散度）</li>
<li>当数据为病人的特征数据，但患病只有0/1（患病与不患病），则需要求出整体患病的概率分布。</li>
<li>极大似然估计：给定输入输出，确定一个分布。</li>
<li>用logistics regression 训练一个分布接近极大似然估计的分布</li>
</ul>
</li>
<li><p><strong>Gradient Descent</strong> 梯度下降，用于update weight（随机梯度下降，是随机采样点，大方向和直接梯度下降是一致的，但复杂度翻倍）（步长=学习率）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Gradient Descent.png" alt="Gradient Descent" style="zoom: 50%;"></p>
</li>
</ul>
</li>
<li><p><strong>Multiclass Classification </strong>多分类问题</p>
<ul>
<li><p>四分类拆成多个二分类问题</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Multiclass Classification.png" alt="Multiclass Classification" style="zoom:50%;"></p>
<ul>
<li><p>正例大大少于负例，样本不平衡</p>
</li>
<li><p>四分类分成两类，形成一对一对的分类</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pairwise classifier.png" alt="pairwise classifier" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>Nonlinear Transform 训练分类</p>
</li>
<li><p>将非线性转成线性</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/nonlinear.png" alt="nonlinear" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>Regularization</strong> 正则化</p>
<ul>
<li><p>缩小高次空间，控制在一定区间内，防止过拟合同时仍具有高次空间的能力</p>
</li>
<li><p>降低复杂度，减轻过拟合（缩减候选函数的个数M）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/regularization coefficient.png" alt="regularization coefficient" style="zoom:43%;"></p>
</li>
<li><p>可加入loss function一起训练正则化</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Regression.png" alt="Regression" style="zoom:43%;"></p>
</li>
<li><p>L1（有一堆特征，但有些是无用的，用L1可去除一些无用特征），L2（常用，比较柔和）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/l1_l2.png" alt="l1_l2" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ol>
<h4 id="Three-NLP"><a href="#Three-NLP" class="headerlink" title="Three. NLP"></a>Three. NLP</h4><ol>
<li><p>what is NLP?</p>
<ul>
<li><p>Turning Test 基于 NLP</p>
</li>
<li><p>感知智能 -&gt; CV</p>
</li>
<li><p>认知智能 -&gt; NLP</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_nlp.png" alt="text_nlp" style="zoom:43%;"></p>
</li>
<li><p>语义角色标注（施动者，受动者，描述）</p>
</li>
<li><p>理解 <strong>NLU: L -&gt; R</strong>;  生成 <strong>NLG: R -&gt; L</strong></p>
</li>
<li><p>让机器get到语言中的meaning</p>
</li>
<li><p>NLP 中不允许存在歧义（程序语言没有歧义，自然语言是存在歧义的）</p>
<ul>
<li>NLP 需要解决语义之间歧义</li>
</ul>
</li>
<li><p>创造一个 <strong>interlingua 中间语</strong>，允许所有自然语言均可以翻译成 interlingua，自然语言是动态的，则 interlingua 几乎不可创造<em>（没有 interlingua，就很难表示 meaning）</em></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_translate.png" alt="ml_translate" style="zoom:50%;"></p>
</li>
<li><p>对自然语言做语法分析，只能越来越逼近 interlingua</p>
</li>
<li><p>在深度学习，则用 respentation / embedding 向量来表示语义的 meaning</p>
</li>
<li><p>知识图谱 -&gt; 让机器获取先验知识（前期需要NLP处理数据挖掘实体之间的关系）</p>
</li>
</ul>
</li>
<li><p>NLP’s hard point</p>
<ul>
<li><p>“哈士奇”不管在哪都是“哈士奇”；“同志”在不同语境表示不一样</p>
</li>
<li><p>歧义 &amp; 动态</p>
</li>
<li><p><strong>语言的本质是谎言</strong>。真话：能正确反映真正事实的话，谎言：不能正确表达真正事实的话（同一句话，不同人理解不一样，都带有各自的偏见，所有不能正确表达真正的事实）</p>
</li>
<li><p>符号系统：人类创造符号来表达信息，语言是其中一种。</p>
</li>
<li><p><strong>所指：meaning；能指：表达meaning的工具</strong></p>
<ul>
<li>所指，能指之间规律不可寻，具有任意性，但在特定的时间，地点可以有局部确定的规律（人类可根据古代壁画符号风格判断年份）</li>
</ul>
</li>
<li><p>语言的任意性所导致的歧义性、动态性，乃至非真实性是语言处理的根本性困难（非真实性：描述抽象概念，没有实体对应（白马非马））</p>
</li>
<li><p>基本歧义（语法结构、词义、词性…）</p>
</li>
<li><p>旧知识 -&gt; 先验知识 -&gt; 先验知识 + 小样本 -&gt; 新知识</p>
</li>
<li><p>乔姆斯基：存在一些普遍语法，并非局部的，是所有语言学的共性</p>
<ul>
<li>例如小孩子就可以小样本学习语言，但没有先验知识（并非多次听到语言，毕竟是教不会动物说话）</li>
<li>“递归”，语言存在递归结构</li>
<li>递归存在（语言/语义）自指结构（是产生悖论的主要原因之一）（“这句话是错的”）</li>
</ul>
</li>
<li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics.png" alt="rule_statistics" style="zoom: 49%;"><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/rule_statistics_2.png" alt="rule_statistics" style="zoom:32%;"></p>
<ul>
<li>只需要判断<strong>特定词汇</strong>，就可以使用CNN（只需要判断句子中有“高兴”词语，就可以判定情感），但判断整句话的所有词，则需要使用RNN</li>
</ul>
</li>
<li><p>语言理解的关键：<strong>背景知识</strong>（上下文）</p>
<ul>
<li>在NLP中，所有词都是有歧义，必须要有一个固定场景，才能确定一个词的意思</li>
<li>语用即语义：词是在场景怎么用的，就是语义</li>
<li>例如描述人，重点在人与其他事物的关系，并不是人体内在结构</li>
<li>graph 是表示事物之间的关系</li>
<li>学习基于事物之间关联，相关性</li>
<li><strong>相关性</strong>恰恰是破解<strong>任意性</strong>（歧义&amp;动态）的钥匙！</li>
<li>NLP 用上下文约束自然语言的任意性</li>
</ul>
</li>
<li><p>表示学习（利用上下文表示语义）</p>
<ul>
<li>基于特征的可解释表示</li>
<li>基于深度学习编码的不可解释表示</li>
<li>与其他非语言对象相结合的表示：如网络表示学习</li>
<li>作为其他学习模型的输入：深度学习模型、线性学习模型</li>
</ul>
</li>
<li><p><strong>word embedding</strong> 词向量表示学习：<strong>基于上下文用向量表示这个词</strong>，向量则可以计算的</p>
<p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_embedding.png" alt="word_embedding" style="zoom:43%;"></p>
<ul>
<li>坑：不在一个语义空间的词不能像比较；即使向量数值一样，意义不一样，词就是不一样（人名和电影不在一个空间）</li>
<li>两个word embedding之间差值，可以表示两者的关系</li>
</ul>
</li>
<li><p>预训练模型（通用知识的自动获取）：表示学习、语言模型、针对特定任务的与训练</p>
<ul>
<li><p>基于很大数据学习最基本的（几何）元素，作为其他模型的输入</p>
</li>
<li><p>属于传统机器学习（已知最基本元素，用于训练提取）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pre_training.png" alt="pre_training" style="zoom:43%;"></p>
</li>
<li><p>深度学习（黑盒子，不知道特征是否重要，用深度学习抽取特征）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DL.png" alt="DL" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>填补先验知识和模型能力（模型搜索空间）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ml_Process.png" alt="ml_Process" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>NLP基本任务</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/NLP_task.png" alt="NLP_task" style="zoom:43%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/essential model.png" alt="essential model" style="zoom:43%;"></p>
</li>
<li><p>NLP’s essential models (<strong>Linear Models</strong>)</p>
<ul>
<li><p><strong>EM算法</strong>（<strong>Expectation-maximization algorithm</strong> 期望最大化算法）</p>
<ul>
<li><p>“猜测隐藏在文字背后的信息”</p>
</li>
<li><p>在概率模型中寻找参数<strong>最大似然估计</strong>或者<strong>最大后验估计</strong>的算法, 其中概率模型依赖于无法观测的<strong>隐性变量</strong>。</p>
<ul>
<li>观察结果依赖于隐藏状态。只能看到观察结果,看不到隐藏状态。如何知道隐藏状态生成观察结果的概率(模型参数)?</li>
</ul>
</li>
<li><p>知道其中一个，可以互相推导 （有输入输出（观察值是输入），做监督学习）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM.png" alt="EM" style="zoom:43%;"></p>
</li>
<li><p>当两个都不知道（即 只有输入，没有输出，则为无监督学习）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/EM algorithm.png" alt="EM algorithm" style="zoom:43%;"></p>
</li>
<li><p>EM算法一定能收敛，但只能局部最优，无法全局最优，可通过尝试多个初始值（瞎猜参数）来改进最优效果</p>
</li>
</ul>
</li>
<li><p><strong>ME (Maximum Entropy) 最大熵模型</strong></p>
<ul>
<li><p>“用特征去束缚语言的任意性”</p>
</li>
<li><p>信息熵：用来描述信息的不确定性</p>
<ul>
<li><p><strong>一个物理系统越无序（无能力流动，则无序），则能量越小，信息熵越大；越有序（能力按有序方向流动），能量越大，信息熵越小</strong></p>
</li>
<li><p>一个体系的能量达到完全均匀分布时，这个系统的熵就达到最大值 </p>
</li>
<li><p>封闭系统总熵时不断增大的（能量传递完成，达到均衡），局部会出现熵减小的情况</p>
</li>
<li><p>能力来自于差异（判读是否有动能/势能/热能，对比其周围是否存在差异，有差异才存在能力流动）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Entropy.png" alt="Entropy" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>分布均匀 &lt;=&gt; 熵最大 =&gt; 最合理结果</strong></p>
</li>
<li><p>信息熵越大信息量越大（信息的不确定性越强）（熵越大的系统承载信息的能力越大）</p>
</li>
<li><p>最大熵：保留全部的不确定性,把风险降到最小</p>
</li>
<li><p>最大熵原理指出,需要对一个随机事件的概率分布进行预测时,我们的预测应当<strong>满足全部已知的条件</strong>，而<strong>对未知的情况不要做任何主观假设</strong>。在这种情况下,概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。</p>
</li>
<li><p>条件熵</p>
<ul>
<li><p>在给定输入的情况下，计算输出概率，实际上在计算以输入为条件的条件熵</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Condition Entropy.png" alt="Condition Entropy" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>最大熵模型：求解带约束(特征函数)的最优化问题</p>
<ul>
<li>引入拉格朗日乘子,定义拉格朗日函数,转化为特征加权和</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>隐马尔可夫链 HMM</strong></p>
<ul>
<li><p>“语言是一个串”</p>
</li>
<li><p>一个隐状态序列产生一个观察值序列。每个隐状态依赖于前一个隐状态</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM.png" alt="HMM" style="zoom:43%;"></p>
<ul>
<li>转移概率：隐状态之间转移（转换）的概率</li>
<li>发射概率：隐状态产生观察值的概率</li>
</ul>
</li>
<li><p>HMM能解决的问题</p>
</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Model.png" alt="HMM_Model" style="zoom: 33%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_Trasfer.png" alt="HMM_Trasfer" style="zoom:43%;"></p>
<ul>
<li>有监督的情形：知道观察序列对应的状态值，直接对训练语料进行统计计数即可，即最大似然</li>
<li>无监督的情形：不知道观察序列对应的状态值，只知道可能的状态集合（用EM）</li>
</ul>
</li>
<li><p><strong>生成与判别</strong></p>
<ul>
<li><p>“纵观全局 or 聚焦一处” 分别对应 生成 or 判别</p>
</li>
<li><p>机器学习有两大类模型：生成式模型、判别式模型</p>
</li>
<li><p>生成模型：学习得到<strong>联合概率分布P(x,y)</strong>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。</p>
</li>
<li><p>判别模型：学习得到<strong>条件概率分布P(y|x)</strong>，即在特征x出现的情况下标记y出现的概率。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/generation and discrimination.png" alt="generation and discrimination" style="zoom:43%;"></p>
<ul>
<li>已知生成模型可以得到各个判别模型；已知判别模型无法得到生成模型，除非已知所有可能的判别关系</li>
<li>判别模型：<ul>
<li>优点：所需数据量小,计算量小，对单一类别判定准确率高。可随意增加新特征。</li>
<li>缺点：无法全局优化，只能完成目标任务，没有提供额外信息的潜力，应用范围受限。</li>
</ul>
</li>
<li>生成模型：<ul>
<li>优点：信息全面，可实现全局优化。</li>
<li>缺点：所需数据量大，计算量大，增加新特征的计算成本高。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么HMM是生成式模型? </p>
<ul>
<li>建模所有状态之间的转移关系和状态与所有词汇的发射关系</li>
<li>所有隐状态概率都要计算</li>
</ul>
</li>
<li><p>最大熵是判别式模型</p>
<ul>
<li>给定条件，计算结果（计算条件概率）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>MEMM 最大熵隐马：最大熵 + HMM</strong> (偏向最大熵)</p>
<ul>
<li><p>在解决序列标注问题时，HMM的输入信息只有参数(π, A, B)和观察序列(即每个状态对应的字)，<strong>没有办法接受更丰富的特征</strong>(例如更多的上下文文字等)。为了解决这个问题，提出了MEMM模型。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM.png" alt="MEMM" style="zoom: 33%;"></p>
</li>
<li><p>MEMM模型在预测当前状态时,将前一个状态和与当前观察值相关的一组特征一起做为最大熵模型的输入,来预测当前状态。MEMM与HMM不同，是判别式模型。（有点像RNN）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/MEMM model.png" alt="MEMM model" style="zoom:43%;"></p>
</li>
<li><p>HMM计算产生整个观察序列的最优状态序列，是全局最优。</p>
</li>
<li><p>MEMM计算单个观察值判定的单个最优状态，是局部最优。</p>
</li>
</ul>
</li>
<li><p><strong>CRF (Conditional Random Field) 条件随机场 (域)</strong></p>
<ul>
<li><p>“在更加宽广的上下文上进行判别”</p>
</li>
<li><p>HMM是特殊的CRF</p>
</li>
<li><p>CRF计算由整个观察序列判定的最优状态序列，是<strong>全局最优</strong>。其中，每个可能的状态序列的概率这样计算：对于序列中的每个状态计算一组特征函数值，然后计算所有状态的特征函数值之和并归一化。</p>
</li>
<li><p>判别式模型</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CRF.png" alt="CRF" style="zoom: 33%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/HMM_MEMM_CRF.png" alt="HMM_MEMM_CRF" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>SVM (support vector machine) 支持向量机</strong></p>
<ul>
<li><p>“从线性到非线性分类”</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM.png" alt="SVM" style="zoom: 33%;"></p>
</li>
<li><p>支持向量机(SVM)同最大熵一样是一种常用的线性(Log linear)分类器。</p>
</li>
<li><p>SVM求使得Margin最大的分类面,并将<strong>Margin上的向量称为支持向量</strong>（绿边上的向量点）。</p>
</li>
<li><p>通过计算样本与哪一类支持向量的内积更大来判断样本类别。</p>
</li>
<li><p>对于线性不可分的样本集,可以将其投射到高维空间中来分割。</p>
</li>
<li><p>SVM用核函数来方便计算高维空间中样本点之间的内积：核函数可以在原空间中计算高位空间中的内积。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVM model.png" alt="SVM model" style="zoom:43%;"></p>
</li>
<li><p><strong>kernel function</strong> （在原维度计算kernel函数就可视为在高维做计算）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/kernel function.png" alt="kernel function" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Topic Models</strong> 主题模型</p>
<ul>
<li><p>LSA, pLSA, LDA</p>
</li>
<li><p>判断图片相似度：匹配像素</p>
</li>
<li><p>判断两篇文章相似度：匹配词汇，每篇文章词汇分布</p>
<ul>
<li><p>或者匹配词汇出现频率</p>
</li>
<li><p>下图有误，行不全是D1，是表示不同文章</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word_match.png" alt="word_match" style="zoom:43%;"></p>
</li>
<li><p>词汇在文章出现，也相当于文章的上下文</p>
</li>
<li><p>避免选择类似冠词the出现频率过高，或者频率过低但又很重要的词汇</p>
</li>
<li><p>逆文档频率（term frequency-inverse document frequency, TF-IDF）</p>
<ul>
<li><p>TF-IDF 越高，则改词汇就很重要</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/TF-IDF.png" alt="TF-IDF" style="zoom:43%;"></p>
</li>
<li><p>但这个词汇向量只在乎了词语的出现频率（属于基于词袋），忽略了语义，词汇的信息（词汇序列才产生信息，与DNA碱基对序列同理）</p>
</li>
</ul>
</li>
<li><p>主题模型是基于词袋的模型</p>
</li>
<li><p>可用于检索，是否与某个‘词’相关，可以使用，但想知道与这个‘词’更细节的语义则不行</p>
</li>
<li><p><strong>潜在语义分析LSA（Latent Semantic Analysis）</strong></p>
<ul>
<li><p>主题就是一个潜在语义</p>
</li>
<li><p>n个文章 -&gt; k个主题 -&gt; m词汇</p>
</li>
<li><p>文章-主题矩阵（文章涉及主题分布），主题-词汇矩阵（主题涉及词汇分布）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSA.png" alt="LSA" style="zoom: 25%;"></p>
</li>
<li><p>拥有右边矩阵，想得到左边两个矩阵</p>
</li>
<li><p>SVD 奇异值分解（矩阵分解）（存在负值，负值不符合物理意义的解释）</p>
</li>
<li><p>LSA只是形式上拟合了文档-主题-词汇的关系,但并没有真正表达这种关系</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>概率潜在语义分析 pLSA</strong></p>
<ul>
<li><p>在LSA基础上，输出概率值，才可赋予物理意义</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA.png" alt="pLSA" style="zoom:43%;"></p>
</li>
<li><p>箭头表示依赖关系</p>
</li>
<li><p>P(d) 文章被抽中的概率（属于先验概率）</p>
</li>
<li><p>P(z|d) 给定文章，主题的概率，P(w|z) 给定主题，词汇的概率（两个属于后验概率）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/pLSA model.png" alt="pLSA model" style="zoom:43%;"></p>
</li>
<li><p>求出所有边的概率，则可以得到 文本-主题矩阵，主题-词汇矩阵</p>
</li>
<li><p><strong><em>只知道观察值，想知道内部的两个隐状态，则用 EM算法</em></strong></p>
</li>
</ul>
</li>
<li><p><strong>潜在狄利克雷分配 LDA</strong>（latent Dirichlet allocation）</p>
<ul>
<li><p>pLSA中单词和主题的先验分布都假设是均匀分布的，也就是假设我们对他们的先验分布一无所知。这种假设使得pLSA比较容易出现过拟合。</p>
</li>
<li><p>文档生成话题和话题生成单词的过程是典型的多项分布,在贝叶斯学习中，狄利克雷分布常作为多项分布的先验分布使用 LDA 将狄利克雷分布做为话题和单词生成的先验分布</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA.png" alt="LDA" style="zoom:43%;"></p>
</li>
<li><p>给文章根据狄利克雷分布随机分配个主题</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LDA model.png" alt="LDA model" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Deep Learning Models</strong> 深度学习模型</p>
<ul>
<li><p>属于生成式模型</p>
</li>
<li><p>线性模型：所有特征加权求和，通过激活函数，则成为线性感知机</p>
<ul>
<li>logistic（缩小输出范围）</li>
<li>softmax（所有值变为概率分布，总值=1）</li>
<li>KL散度（交叉熵）评估输出概率分布和真实分布的差距</li>
</ul>
</li>
<li><p>人工神经网络</p>
<ul>
<li>神经元激活规则<ul>
<li>主要是指神经元输入到输出之间的映射关系,一般为非线性函数。</li>
</ul>
</li>
<li>网络的拓扑结构<ul>
<li>不同神经元之间的连接关系。</li>
</ul>
</li>
<li><p>学习算法</p>
<ul>
<li>通过训练数据来学习神经网络的参数。</li>
</ul>
</li>
<li><p>ANN</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/ANN.png" alt="ANN" style="zoom: 50%;"></p>
</li>
<li><p><strong>全连接前馈神经网络</strong></p>
<ul>
<li>在前馈神经网络中,各神经元分别属于不同的层。整个网络中无反馈，信号从输入层向输出层单向传 播,可用一个有向无环图表示。</li>
<li>全连接复杂度高，发挥全部能力</li>
<li>容易过拟合（使用dropout价格低复杂度，避免过拟合）</li>
<li>通用近似定理<ul>
<li>对于具有线性输出层和至少一个使用 “挤压” 性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够,它可以以任意精度来近似任何从一个定义在实数空间中的有界闭集函数</li>
<li>“挤压” 性质：将输入数值范围挤压到一定的输出数值范围 </li>
<li>不是“挤压”性质的就是线性的</li>
</ul>
</li>
<li>反向传播更新参数</li>
</ul>
</li>
</ul>
</li>
<li><p>深度学习三个步骤</p>
<ul>
<li>定义网络 -&gt; 损失函数 -&gt; 优化</li>
</ul>
</li>
<li><p>梯度爆炸</p>
<ul>
<li>若初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后,可能会导致求导的结果很大，形成梯度爆炸</li>
</ul>
</li>
<li><p><strong>梯度消失</strong></p>
<ul>
<li><p>若使用标准化初始w，那么各个层次的相乘都是0-1之间的小数,而激活函数f的导数也是0-1之间的数,其连乘后,结果会变的很小，导致梯度消失</p>
</li>
<li><p>Activation Function</p>
<ul>
<li>sigmoid是非0均值（相当于加了一个偏置），还计算指数（指数计算相对复杂）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/activation function.png" alt="activation function"></p>
</li>
<li><p><strong>CNN</strong></p>
<ul>
<li><p>让每个神经元不代表一个像素，而是代表一个区域，而且，区域更容易捕捉局部特征</p>
</li>
<li><p>生物学上局部感受野</p>
</li>
<li><p>结构特点：<strong>局部连接，权重共享</strong></p>
</li>
<li><p>同时使用多组卷积核,每个负责提取不同特征</p>
</li>
<li><p><strong>padding</strong> 在图片外面填充一圈0（在没有padding，则边缘像素被访问概率相对对较低）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/conv.png" alt="conv" style="zoom: 33%;"></p>
</li>
<li><p><strong>pooling 池化</strong>：卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CNN.png" alt="CNN" style="zoom:43%;"></p>
</li>
<li><p>优点：善于提取特征、适用于分类、可并行、效率高</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>RNN</strong></p>
<ul>
<li><p>假设每次输入都是独立的，也就是说每次网络的输出只依赖于当前的输入</p>
</li>
<li><p>一个网络的输出做为另一个网络的输入</p>
<ul>
<li>y3是取决于前面y1, y2 (但重要程度是不一样的，越近越重要（但不是什么时候都符合这个，在序列较长时，存在远距离相关，则需要LSTM/GRU）)，但没有考虑到后者y4, y5（若需要考虑上下文，则需要双向RNN）</li>
<li>最后的y则包含所有信息</li>
<li>马尔科夫链每个状态只由前一个状态影响 而RNN每一个节点由前面所有节点影响</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN.png" alt="RNN" style="zoom:43%;"></p>
</li>
<li><p>LSTM （长短期记忆神经网络）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p>
</li>
<li><p>GRU （降低复杂度，能达到LSTM的效果）</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GRU.png" alt="GRU" style="zoom:43%;"></p>
</li>
<li><p>各种类型RNN</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/RNN model.png" alt="RNN model" style="zoom:43%;"></p>
</li>
<li><p>层叠循环神经网络：可以捕捉更加抽象的内涵</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deep RNN.png" alt="deep RNN" style="zoom:43%;"></p>
</li>
<li><p>双向循环神经网络：可以捕捉两侧的上下文信息</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/BRNN.png" alt="BRNN" style="zoom:43%;"></p>
</li>
<li><p>递归神经网络 Recursive Neural Network</p>
<ul>
<li><p>自然语言的句法结构</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/Recursive NN.png" alt="Recursive NN" style="zoom:43%;"></p>
</li>
<li><p>递归神经网络实在一个有向图无循环图上共享一个组合函数</p>
</li>
<li><p>叶子节点为输入</p>
</li>
<li><p>对于有歧义的句子（句法的歧义，<strong>不知道（形容）词语指向哪个主语</strong>）或者图片（图片中<strong>物品属于哪个主人</strong>），（需要从属关系的时候）可以用递归神经网络，用树的结构区别句法的结构和物品所属</p>
</li>
<li><p>可以退化为循环神经网络（属于RNN的特例）</p>
</li>
</ul>
</li>
<li><p>优点：善于累积序列信息、适用于序列标注或编码、不可并行、效率低</p>
</li>
</ul>
</li>
<li><p><strong>Attention</strong></p>
<ul>
<li><p>基于RNN的机器翻译中的注意力现象：源语言词汇对每个目标语的依赖程度不同</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/autoenocder.png" alt="autoenocder" style="zoom:43%;"></p>
</li>
<li><p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/attention.png" alt="attention" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>different network</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/different network.png" alt="different network" style="zoom:43%;"></p>
</li>
<li><p>word embedding</p>
<ul>
<li>one-hot(独热)编码<ul>
<li>向量维度为数据库中总词汇数,每个词向量在其对应词处取值为1，其余处为0</li>
<li>存在的问题: 维度灾难，语义鸿沟</li>
</ul>
</li>
<li><p>分布式表示 Distributed Representation</p>
</li>
<li><p>假设一个单词的语义和这个单词的上下文是相关的，我们可以使用这个单词的上下文来表示这个单词的语义信息</p>
</li>
<li><p>延申：语义相似的单词也应该具有相似的上下文。</p>
<ul>
<li><p>上下文(context): 在附近出现的所有单词的集合。–&gt; 窗口window</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/DR.png" alt="DR" style="zoom:43%;"></p>
</li>
<li><p>如何训练分布式</p>
<ul>
<li><p>共现矩阵 Co-occurrence Matrix</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/COM.png" alt="COM" style="zoom:43%;"></p>
</li>
<li><p>潜在语义分析 LSA (Latent Semantic Analysis)</p>
</li>
</ul>
</li>
<li><p>奇异值分解 Singular Value Decomposition</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SVD.png" alt="SVD" style="zoom:43%;"></p>
</li>
<li><p>前馈神经网络语言模型 FNNLM</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM.png" alt="FNNLM" style="zoom:43%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/FNNLM_2.png" alt="FNNLM_2" style="zoom:43%;"></p>
</li>
<li><p>Word2Vec</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/word2vec.png" alt="word2vec" style="zoom:43%;"></p>
</li>
<li><p>CBOW (continuous bag-of-words)</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/CBOW.png" alt="CBOW" style="zoom:43%;"></p>
</li>
<li><p>Skip-Gram</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/skip-gram.png" alt="skip-gram" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Pre-training 预训练 </p>
<ul>
<li>先验知识 -&gt; 学习模型 &lt;- 经验数据<ul>
<li>模型选择</li>
<li>参数设定</li>
<li>领域知识</li>
</ul>
</li>
<li>预训练模型提供了先验知识，不需要知道后面的任务目标，获得embedding</li>
<li>越深层，特征越具体</li>
<li>小样本学习 = 预训练 + 微调</li>
<li>目标任务与云训练模型最好是同类型的，效果会更好</li>
<li>RNN:不能准确捕捉远距离依赖，不能并行<ul>
<li>解决并行问题:给每个词编码，然后在词编码上用NN输出一个向量，NN可以并行。可以并行的网络可以做的更深</li>
<li>解决远距离依赖问题：给每个词编码的时候用注意力机制</li>
</ul>
</li>
<li>Transformer是一个典型的Encoder-Decoder模型，最初用于机器翻译。其中间部分(Encoder的输出)，是一个句子的向量表示。因此,Transformer的Encoder部分可以用作句子向量的预训练模型。</li>
</ul>
</li>
<li><p><strong>GNN</strong></p>
<ul>
<li><p>卷积模型、序列模型</p>
</li>
<li><p>无论卷积还是序列模型，实际上都假定输入对象的结构是一个<strong>均匀</strong>的网络。换言之，就是基本元素(像素、词汇)之间的关系结构是处处相同的。（符合欧式距离）</p>
</li>
<li><p>但是，现实中元素之间的结构并不总是均匀的。而任意图才是元素结构的一般化表示,网格与序列都只是一般图的特例</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/non_eu_and eu.png" alt="non_eu_and eu" style="zoom:43%;"></p>
</li>
<li><p>具有一般图结构的对象十分广泛，都无法用普通的CNN和RNN有效处理</p>
<p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_data.png" alt="graph_data" style="zoom:43%;"></p>
</li>
<li><p>若使用基于局部特征的方法来处理，一般图，如何定义卷积核的尺寸和方法? (<strong>CNN -&gt; GCN</strong>)</p>
</li>
<li><p>若使用序列的方法来处理一般图，如何给出序列的行走路线? (<strong>RNN -&gt; deepwalk</strong>)</p>
</li>
<li><p>通过NN获得<strong>embedding</strong>：包含 feature information + structure information</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graph_process.png" alt="graph_process" style="zoom:43%;"></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/gnn_process.png" alt="gnn_process" style="zoom:43%;"></p>
</li>
<li><p><strong>SDNE (Structural deep network embedding)</strong></p>
<ul>
<li><p>同时优化一阶和二阶相似度</p>
</li>
<li><p>每个结点用一个自编码器来重建领域信息,从而建模二阶相似度</p>
</li>
<li><p>节点之间使用拉普拉斯特征映射(反映节点之间的距离)来惩罚使得相邻节点距离较远的编码结果</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/SDNE.png" alt="SDNE" style="zoom: 50%;"></p>
</li>
</ul>
</li>
<li><p><strong>DeepWalk</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/deepwalk.png" alt="deepwalk" style="zoom:43%;"></p>
</li>
<li><p><strong>Node2Vec</strong></p>
<ul>
<li><p>与DeepWalk的最大区别在于，node2vec采用有偏随机游走,在广度优先(bfs)和深度优先(dfs)图搜索之间进行权衡,从而产生比DeepWalk更高质量和更多信息量的嵌入</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/node2vec.png" alt="node2vec" style="zoom:43%;"></p>
</li>
<li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/bsf_dsf.png" alt="node2vec_" style="zoom:43%;"></p>
</li>
<li><p>通过调整参数可以使得顶点的上下文在远距离邻居(DFS)和近距离邻居(BFS)之间调整</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/dsf_bsf.png" alt="dsf_bsf" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>Metapath2vec</strong>: 异质性网络中的顶点表示</p>
<ul>
<li><p>随机路径必须符合预设的若干元路径(Metapath)</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/metapath2vec.png" alt="metapath2vec" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>LINE</strong>: explicitly preserves both first-order and second-order proximities.</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/LINE.png" alt="LINE" style="zoom:43%;"></p>
</li>
<li><p><strong>PTE</strong>: learn heterogeneous text network embedding via a semi-supervised manner.</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/PTE.png" alt="{TE" style="zoom:43%;"></p>
</li>
<li><p><strong>GCN</strong></p>
<ul>
<li><p>以每个节点为核心，将其邻域设为卷积范围，卷积方法是汇聚邻居节点的信息做为核心节点的表示。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GCN.png" alt="GCN" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>GAT</strong></p>
<ul>
<li><p>基本的GCN中邻居节点的权重是平均的。</p>
</li>
<li><p>GAT中邻居节点的权重是可以训练的参数。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/GAT.png" alt="GAT" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p>GCN和GAT 是Transductive learning: 训练语料包含待标注语料，标注在训练过程中完成。</p>
<ul>
<li>优点：质量高</li>
<li>缺点：扩展性差(标注新样本需要全局重新训练)</li>
<li>GCN和GAT的缺点：网络的任何变化都要重新进行全局训练 (类似word embedding)</li>
</ul>
</li>
<li><p><strong>GraphSage</strong> 是Inductive learning：训练语料不包含待标注语料，先训练获得模型,然后泛化到测试语料上。</p>
<ul>
<li><p>GraphSage学习一个由邻居节点形成中心节点表示的神经网络模型(聚合函数)</p>
<p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage.png" alt="graphsage" style="zoom:43%;"></p>
</li>
<li><p>GraphSage是分层的，类似神经网络的层次。每一层的节点表示由前一层的邻居节点通过聚合函数获得。</p>
</li>
<li><p>随着层次的推进，每个结点实际上不仅可以获得邻居结点的信息，还可以获得更远距离的结点的信息。</p>
<p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/graphsage2.png" alt="graphsage2" style="zoom:43%;"></p>
</li>
<li><p>GraphSage的参数学习需要设计一个损失函数。（有监督/无监督）</p>
</li>
<li><p>对于无监督学习，损失函数应该让临近的节点的拥有相似的表示。</p>
</li>
</ul>
</li>
<li><p>文本分类: <strong>Text-GCN 2019</strong></p>
<ul>
<li><p>以文档和词汇为结点构造异质性网络，训练获得文档的向量表示并分类到类别。</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/text_gcn.png" alt="text_gcn" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>关系抽取: 2018</strong></p>
<ul>
<li><p>以依存句法树做为GCN的输入图,得到词汇的表示,进而分类词汇是否为关系标记词</p>
<p> <img src="https://github.com/soloistben/images/raw/master/machine_learning_image/关系抽取.png" alt="关系抽取" style="zoom:43%;"></p>
</li>
</ul>
</li>
<li><p><strong>个性化推荐: 2018</strong></p>
<ul>
<li><p>建立用户-用户-物品关系图</p>
</li>
<li><p>在关系图上分别得到用户和物品的表示</p>
</li>
<li><p>基于用户和物品的表示建立Rating预测模型</p>
<p><img src="https://github.com/soloistben/images/raw/master/machine_learning_image/个性化推荐.png" alt="个性化推荐" style="zoom:43%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/06/machine-learning/" data-id="ckfs9mt010015g6eg1w8yx7t5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-RNN-LSTM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/06/RNN-LSTM/" class="article-date">
  <time datetime="2020-05-06T12:11:28.000Z" itemprop="datePublished">2020-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/06/RNN-LSTM/">RNN_LSTM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><ul>
<li><p><strong>recurrent</strong> (it performs the same function for every input)</p>
</li>
<li><p>the output of the current input depends on the <strong>past one</strong> computation </p>
</li>
<li><p>RNN can use their <strong>internal state (memory)</strong> to process sequences of inputs</p>
</li>
<li><p>In RNN, all the inputs are <strong>related</strong> to each other (In other neural networks, all the inputs are independent of each other)</p>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/rnn.png" alt="rnn" style="zoom:50%;"></p>
</li>
<li><p>activation function is <strong>tanh()</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/tanh.png" alt="tanh" style="zoom:43%;"></p>
</li>
<li><p>output</p>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/yt.png" alt="yt" style="zoom:43%;"></p>
</li>
<li><p>Advantages</p>
<ul>
<li><strong>RNN</strong> can model sequence of data so that each sample can be assumed to be dependent on previous ones </li>
<li><strong>RNN</strong> are even used with convolutional layers to extend the effective pixel neighborhood. </li>
</ul>
</li>
<li><p>Disadvantages</p>
<ul>
<li>Gradient vanishing and exploding problems </li>
<li>Training an RNN is a very difficult task </li>
<li>It cannot process very long sequences if using <em>tanh</em> or <em>relu</em> as an activation function</li>
</ul>
</li>
</ul>
<h4 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM (Long Short-Term Memory)"></a>LSTM (Long Short-Term Memory)</h4><ul>
<li><p>LSTM is a modified version of RNN, which makes it easier to <strong>remember</strong> past data in memory </p>
</li>
<li><p>The <strong>vanishing gradient</strong> problem of RNN is resolved here </p>
</li>
<li><p>LSTM is well-suited to classify, process and predict <strong>time series given time lags of unknown duration</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/LSTM.png" alt="LSTM" style="zoom:43%;"></p>
</li>
<li><p><strong>Input gate</strong> — discover which value from input should be used to modify the memory </p>
<ul>
<li><strong>Sigmoid</strong> function decides which values to let through <strong>0,1.</strong></li>
<li><strong>tanh</strong> function gives weightage to the values which are passed deciding their level of importance ranging from<strong>-1</strong> to <strong>1</strong> </li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/input_gate.png" alt="input_gate" style="zoom:43%;"></p>
</li>
<li><p><strong>Forget gate</strong> — discover what details to be discarded from the block</p>
<ul>
<li><strong>sigmoid</strong> function looks at the previous state(<strong>ht-1</strong>) and the content input(<strong>Xt</strong>) and outputs a number between <strong>0</strong> (<em>omit this</em>) and <strong>1</strong>(<em>keep this**</em>)<strong> for each number in the cell state </strong>Ct−1**. </li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/forget_gate.png" alt="forget_gate" style="zoom:43%;"></p>
</li>
<li><p><strong>Output gate</strong> — the input and the memory of the block is used to decide the output </p>
<p><img src="https://github.com/soloistben/images/raw/master/rnn_lstm_image/output_gate.png" alt="output_gate" style="zoom: 33%;"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/06/RNN-LSTM/" data-id="ckfs9msz2000cg6egl639g931" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Amino-acids-proteins" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/06/Amino-acids-proteins/" class="article-date">
  <time datetime="2020-05-06T11:17:00.000Z" itemprop="datePublished">2020-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/06/Amino-acids-proteins/">Amino_acids_proteins</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Amino-acids-amp-proteins"><a href="#Amino-acids-amp-proteins" class="headerlink" title="Amino acids &amp; proteins"></a>Amino acids &amp; proteins</h4><ul>
<li><a href="https://www.bilibili.com/video/BV1aK41157GP?from=search&amp;seid=9348904521485452131" target="_blank" rel="noopener">what is protein</a></li>
<li>所有蛋白质由21种（基本单元）氨基酸构成</li>
<li><p>Amino acids 由 carbon (C), Oxygen (O), Hydrogen (H), Nitrogen(N), Sulfur (S) 构成</p>
<ul>
<li>硒代半胱氨酸是唯一的含有一个 Sel (硒原子) 的标准氨基酸</li>
</ul>
</li>
<li>Amino acids 由 Amino Group (氨基), Carboxyl Group (羧基),  Side Chain (侧链), Alpha Carbon (中央碳原子) 组成<ul>
<li>side chain 是不同氨基酸的唯一不同的部分，它决定了氨基酸的性质<ul>
<li><strong>Hydrophobic</strong> Amino acids 疏水性氨基酸具有丰富碳侧链，因此不能很好与水相互作用</li>
<li><strong>Hydrophilic</strong> Amino acids 亲水性（极性）氨基酸可以很好与水相互作用</li>
<li><strong>Charged</strong> Amino acids 带电荷的氨基酸与带相反电荷的氨基酸或其他分子相互作用</li>
</ul>
</li>
</ul>
</li>
<li><strong>primary structure（一级结构）</strong>是通过DNA编码的线性氨基酸序列，蛋白质中的氨基酸通过连接一个氨基酸氨基与另一个氨基酸羧基的肽键相连。每次肽键结合时都会释放一个水分子，相连的碳、氮、氧原子的序列构成了protein backbone（蛋白质的骨架）。</li>
<li><p>这些蛋白质链通常折叠成两种类型 <strong>secondary structure（二级结构）</strong></p>
<ul>
<li>alpha helix（螺旋）<ul>
<li>通过附近的氨基酸的氨基和羧基之间的 hydrogen bond (氢键) 稳定下来的右手螺旋线圈</li>
</ul>
</li>
<li>beta sheet（折叠）<ul>
<li>当两个或者多个相邻的链被氢键固定，形成 beta sheet</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>tertiary structure（三级结构）</strong></p>
<ul>
<li>蛋白质链的三维形状</li>
<li>这种形状由构成链的氨基酸的性质决定</li>
<li>许多蛋白质形成球状，把疏水侧链包围在<strong>内部</strong>，远离周围的水</li>
<li>膜结合蛋白外面聚集着疏水残基，以便它们可以与膜中的脂质相互作用</li>
<li>带电荷的氨基酸允许蛋白质与具有互补电荷的分子相互作用</li>
<li>许多蛋白质的功能依赖于它们的三维形状<ul>
<li>血红蛋白形成一个袋状，以在中心保持血红素，一种含有铁原子的小分子，用来与氧气结合</li>
</ul>
</li>
</ul>
</li>
<li>quaternary structure（四级结构）<ul>
<li>两条或者更多条多肽链可以通过几个亚基结合在一起，形成一个功能分子</li>
<li>血红蛋白的四个亚基相互作用以便他们的符合物可以在肺部吸收更多氧气，并将其他释放到体内</li>
</ul>
</li>
<li>protein size<ul>
<li>大多数蛋白质小于光的波长</li>
<li>血红蛋白分子的尺寸约为6.5nm</li>
</ul>
</li>
<li><p>蛋白质的三维形状决定了他们的功能</p>
<ul>
<li><p>defense（防御）</p>
<ul>
<li>antibody 抗体灵活的手臂通过识别并与病原体结合以识别它们，作为免疫系统破坏的目标来保护我们远离疾病</li>
</ul>
</li>
<li><p>communication</p>
<ul>
<li>insulin 胰岛素是一种小而稳定的蛋白质可以在血液中旅行时保持形状，来调节血糖</li>
</ul>
</li>
<li><p>enzymes</p>
<ul>
<li>alpha 淀粉酶是一种在唾液中消化淀粉的酶</li>
</ul>
</li>
<li><p>transport</p>
<ul>
<li>钙泵由镁辅助并由ATP提供动力，在每次肌肉收缩后，将钙离子移动回肌浆网</li>
</ul>
</li>
<li><p>storage</p>
<ul>
<li>铁蛋白是一种带通道的球形蛋白质，根据有机体的需要，允许铁原子进入和退出，在铁蛋白内部形成一个空间，使铁原子附着在其内壁，铁蛋白以无毒形式储存铁。</li>
</ul>
</li>
<li><p>structure</p>
<ul>
<li>胶原蛋白形成强大的三重螺旋，用来在整个身体中支撑结构。胶原蛋白分子可以形成细长的原纤维，并聚集形成胶原纤维，这种类型胶原蛋白存在于皮肤和筋中</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>CDS (coding region 基因编码区，<a href="https://www.omicsclass.com/article/805" target="_blank" rel="noopener">Coding DNA Sequence</a>)</p>
<ul>
<li><p>完整一段基因，能翻译成蛋白质的区域是“间隔的、不连续的”（蛋白质编码序列和非蛋白质编码序列两部分组成）</p>
</li>
<li><p>编码序列（编码区（基因序列）中可翻译成蛋白质的序列）：exon 外显子</p>
</li>
<li><p>非编码序列（编码区（基因序列）中不可翻译成蛋白质的序列）：intron 内含子</p>
<p><img src="https://github.com/soloistben/images/raw/master/protein/exon_intron.jpeg" alt="exon_intron"></p>
<p><img src="https://github.com/soloistben/images/raw/master/protein/mRNA_protein.jpg" alt="Gene"></p>
</li>
<li><p>启动子（属于ORF的调控序列）是在DNA上，是作用于转录阶段，mRNA不包含启动子</p>
</li>
<li><font color="red">对于真核生物的大部分ORF在转录时，是包括外显子和内含子的。转录后，RNA在内含子处进行自我切割，只有外显子可以转录为成熟mRNA</font>
</li>
<li><p>mRNA上的CDS区域外显子的核苷酸–翻译–&gt;氨基酸</p>
</li>
<li><p><strong>CDS</strong>（mRNA）是<a href="https://weibo.com/ttarticle/p/show?id=2309404030921119537751" target="_blank" rel="noopener">mRNA上从起始密码子到终止密码子之间的RNA序列</a>（指编码一段蛋白产物的序列，是与蛋白质密码子一一对应的序列）</p>
</li>
<li><p><strong>ORF</strong>（DNA）是open reading frame的缩写，翻译成开放阅读框，基因的有意编码部分也就是开放阅读框（ORF）</p>
</li>
<li><p><strong>基因组DNA分为基因序列和非基因序列——基因序列就是一个完整的表达盒它包括ORF和ORF的调控序列——ORF转录后经加工，使得内含子被切除，外显子组成mRNA序列——mRNA包括了不能翻译的UTR序列和能翻译的CDS。</strong></p>
</li>
<li><p>CDS是ORF中不包含UTR的外显子部分</p>
<p><img src="https://github.com/soloistben/images/raw/master/protein/CDS.png" alt="CDS"></p>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/06/Amino-acids-proteins/" data-id="ckfs9msyl0001g6egezcs27cn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/basic-protein/">basic protein</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
          </li>
        
          <li>
            <a href="/2020/10/02/SVM/">SVM</a>
          </li>
        
          <li>
            <a href="/2020/10/02/Decision-Tree/">Decision_Tree</a>
          </li>
        
          <li>
            <a href="/2020/09/11/Statistics/">Statistics</a>
          </li>
        
          <li>
            <a href="/2020/08/12/Bayes-MRF/">Bayes_MRF</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>