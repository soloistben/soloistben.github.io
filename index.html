<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Personal notes">
<meta property="og:type" content="website">
<meta property="og:title" content="MR.C">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Personal notes">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MR.C">
<meta name="twitter:description" content="Personal notes">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-C-plus-note" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/11/17/C-plus-note/" class="article-date">
  <time datetime="2020-11-17T08:15:24.000Z" itemprop="datePublished">2020-11-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/11/17/C-plus-note/">C_plus_note</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="one.-definition-of-class-without-pointer">One. Definition of Class (without pointer)</h4>
<ul>
<li><p>防止重复引用头文件（e.g. complex.h 对应 __COMPLEX__）（guard 防卫式声明）</p></li>
<li><p>public（定义提供外部调用函数），private （<font color="red"><strong>数据放入private</strong></font>，仅限类内用）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li>
<li>Constructor Function 构造函数
<ul>
<li>构造函数与类名称一样</li>
<li>Overloading 重载（一个类可以重载多个构造函数，下面特例不允许）</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="comment">// (default argument 默认实参，构造函数不需要写返回类型，默认为类)</span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)      <span class="comment">// initialization list 初始化列表（仅构造函数有）</span></span><br><span class="line">    &#123;...&#125;               	<span class="comment">// r, i 也可以在函数体内是assignments赋值</span></span><br><span class="line">    						<span class="comment">// （但是比初始化列表慢一点）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">complex</span>() 				<span class="comment">// 与上面构造函数冲突，不允许这么设置</span></span><br><span class="line">        : re(<span class="number">0</span>), im(<span class="number">0</span>)</span><br><span class="line">    &#123;...&#125; </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>Destructor Function 析构函数（不带指针的类，一般不用写析构函数）</li>
<li>Initialization list 初始化列表（仅构造函数有），<font color="red"><strong>优先考虑使用初始化列表</strong></font></li>
<li><p>构造函数一般写在public，也可以写在private中，但仅限类内调用，外部可用singleton调用，但只能调用一个</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> A&amp; <span class="title">getInstance</span><span class="params">()</span></span>;	<span class="comment">// 单例模式</span></span><br><span class="line">    setup() &#123;...&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    A();</span><br><span class="line">    A(<span class="keyword">const</span> A&amp; rhs);</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">A&amp; A::getInstance()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">static</span> A a;	<span class="comment">// 只有当调用时，才会创建该对象</span></span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用</span></span><br><span class="line">A::getInstance().setup();</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Template 模板
<ul>
<li><p>当未最终确定数据变量的类型时，或者需要多种数据类型，可以<strong>使用Template</strong></p></li>
<li><p>当设置多种类型，则当前就有上面类的定义，这是模板带来的代码膨胀（这不是缺点，是必须要两套代码）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(T r=<span class="number">0</span>, T i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">T <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function">T <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据放入private，仅限类内用</span></span><br><span class="line">    T re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">--------------------------------</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"complex.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">complex</span>&lt;<span class="keyword">double</span>&gt; c1(<span class="number">2.0</span>, <span class="number">1.0</span>);   <span class="comment">// &lt;double&gt; 确定变量类型</span></span><br><span class="line">	<span class="keyword">complex</span>&lt;<span class="keyword">int</span>&gt; c1(<span class="number">2</span>, <span class="number">1</span>);   </span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;c1.real();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>function template 函数模板</p>
<ul>
<li><p>当函数内容一样时，仅传入对象类型不一样，即可使用函数模板</p></li>
<li><p>编译器会对function template进行引数推导(argument deduction)（自动检测是什么类型）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">T</span>&amp; <span class="title">min</span> (<span class="title">const</span> <span class="title">T</span>&amp; <span class="title">a</span>, <span class="title">const</span> <span class="title">T</span>&amp; <span class="title">b</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">return</span> b&lt;a ? b:a;	<span class="comment">// 即使是自定义的类，操作符 &lt; 重载就可以</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>namespace 命名空间（防止命名冲突）
<ul>
<li>directive: &quot;using namespace std;&quot; 写demo时常用</li>
<li>declaration: &quot;using std::cout;&quot; 只声明了一个</li>
</ul></li>
<li>Inline Function 内联函数
<ul>
<li>在类内定义的函数，且不是复杂函数，都可以编译为inline函数</li>
<li>在类外定义的函数，需要在<font color="red">成员函数前设定特有字符 <strong>inline</strong></font></li>
<li>优点是执行得快</li>
<li>是否变成inline，由编译器决定（创建权在程序员手上，决定权在编译器上）</li>
</ul></li>
<li>pass Value &amp; pass Reference 传递参数：传值与传引用
<ul>
<li>直接pass value会因为值大小影响函数速度</li>
<li>引用本质也是指针，指针4个字节，速度快</li>
<li><font color="red"><strong>提前考虑是否需要const</strong></font></li>
<li><p><font color="red"><strong>建议所有参数均传引用，return也尽量返回引用</strong></font>（在变量生命周期外（局部变量）返回引用是错误的，其余情况都可以返回引用）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;</span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Friend Function 友元函数
<ul>
<li><p>可以直接访问private数据，比函数读取private数据更快（用来做特例，破坏类的整体封装性）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// constructor function </span></span><br><span class="line">    <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">        : re(r), im(i)</span><br><span class="line">    &#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// inline function，在类内定义的函数，且不是复杂函数</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125; <span class="comment">// 不会改变数据内容的函数，必须加const</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">double</span> re, im;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// friend function</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">complex</span>&amp; __doapl (<span class="keyword">complex</span>*, <span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 参数1会被改动，参数2不会被改动（ths是已创建变量，可以当成引用返回）</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    ths-&gt;re += r.re;	<span class="comment">// 直接读取private变量</span></span><br><span class="line">    ths-&gt;im += r.im;</span><br><span class="line">    <span class="keyword">return</span> *ths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inline function</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// this可以隐藏，不可以写</span></span><br><span class="line">    <span class="comment">// this是默认存在，不是临时变量</span></span><br><span class="line">    <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>相同class的各个object互为friend</strong>，可以直接获取private数据</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __COMPLEX__</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">define</span> __COMPLEX__</span></span><br><span class="line">  </span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">complex</span></span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="comment">// constructor function </span></span><br><span class="line">      <span class="keyword">complex</span>(<span class="keyword">double</span> r=<span class="number">0</span>, <span class="keyword">double</span> i=<span class="number">0</span>)   </span><br><span class="line">          : re(r), im(i)</span><br><span class="line">      &#123;...&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// inline function</span></span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">real</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> re;&#125;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">imag</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> im;&#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">complex</span>&amp; <span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp;);</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">func</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; param)</span></span></span><br><span class="line"><span class="function">      </span>&#123; </span><br><span class="line">          <span class="keyword">return</span> param.re+param.im; <span class="comment">// 直接读取private变量</span></span><br><span class="line">      &#125;</span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> re, im;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>operator overloading 操作符重载
<ul>
<li>其实跟定义函数一样，只是操作符更直观</li>
<li>编译器会去寻找相关的operator的被重写的函数（不管用什么方法，只能写一个，<strong>编译器只选择其中一个使用，没有优先级</strong>）</li>
<li>两种方法：
<ul>
<li><p>成员函数（类内），默认有this</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// 全局函数</span></span><br><span class="line">  <span class="comment">// 参数1会被改动，参数2不会被改动</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; __doapl(<span class="keyword">complex</span>* ths, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; r) <span class="comment">// const complex&amp; r （可以用complex，传值，但会慢）</span></span><br><span class="line">  &#123;</span><br><span class="line">      ths-&gt;re += r.re;</span><br><span class="line">      ths-&gt;im += r.im;</span><br><span class="line">      <span class="keyword">return</span> *ths;    <span class="comment">// 传递者(*ths)无需知道接收者(complex&amp;)是以reference形式接收</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 类的成员函数</span></span><br><span class="line">  <span class="keyword">inline</span> <span class="keyword">complex</span>&amp; <span class="keyword">complex</span>::<span class="keyword">operator</span> += (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; r)	<span class="comment">// 默认有this，但不能写出来</span></span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">return</span> __doapl(<span class="keyword">this</span>, r);    <span class="comment">// 返回类型必须是引用complex&amp;（防止c3+=c2+=c1; 连续赋值情况）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>非成员函数（类外），无this；写在类外，例如加法有多个情况，全部写在类内是有局限性</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将加法所有情况，都写出来</span></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// temp object 临时变量（函数执行完就销毁）</span></span><br><span class="line">    <span class="comment">// 不能当成reference返回</span></span><br><span class="line">    <span class="comment">// 要返回value，才能保存生成的临时变量（局部变量）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+real(y), imag(x)+imag(y));   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x)+y, imag(x));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(x+real(y), imag(y));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> + (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="keyword">operator</span> - (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(-real(x), -imag(x));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==real(y) &amp;&amp; imag(x)==imag(y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x, <span class="keyword">double</span> y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(x)==y &amp;&amp; imag(x)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">double</span> x, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; y)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> real(y)==x &amp;&amp; imag(y)==<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">complex</span> <span class="title">conj</span><span class="params">(<span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">complex</span>(real(x), -imag(x)); <span class="comment">// 共轭复数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// '&lt;&lt;' 该操作符只能写成全局函数（非成员函数）</span></span><br><span class="line"><span class="comment">// os 用于显示，一直在其内容变化，不能写成const </span></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> <span class="keyword">complex</span>&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; <span class="string">'('</span> &lt;&lt; real(x) &lt;&lt; <span class="string">','</span> &lt;&lt; imag(x) &lt;&lt; <span class="string">')'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul>
<h4 id="two.-definition-of-class-with-pointer">Two. Definition of Class (with pointer)</h4>
<ul>
<li>当定义类中含有指针，则必须有<font color="red">“拷贝构造”、“拷贝赋值”、“析构函数”</font>
<ul>
<li>拷贝构造、拷贝赋值属于深拷贝（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</li>
<li>若不实现深拷贝，系统默认是浅拷贝，即两个指针指向同一内存</li>
<li><p>析构函数则在对象离开作用域（对象定义的花括号内）后，在析构函数释放数据内存，再销毁对象</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __MY_STRING__</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __MY_STRING__</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;ostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造函数</span></span><br><span class="line">    String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr=<span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*带指针的类，必须要有 拷贝构造、拷贝赋值、析构函数*/</span></span><br><span class="line">    <span class="comment">// 拷贝构造（深拷贝，即开辟新内存，存储为新数据，与原本数据仅值相同）</span></span><br><span class="line">    <span class="comment">//（如果不重写，系统默认是浅拷贝，即两个指针指向同一内存）</span></span><br><span class="line">    String(<span class="keyword">const</span> String&amp; str);  </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拷贝赋值（深拷贝）</span></span><br><span class="line">    String&amp; <span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 析构函数（带有指针的类，需要在对象离开作用域（对象定义的花括号）后，在系够函数释放数据内存）</span></span><br><span class="line">    ~String();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">char</span>* <span class="title">get_data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> m_data;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 指针 动态分配</span></span><br><span class="line">    <span class="comment">// 数组 固定分配</span></span><br><span class="line">    <span class="keyword">char</span>* m_data;	<span class="comment">// 32位，指针4Byte</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> <span class="keyword">char</span>* cstr)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (cstr)   <span class="comment">// 传入字符串，不是0</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 最后一位有标识符号</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(cstr)+<span class="number">1</span>]; </span><br><span class="line">        <span class="built_in">strcpy</span>(m_data, cstr);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;  <span class="comment">//没有传入字符串</span></span><br><span class="line">        m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">1</span>];</span><br><span class="line">        *m_data = <span class="string">'\0'</span>;     <span class="comment">// 默认只有最后的标识符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::String(<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 深拷贝</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String&amp; String::<span class="keyword">operator</span> = (<span class="keyword">const</span> String&amp; str)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 拷贝赋值 </span></span><br><span class="line">    <span class="comment">// 判断 两者是否指向同一内存（必须操作）</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> == &amp;str)</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 清除原指针指向</span></span><br><span class="line">    <span class="keyword">delete</span>[] m_data;</span><br><span class="line">    <span class="comment">// 重新分配内存</span></span><br><span class="line">    m_data = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.m_data)+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">strcpy</span>(m_data, str.m_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> String::~String()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">delete</span>[] m_data;	<span class="comment">// 数组的分类内存，则需要delete[]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ostream&amp; <span class="keyword">operator</span> &lt;&lt; (ostream&amp; os, <span class="keyword">const</span> String&amp; x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> os &lt;&lt; x.get_data();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">-------------------------------</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"my_string.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">s1</span><span class="params">(<span class="string">"c++"</span>)</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">s2</span><span class="params">(<span class="string">"hello"</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">String <span class="title">s3</span><span class="params">(s1)</span></span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s1&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s2&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    s3 = s2;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; s3&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h4 id="three.-stack-heap">Three. Stack &amp; Heap</h4>
<ul>
<li>stack 栈：是存在于作用域中的一块内存空间。(local object / auto object)
<ul>
<li>当在作用域内调用函数时，函数会形成一个stack来防治</li>
<li>当在作用域内创建新对象（局部变量），则会开辟新内来存储它，当作用域结束，则会被释放掉（调用析构函数）</li>
</ul></li>
<li>heap 堆：由操作系统提供的全局空间，动态分配。（每次分配内存，需要手动销毁）（heap object）
<ul>
<li>若没有手动delete，会出现内存泄漏（memory leak）</li>
<li>对象离开作用域，指针p结束了，但指向p的对象仍然存在，没有机会去delete它了</li>
<li>new 一个对象，先是分配内存，再调用构造函数 -&gt; （c 语言）malloc(n) + 数据转型 +调用构造函数
<ul>
<li>指针p为对象在内存中的起始位置，同时是类中的this</li>
</ul></li>
<li>delete：先调用析构函数，再释放内存 -&gt; （c 语言）调用析构函数 + free(p)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Complex</span>&#123;</span>...&#125;;		<span class="comment">// 定义类</span></span><br><span class="line">...</span><br><span class="line"><span class="function">Complex <span class="title">c3</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;	<span class="comment">//global object</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="function">Complex <span class="title">c1</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;	<span class="comment">// c1所占用空间来自stack，离开作用域则会自动销毁</span></span><br><span class="line">    Complex* p = <span class="keyword">new</span> Complex(<span class="number">3</span>);	<span class="comment">// 由系统动态分配全局内存，离开作用域不会自动销毁，则需手动销毁</span></span><br><span class="line">    <span class="keyword">delete</span> p；</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> Complex <span class="title">c2</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span>;	<span class="comment">// static object</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>static object 静态对象：离开作用域仍然存在，直至整个程序结束
<ul>
<li>静态变量或者函数只有一份内存</li>
<li>非静态函数或变量，多次被调用，则会产生多份内存</li>
<li>静态类内变量，则是所有对象共有这个静态数据（必须给它定义，<strong>类内声明，类外定义</strong>）</li>
<li><p>静态的成员函数没有 this</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span>	// 银行账户类</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">double</span> m_rate;	<span class="comment">// 利率多少与用户无关，所以应该是共同一样的，因此为静态变量</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">set_rate</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span>&amp; x)</span></span>&#123;m_rate=x;&#125; <span class="comment">// 静态变量只能由静态函数处理</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> Account::m_rate = <span class="number">8.0</span>;	<span class="comment">// 定义静态变量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	Account::set_rate(<span class="number">5.0</span>);		<span class="comment">// 可以直接调用</span></span><br><span class="line">    </span><br><span class="line">    Account a;</span><br><span class="line">    a.set_rate(<span class="number">5.0</span>);	<span class="comment">// 对象调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>global object 全局对象：作用域为整个程序，直至整个程序结束</li>
<li><strong>new 动态分配内存块</strong>（VC编译器）
<ul>
<li>Cookie
<ul>
<li>当在debug模式调用程序时，除了类的大小，不仅有上下两个cookie（最终大小必须是类的倍数，不足则加pad）而且会分配更多空间（方便系统回收内存）</li>
<li><p>在正常执行模式调用程序时，只有类的大小和cookie（最终大小必须是类的倍数）</p>
<p><img src="http://github.com/soloistben/images/raw/master/cplus_image/动态分配内存.png" alt="动态分配内存" style="zoom: 50%;"></p></li>
<li>程序员只看到绿色那块（类的大小），不会看到完整cookie的大小</li>
<li><strong>Cookie (16): 00000041 表示 大小为4×16=64，1表示操作系统已经分配出去</strong>（因为16进制在二进制后面四个比特位均为0），0表示已经归还给操作系统，记录了整体长度，方便程序知道回收大小</li>
</ul></li>
<li><p>动态分配数组 <img src="http://github.com/soloistben/images/raw/master/cplus_image/动态分配数组内存.png" alt="动态分配数组内存" style="zoom:67%;"></p>
<ul>
<li>3 表示数组大小</li>
<li>数组的分类内存，则需要delete[]，（系统才知道删除的是个数组），否则会出错 <img src="http://github.com/soloistben/images/raw/master/cplus_image/删除数组.png" alt="删除数组" style="zoom:75%;"></li>
</ul></li>
</ul></li>
</ul>
<h4 id="four.-object-oriented-programming-oop-object-oriented-design-ood">Four. Object Oriented Programming (OOP), Object Oriented Design (OOD)</h4>
<ul>
<li>类与类之间的关系
<ul>
<li><strong>Inheritance</strong> 继承（<strong>is a</strong>）
<ul>
<li><p>public, protected, private 三种继承（Java只有public继承）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>：<span class="title">public</span> <span class="title">Base</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>父类所有参数和函数都被继承（Derived(子类)包含Base(父类)）</li>
<li>构造函数：先Base的默认构造函数（若没有默认的，并且多个构造函数，则需要表明用哪个构造函数），再Derived构造函数（由内而外）</li>
<li>析构函数：先Derived析构函数，再Base析构函数（由外而内）</li>
<li>base class 的析构函数必须是<strong>virtual function（虚函数）</strong>，否则出现undefined behavior
<ul>
<li>在成员函数前加上virual，就变成成员函数</li>
<li>父类中函数，继承的是调用权</li>
<li>non-virtual函数：不希望子类override（重新定义）该函数</li>
<li>virtual函数：希望子类override（重新定义）该函数，并且默认有一个定义（空函数也是定义的一种）</li>
</ul></li>
<li><p>pure virtual 函数：希望子类必须override（重新定义）该函数，默认没有定义</p>
<p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> <span class="keyword">const</span></span>=<span class="number">0</span>;	<span class="comment">// pure virtual</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">error</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; msg)</span></span>;	<span class="comment">// impure virtual</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">ObjectID</span><span class="params">()</span> <span class="keyword">const</span></span>;	<span class="comment">// non-virtual</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
<li><strong>Composition</strong> 复合 （<strong>has a</strong> 包含）
<ul>
<li>class A 内部定义 class B的对象（表示 class A has a class B）</li>
<li>class A is Container 容器, class B is Component 组件（A 包含B）</li>
<li>构造函数：先Component的默认构造函数（若没有默认的，并且多个构造函数，则需要表明用哪个构造函数），再Container构造函数（由内而外）</li>
<li>析构函数：先Container析构函数，再Component析构函数（由外而内）</li>
</ul></li>
<li><strong>Delegation</strong> 委托 （<strong>Composition by Reference</strong>）
<ul>
<li>class A 内部定义 class B的对象指针</li>
<li>需要用class B的对象时，才会去创建它，不像Composition一样同步创建class对象
<ul>
<li>Composition：当A对象复制三遍，则三个A对象分别创建三个不同B对象</li>
<li>Delegation：当A对象复制三遍，则三个A对象指向同一个B对象，则可以节省空间（共享内存）
<ul>
<li>共享内存：不能轻易发生修改</li>
<li>当需要修改其中一个A对象时，拷贝一个B对象，再修改；剩下两个A对象共享原本的B对象（copy on write, COW）</li>
</ul></li>
</ul></li>
<li>书写风格：Handle/Body（pimlp）：Container当成外部接口，Composition当成内部操作</li>
</ul></li>
</ul></li>
<li><strong>Inheritance + Compsition</strong>
<ul>
<li>子类中继承了父类，子类又有复合，构造函数顺序为父类、复合、子类</li>
<li>子类中继承了父类，父类又有复合，构造函数顺序为复合、父类、子类</li>
</ul></li>
<li><strong>Delegation + Inheritance</strong>（最强组合）
<ul>
<li><p>一份数据，多种表现</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Subject</span></span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">int</span> m_value;</span><br><span class="line">      <span class="built_in">vector</span>&lt;Observer*&gt; m_views;	<span class="comment">// Delegation 委托</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">attach</span><span class="params">(Observer* obs)</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          m_views.push_back(obs);</span><br><span class="line">  	&#125;</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">set_val</span><span class="params">(<span class="keyword">int</span> value)</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          m_value = value;	<span class="comment">// 一旦数据改变</span></span><br><span class="line">          notify();	<span class="comment">// 通知所有数据的可视化</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">notify</span><span class="params">()</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          <span class="comment">// 将所有可视化遍历更细显示的数据值</span></span><br><span class="line">          <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m_views.size(); ++i)</span><br><span class="line">              m_views[i] -&gt; update(<span class="keyword">this</span>, m_value);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Observer</span>	// 可以被 <span class="title">Inheritance</span> 继承，形成多种数据可视化</span></span><br><span class="line"><span class="class">  &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(Subject* sub, <span class="keyword">int</span> vlaue)</span></span>=<span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
<li>Composite <a href="https://www.bilibili.com/video/BV1gb411g7pa?p=13" target="_blank" rel="noopener">video</a></li>
<li><p>Prototype (在父类可以创建未来的派生子类；子类创造自己，委托给父类)</p></li>
</ul></li>
</ul>
<h4 id="five.-generic-programming">Five. Generic Programming</h4>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/11/17/C-plus-note/" data-id="ckhlr6dh800040jeg937bfbp5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C/">C++</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-EM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/EM/" class="article-date">
  <time datetime="2020-10-06T13:07:10.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/EM/">EM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/EM/" data-id="ckhlr6dhy000d0jegi6xek7e9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hidden-Markov-Model" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/Hidden-Markov-Model/" class="article-date">
  <time datetime="2020-10-06T12:55:04.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><strong>Hidden Markov Model = Markov Random Field + Time</strong>
<ul>
<li>动态模型=概率图模型+时间（可以是真正的时间，也可以是序列）</li>
<li><p>GMM高斯混合模型（样本之间独立同分布）；但是动态模型，样本之间不是独立同分布</p>
<p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM1.png" alt="HMM1" style="zoom: 67%;"></p></li>
<li>图中动态模型包括：横向的时间关系（Time），纵向的混合关系（Mixture，不同变量混合）</li>
<li><p><strong>若hidden variable是离散的，则动态模型为HMM</strong>；线性连续的，则是Kalmm Filter 卡尔曼滤波器；非线性连续的，则是Partide Filter</p></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/HMM/HMM2.png" alt="HMM2" style="zoom:75%;"></p></li>
<li>HMM: param λ = (π, A, B)（π是初始概率分布、A转移矩阵、发射矩阵）
<ul>
<li><strong>observed variable O</strong>: o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>t</sub>,... → V = {v<sub>1</sub>, ...v<sub>M</sub>} （观察值，观测变量o的值域）</li>
<li><strong>hidden variable I</strong>: i<sub>1</sub>, i<sub>2</sub>, ..., i<sub>t</sub>,... → Q = {q<sub>1</sub>, ...q<sub>N</sub>} （隐变量i的值域）</li>
<li><span class="math inline">\(A = [a_{ij}], a_{ij} = P(i_{t+1}=q_{j}|i_{t}=q_{i})\)</span></li>
<li><span class="math inline">\(B = [b_{j(k)}], b_{j(k)} = P(o_{t}=v_{k}|i_{t}=q_{j})\)</span></li>
</ul></li>
<li>两个假设
<ul>
<li>齐次马尔可夫假设
<ul>
<li><span class="math inline">\(P(i_{t+1}|i_{t}, i_{t-1}, ..., i_{1}, o_{t}, o_{t-1}, ..., o_{1}) = P(i_{t+1}|i_{t})\)</span></li>
<li>隐状态i<sub>t+1</sub>只与隐状态i<sub>t</sub>有关</li>
</ul></li>
<li>观测独立假设
<ul>
<li><span class="math inline">\(P(o_{t}|i_{t}, i_{t-1}, ..., t_{1}, o_{t-1}, ..., o_{1}) = P(o_{t}|i_{t})\)</span></li>
<li>观测状态o<sub>t</sub>只与观测状态i<sub>t</sub>有关</li>
</ul></li>
</ul></li>
<li>HMM解决三个问题（已知参数 λ = (π, A, B)）
<ul>
<li>Evalution → P(O|λ) （已知参数下，求解O现象的概率）→ 前向后向算法</li>
<li>Learning → 求解参数，λ = argmax P(O|λ) → EM算法（以前是Baum Welch算法）</li>
<li>Decoding → 找到一个状态序列，使 P(I|O)达到最大（I～ = argmax P(I|O)）
<ul>
<li>预测问题：<span class="math inline">\(P(i_{t+1}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，预测下一时刻t+1的隐状态</li>
<li><strong>滤波问题</strong>：<span class="math inline">\(P(i_{t}|o_{1},...,o_{t-1}, o_{t})\)</span> 已知t个观察序列，求解当前时刻t的隐状态</li>
</ul></li>
</ul>
<p><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=82" target="_blank" rel="noopener">白板系列</a></p></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/Hidden-Markov-Model/" data-id="ckhlr6dif000q0jegedztm7d4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-PGM-Inference" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/06/PGM-Inference/" class="article-date">
  <time datetime="2020-10-06T07:09:49.000Z" itemprop="datePublished">2020-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Inference 推断 → 求概率
<ul>
<li>P(X) = P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>)</li>
<li>边缘概率：$P(x_{i}) = Σ_{1}...Σ_{i-1} Σ_{i+1}...Σ_{p} P(X) $（除了i的都积分）</li>
<li>条件概率：<span class="math inline">\(P(x_{a}|x_{b}),(x=x_{a}\bigcup x_{b})\)</span></li>
<li>最大后验 MAP Inference：<span class="math inline">\(\hat{Z} = argmaxP(Z|X) \propto argmax P(Z,X)\)</span>（贝叶斯定理）（不一定直接求后验，只要得到最大即<span class="math inline">\(\hat{Z}\)</span>可）</li>
</ul></li>
<li>方法
<ul>
<li>精确推断
<ul>
<li>Variable Elimination (VE) 变量消除法</li>
<li><strong>Belief Propagation</strong> (BP) 信念传播（与反向传播不一样）（弥补VE缺点）
<ul>
<li>另外一个名称：Sum-Product Algorithm</li>
<li>针对树结构</li>
</ul></li>
<li>Junction Tree Algorithm
<ul>
<li>基于BP，由树结构扩展到普通图结构</li>
</ul></li>
</ul></li>
<li>近似推断
<ul>
<li>Loop Belief Propagation
<ul>
<li>基于BP，针对有环图</li>
</ul></li>
<li>Mente Carlo Inference
<ul>
<li>Importance Sampling</li>
<li>MCMC (Markov Chain Mente Carlo)</li>
<li>基于采样</li>
</ul></li>
<li>Variational Inference
<ul>
<li>确定性近似</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="variable-elimination">Variable Elimination</h5>
<p><img src="https://github.com/soloistben/images/raw/master/PGM_Inference/VE1.png" alt="VE1" style="zoom:75%;"></p>
<ul>
<li>假设a,b,c,d均为二值的随机变量{0,1}</li>
<li><span class="math inline">\(P(d) = Σ_{a,b,c} P(a,b,c,d) = Σ_{a,b,c} P(a)P(b|a)P(c|b)P(d|c)\)</span> （将各个条件概率看成因子）
<ul>
<li>→ P(a=0)P(b=0|a=0)P(c=0|b=0)P(d|c=0) + P(a=1)P(b=0|a=1)P(c=0|b=0)P(d|c=0) +...+ P(a=1)P(b=1|a=1)P(c=1|b=1)P(d|c=1) = 8*因子积</li>
<li>由于马尔可夫的性质，结点只与相邻的相关，与其他无关，可以优先将有关的先合并。</li>
<li>→ <span class="math inline">\(Σ_{b,c} P(c|b)P(d|c) Σ_{a} P(a)P(b|a)\)</span>（<span class="math inline">\(Σ_{a} P(a)P(b|a) = Σ_{a} P(a,b) = P(b)\)</span>， <strong>将P(a)看成一个函数Φ(a)，P(b|a)看成Φ(a,b)，则Σ<sub>a</sub> P(a)P(b|a)看成Φ<sub>a</sub>(b)</strong>）</li>
<li>→ <span class="math inline">\(Σ_{c} P(d|c) Σ_{b}P(c|b)Φ_{a}(b) = Σ_{c} P(d|c) Φ_{b}(c) = Φ_{c}(d)\)</span></li>
</ul></li>
<li>若是无向图，则为 <span class="math inline">\(P(a,b,c,d) = \frac{1}{Z}\prod Φ(x)\)</span></li>
<li>主要思想：乘法分配律（ab+ac=a(a+c)）</li>
<li>缺点：1、重复计算（求另外一个点时，则需要重新求（则没有存储中间结果，若是链很长，则计算量很大））；2、消去次序（一般相关最少的先消去，但在无向图找到最优消去次序是NP-Hard问题）</li>
</ul>
<h5 id="belief-propagation">Belief Propagation</h5>
<ul>
<li></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/06/PGM-Inference/" data-id="ckhlr6di9000l0jegeumyh3qn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Exponential-Family-Distribution" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/05/Exponential-Family-Distribution/" class="article-date">
  <time datetime="2020-10-05T08:43:34.000Z" itemprop="datePublished">2020-10-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="指数族分布">指数族分布</h4>
<ul>
<li>Gaussian Distribution, Bernoulli Distribution (Categorical Distribution), Binomial Distribution (Multinomial Distribution), Poisson Distribution, Beta Distribution, Dirichlet Distribution, Gamma Distribution</li>
<li><font color="red"><span class="math inline">\(P(x|η) = h(x) e^{η^{T} Φ(x) - A(η)}\)</span></font> （分为三部分）
<ul>
<li>η属于参数、p维向量；A(η): log partition function（配分函数）；h(x)与η无关，往往设置为1</li>
<li>partition function（源自于统计物理学）：<span class="math inline">\(P(x|θ) = \frac {1}{Z} \hat{P}(x|θ)\)</span> （Z为归一化因子，<span class="math inline">\(Z=\int \hat{P}(x|θ) d_{x}\)</span>）</li>
<li><span class="math inline">\(P(x|η) = \frac {1}{e^{A(η)}} h(x) e^{η^{T} Φ(x)} → e^{A(η)} = Z → A(η) = log Z\)</span>，所以A(η)是log partition function</li>
</ul></li>
<li>特点
<ul>
<li><strong>充分统计量 sufficient statistics</strong>：Φ(x)
<ul>
<li>统计量：对样本的加工，关于样本的一个函数，均值、方差等</li>
<li>充分：统计量就可以完整表达样本特征信息了</li>
<li>Online Learning （可以仅存储样本统计量信息，就不需要存储大量样本，起到压缩数据的效果）</li>
</ul></li>
<li><strong>共轭</strong>
<ul>
<li><span class="math inline">\(P(Y|X) = \frac{P(X|Y)P(Y)}{\int P(X|Y)P(Y) d_{Y}}\)</span>（后验概率是求不出来，或者积分难问题，太复杂，难以求解）</li>
<li>计算f(Y)后验分布的期望：近似推断（变分推断、MCMC）</li>
<li>若似然P(X|Y)与先验P(Y)共轭（如二项分布和Beta分布），则后验与先验同分布，则仅需算后验分布的参数即可，就可以不用计算积分</li>
</ul></li>
<li><strong>最大熵</strong>（无信息先验）
<ul>
<li>没有先验，则认为是所有样本等概率，但无法定量分析，则引入最大熵</li>
<li>赋予先验：共轭（为了计算方便）；最大熵（无信息先验）</li>
</ul></li>
</ul></li>
<li>模型和推断
<ul>
<li><strong>广义线性模型</strong>
<ul>
<li>目标：解决分类、回归问题</li>
<li>线性组合（w<sup>T</sup>x）</li>
<li>link function（激活函数的反函数）</li>
<li>指数族分布（y|x ~ 指数族分布）（如线性回归：y|x ~ N(μ,σ<sup>2</sup>)；线性分类：y|x ~ 0/1分布（Bernoulli））</li>
</ul></li>
<li><strong>概率图模型</strong>
<ul>
<li>无向图：RBM 波尔兹曼机</li>
</ul></li>
<li><strong>变分推断</strong>
<ul>
<li>指数族分布可以简化变分推断</li>
</ul></li>
</ul></li>
<li>Gaussian Distribution
<ul>
<li><span class="math inline">\(P(x|θ) = \frac{1}{\sqrt{2\pi}σ} e^{\frac{(x-μ)^2}{-2σ^2}},θ=(μ,σ^2)\)</span> 一维高斯分布
<ul>
<li>η = η(θ)，A(η) = A(η(θ))，将θ映射成η​</li>
<li><span class="math inline">\(P(x|\theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2\sigma^2}(x^2-2\mu x+\mu ^2)} = e^{-\frac{1}{2}log^{2\pi \sigma^2}} e^{-\frac{1}{2\sigma^2} {\begin{bmatrix} -2\mu \ 1 \end{bmatrix}}{\begin{bmatrix}x \\ x^2 \end{bmatrix}} -\frac{\mu^2}{2\sigma^2}} \\ = exp({\begin{bmatrix} \frac{\mu}{\sigma^2} \ \frac{-1}{2\sigma^2} \end{bmatrix}}{\begin{bmatrix}x \\ x^2 \end{bmatrix}} - (\frac{\mu^2}{2\sigma^2} + \frac{1}{2} log^{2\pi \sigma^2}))\)</span>
<ul>
<li><span class="math inline">\(η^T = {\begin{bmatrix}\frac{μ}{σ^2}\ \frac{-1}{2σ^2} \end{bmatrix}}, Φ(x) = {\begin{bmatrix}x \\ x^2 \end{bmatrix}}, A(η)=\frac{μ^2}{2σ^2}+ \frac{1}{2} log^{2\piσ^2}\)</span></li>
<li>设定<span class="math inline">\(η={\begin{bmatrix}η \\ η^2 \end{bmatrix}}\)</span>，则<span class="math inline">\(A(η) = -\frac{η_1^2}{4η_2}+\frac{1}{2}log^{-\frac{\pi}{η_2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>充分统计量Φ(x) 与 对数配分函数A(η)</strong>
<ul>
<li>P(x|η) 概率积分为1</li>
<li><span class="math inline">\(P(x|η) = \frac{1}{e^{A(η)}}h(x)e^{η^T Φ(x)}→e^{A(η)}=\int h(x)e^{η^TΦ(x)}d_x\)</span></li>
<li>两边对η求导：<span class="math inline">\(e^{A(η)}*A&#39;(η) = \frac{d(\int h(x)e^{η^TΦ(x)}d_{x})}{dη}=\int h(x) Φ(x)e^{η^TΦ(x)}d_{x}\)</span></li>
<li><span class="math inline">\(A&#39;(η) =\int h(x) Φ(x)e^{η^T Φ(x)-A(η)}d_{x}=\int Φ(x)P(x|η)d_{x} = E_{P(x|η)}[Φ(x)]\)</span></li>
<li><span class="math inline">\(A&#39;&#39;(η)=Var_{P(x|η)} [Φ(x)]\)</span>，则A(η)是一个凸函数</li>
</ul></li>
<li><strong>充分统计量Φ(x) 与 极大似然估计</strong>
<ul>
<li>Data: D = {x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>} N个样本</li>
<li><span class="math inline">\(η_{MLE}=argmax \ log^{P(D|η)}=argmax \ log^{\prod P(x_i|η)}=argmax \sum log^{P(x_i|η)} \\ = argmax \sum log^{h(x_i)e^{η^T Φ(x_i)-A(η)}} = argmax \sum [log^{h(x_i)}+η^T Φ(x_i)-A(η)]\)</span>
<ul>
<li><span class="math inline">\(→ η_{MLE} ∝argmax \ \sum [η^T Φ(x_i)-A(η)]\)</span></li>
<li><span class="math inline">\(\frac{d(Σ [η^T Φ(x_i)-A(η)])}{dη} =\sum \frac{d(η^T Φ(x_i)-A(η))}{dη}=\sum[Φ(x_i)-A&#39;(η)] =\sum Φ(x_i)- N*A&#39;(η)=0 \\ → A&#39;(η_{MLE})=\frac{1}{N}\sum Φ(x_i)\)</span></li>
<li><span class="math inline">\(η_{MLE} = A&#39;^{(-1)}(η_{MLE})\)</span>（反函数） → 则η<sub>MLE</sub>是可求解的，只需要记录即<span class="math inline">\(\frac{1}{N}\sum Φ(x_i)\)</span>可，不需要记录所有样本</li>
</ul></li>
</ul></li>
<li><strong>最大熵角度</strong>
<ul>
<li>若一个事件发生概率为p，其信息量为-log p （若p=1,信息量就为0；一个确定的事件没有信息量）</li>
<li>熵：<span class="math inline">\(E[-log^p]=\int -p(x)log^{p(x)} d_{x}=-\sum p(x)log^{p(x)}\)</span> （对信息的衡量，对信息可能性的衡量）（熵只与x的分布有关，与取值无关）</li>
<li>最大熵&lt;=&gt;等可能（利用最大熵对等可能定量分析）（最大熵，对未知的分布进行猜测，因为不知道，所以认为都是等可能的）（<strong>要想熵最大，未知的分布必须是等可能</strong>）
<ul>
<li><span class="math inline">\(H[p]=-\sum p(x)log^{p(x)}\)</span></li>
<li>假设x是离散的，P(x=1) = p<sub>1</sub>，P(x=2) = p<sub>2</sub>，...，P(x=k) = p<sub>k</sub>，Σp<sub>i</sub> = 1</li>
<li>则 <span class="math inline">\(max \ H[P] = max[-\sum p_i log^{p_i}],s.t.\sum p_i=1\)</span></li>
<li><span class="math inline">\(\hat{p_i} = argmax \ H[P] = argmin \ \sum p_i log^{p_i}\)</span> （优化问题）</li>
<li>拉格朗日：<span class="math inline">\(L(p, λ) = \sum p_i log^{p_i} - λ(1- \sum p_i)，\frac{dL}{dp_i} = log^{p_i} + 1 - λ = 0 → p_i = e^{λ-1}\)</span> (常数)</li>
<li>则 p<sub>1</sub> = p<sub>2</sub> = ... p<sub>k</sub> = 1/k，p(x)是了离散型均匀分布</li>
</ul></li>
<li>最大熵原理：<strong>在满足已知事实（约束条件）（已知数据）下，什么分布是具有最大熵的分布</strong>
<ul>
<li>Data = {x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>} 通过经验分布（概率分布<span class="math inline">\(\hat{P}(X=x) = \hat{p}(x) = \frac{count(x)}{N}\)</span>）定量描述数据</li>
<li>需要P分布的<span class="math inline">\(E_{\hat{p}}[x], Var_{\hat{p}}[x]\)</span>，设定f(x)是任意关于x的向量函数（<span class="math inline">\(f=[f_1,f_2,...,f_Q]^T\)</span>），则<span class="math inline">\(E_{\hat{p}}[f(x)]=Δ\)</span>（即是已知事实）</li>
<li><span class="math inline">\(P(X)→p(x),H[p]=-\sum p(x)log^{p(x)}\)</span> $= argmin ∑ p(x)log^{p(x)}, s.t.∑p(x)=1,E_p[f(x)]=∑p(x)f(x)=E_{}[f(x)]=Δ $</li>
<li>拉格朗日：<span class="math inline">\(L(p, λ_0, λ) = \sum p(x)log^{p(x)} - λ_0(1- \sum p(x)) - λ^T(Δ - \sum p(x)f(x))\)</span></li>
<li>求导：<span class="math inline">\(\frac{dL}{d_{p(x)}} = \sum [log^{p(x)} + 1] - \sum λ_0 - \sum λ^T f(x) = Σ[log^{p(x)}+1-λ_0-λ^T f(x)] = 0 \\ \rightarrow log^{p(x)}+1-λ_0-λ^T f(x)=0 →p(x) = e^{λ^T f(x) - (1-λ_0)}\)</span></li>
<li>令η=λ<sup>T</sup>，Φ(x)=f(x)，A(η)=(1-λ<sub>0</sub>)</li>
<li>则用最大熵原理推出，p(x)是指数分布时熵最大</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/05/Exponential-Family-Distribution/" data-id="ckhlr6di2000f0jegjrw2ia12" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Dimensionality-Reduction" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Dimensionality-Reduction/" class="article-date">
  <time datetime="2020-10-02T13:10:36.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Dimensionality-Reduction/">Dimensionality_Reduction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>过拟合
<ul>
<li>解决方法：增加数据、正则化、降维</li>
<li>原因：<strong>维度灾难</strong>
<ul>
<li>在没有很多数据集时，只能降维</li>
<li>每增加一维，二值的特征，都是2的指数倍增长，要想覆盖所有样本空间，则需要2的指数倍数据才可以（而且往往不只是二值）</li>
<li><p>从几何层面看：</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR1.png" alt="DR1" style="zoom: 33%;"></p>
<ul>
<li>2维正方形面积：1，圆形：<span class="math inline">\(\pi*(0.5)^2\)</span></li>
<li>3维正方体体积：1，球体体积：<span class="math inline">\(\frac{4}{3}*\pi*(0.5)^3 = K*(0.5)^3\)</span></li>
<li>D维超立方体体积：1，超球体体积： <span class="math inline">\(K*(0.5)^D\)</span></li>
<li><p>D趋向无穷大之后，超球体体积约等于0，则为空心的，则数据分布在超立方体的四角，造成了样本数据十分稀疏且分布不均匀，因此很难分类</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR2.png" alt="DR2" style="zoom:33%;"></p></li>
<li>D维外超球体体积：<span class="math inline">\(K*1^D = K\)</span>，环形体积：外超球体体积 - 内超球体体积 = <span class="math inline">\(K - K(1-e)^D\)</span>
<ul>
<li>V外/V内 <span class="math inline">\(=1-(1-e)^D =1\)</span>（<span class="math inline">\(0&lt;e&lt;1\)</span>，D趋向无穷大之后，<span class="math inline">\((1-e)^D\)</span>趋向于0）</li>
<li>则无论e多小，在高维空间，环形体积约等于1，内超球体为空心，数据分布在外超球体壳上</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Data
<ul>
<li>N个p维样本 X（维度N×p）（设<span class="math inline">\(I\)</span>为N维全1列向量）</li>
<li>样本均值 <span class="math inline">\(\hat{X}=\frac{1}{N} \sum x_i = \frac{1}{N} X^T I\)</span>（维度p×1）</li>
<li>方差 <span class="math inline">\(S = \frac{1}{N} \sum (x_i - \hat{X})(x_i - \hat{X})^T = \frac{1}{N} X^T (I - \frac{1}{N} I I^T) (1-\frac{1}{N} I I^T)^T X = \frac{1}{N} X^T H H^T X = \frac{1}{N} X^T H X\)</span>
<ul>
<li>（维度p×p）</li>
<li><span class="math inline">\(H = I-\frac{1}{N} I I^T\)</span> centering matrix（将数据平移转换，数据分布在坐标中心）（维度N×N）</li>
<li><span class="math inline">\(H^T = H, H^2 = H H^T = (I-\frac{1}{N}I I^T) (I-\frac{1}{N} I I^T)^T = I-\frac{1}{N} I I^T = H\)</span></li>
<li><span class="math inline">\(H^n = H\)</span></li>
</ul></li>
</ul></li>
<li>降维方法
<ul>
<li>直接降维 （特征选择：人工选取重要特征 ）</li>
<li>线性降维
<ul>
<li><strong>Principal Components Analysis PCA 主成分分析</strong>
<ul>
<li><p>将线性相关的特征通过正交变换为线性无关（对原始特征空间的重构）（线性相关（存在2个以上特征之间联系））</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR3.png" alt="DR3" style="zoom: 67%;"></p></li>
<li>最大投影方差
<ul>
<li>找到一个u<sub>1</sub>平面，使投影间距达到最大（投影到u<sub>2</sub>平面，距离太小，没有意义）</li>
<li>这个平面就是主成分（线性无关的基(特征向量)为数据中的主要成分，降到k维，则选取第k大的特征值所对应的特征向量）</li>
<li>第1步：中心化：先将所有数据平移，利于计算，即 <span class="math inline">\(x_i - \hat{X}\)</span></li>
<li>第2步：投影到u<sub>1</sub>平面：<span class="math inline">\((x_i - \hat{X})^T u_1\)</span>（设定 <span class="math inline">\(|u_1|=1\)</span>，即<span class="math inline">\(u_1^T u_1=1\)</span>）</li>
<li>第3步：投影方差$ J =  ^2 =  = u_1^T S u_1$
<ul>
<li><span class="math inline">\(\hat{u_1} = argmax \ u_1^T S u_1,s.t. u_1^T u_1=1\)</span>
<ul>
<li>使用拉格朗日求解</li>
<li><span class="math inline">\(L(u_1, λ) = u_1^T S u_1 + λ(1-u_1^T u_1)\)</span></li>
<li><span class="math inline">\(\frac{dL}{du_1} = 2 S u_1 - 2λ u_1 = 0 → S u_1 = λ u_1\)</span></li>
<li><strong>u<sub>1</sub>为eigen-vector特征向量，λ为eigen-value特征值</strong></li>
<li>解法1：对方差矩阵特征分解，即可求解PCA</li>
<li>解法2：直接对原始数据进行操作：中心化后的 <span class="math inline">\(HX = UΣV^T\)</span> 进行奇异值分解（U和V均为正交矩阵），<span class="math inline">\(S = X^T H X = X^T H^T H X = (VΣU^T)(UΣV^T) = V Σ^2 V^T\)</span>（可先忽略1/N常数）（维度p×p），因此直接求解HX奇异值分解，也就是求解了S的特征分解</li>
<li>假设 <span class="math inline">\(B = H X X^T H = (UΣV^T)(VΣU^T) = U Σ^2 U^T\)</span>（维度N×N），则B与S有一样的eigenvalue，（S特征分解得到方向V（主成分）然后通过<span class="math inline">\(HX V\)</span>得到新坐标）（B特征分解直接得到坐标U，称为主坐标分析 <strong>principal coordinate analysis PCoA</strong>）</li>
<li><span class="math inline">\(HX V = UΣV^T V = UΣ\)</span>（UΣ是坐标矩阵），<span class="math inline">\(BUΣ = U Σ^2 U^T UΣ = UΣ Σ^2\)</span>（UΣ是特征向量组成的矩阵）</li>
</ul></li>
</ul></li>
</ul></li>
<li>最小重构代价
<ul>
<li>投影在u<sub>1</sub>平面的点恢复到原来样子的代价</li>
<li>设从原p维降到q维（下面u代表特征）
<ul>
<li><span class="math inline">\(x_i = \sum_k^p (x_i^T u_k) u_k, \hat{x_i} = \sum_k^q (x_i^T u_k) u_k\)</span> (用特征<span class="math inline">\(u_k\)</span>描述样本<span class="math inline">\(x_i\)</span>，<span class="math inline">\(x_i^T u_k\)</span>为距离大小，<span class="math inline">\(u_k\)</span>为单位大小，则第k维描述为<span class="math inline">\((x_i^T u_k) u_k\)</span>)</li>
<li>代价函数 <span class="math inline">\(L =\frac{1}{N} \sum||x_i - \hat{x_i}||^2 = \sum_{k=q+1}^p u_k^T S u_k = \sum_{k=q+1}^p λ_k, s.t. u_k^T u_k=1\)</span></li>
<li><span class="math inline">\(u_k = argmin \ L\)</span></li>
</ul></li>
</ul></li>
<li>概率角度
<ul>
<li>P-PCA
<ul>
<li>设定observed data X为p维（特征为连续型数据），latent variable Z为q维（q&lt;p）</li>
<li>设定<span class="math inline">\(Z\)</span>~<span class="math inline">\(N(0, I)\)</span>（服从高斯分布，q维）</li>
<li><span class="math inline">\(X = WZ + u + ε\)</span>​（X是Z的一个线性变换加噪声）</li>
<li><span class="math inline">\(X|Z\)</span>~<span class="math inline">\(N(WZ + u, σ^2 I)，X~N(u, WW^T+σ^2 I)\)</span></li>
<li>噪声<span class="math inline">\(ε\)</span>~<span class="math inline">\(N(0, σ^2 I)\)</span>（p维）</li>
<li>这也是一种Linear Gaussian Model（称该模型各向同性，对各方向影响是一样的）</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>求P(Z), P(X|Z), P(X), 最后求后验P(Z|X)、用EM求参数W, u, σ</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Dimensionality_Reduction/DR4.png" alt="DR4" style="zoom:75%;"></p>
<pre><code>  + 从服从高斯分布的Z中，投影点在方向W，进行线性变换得到X，也得到服从高斯分布的X|Z，方向W上有很多的高斯分布（各向同性）；X的分布不在方向W上，且中间很宽（因为高斯分布中间高两边低）[详情](https://www.bilibili.com/video/BV1aE411o7qd?p=27)</code></pre>
<ul>
<li>MDS</li>
<li>非线性降维
<ul>
<li>流型</li>
<li>Isomap</li>
<li>LLE</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Dimensionality-Reduction/" data-id="ckhlr6di0000e0jegq54p9jec" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-SVM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/SVM/" class="article-date">
  <time datetime="2020-10-02T08:50:04.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/SVM/">SVM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Data : N个p维样本 X（维度N×p），y = {-1,1}</li>
<li><p>SVM 三宝：间隔，对偶，核技巧</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM1.png" alt="SVM1" style="zoom:50%;"></p></li>
<li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）</li>
<li><p><strong>hard-margin SVM</strong>（硬间隔）</p>
<ul>
<li>最大间隔分类器 <span class="math inline">\(= max \ margin(w, b),s.t. y_i(w^T x_i+b) &gt; 0,for i = 1,...,N\)</span></li>
<li>点到直线距离，垂直线最短</li>
<li><span class="math inline">\(margin(w, b) = min \ distance(w, b, x_i) = min \ \frac{1}{||w||} |w^T x_i + b|\)</span></li>
<li><span class="math inline">\(→ max_{w,b} \ min_x \frac{1}{||w||} |w^T x_i + b|,(s.t. y_i(w^T x_i+b) &gt; 0)=max_{w,b} \frac{1}{||w||} min_x y_i(w^T x_i + b)\)</span> (存在<span class="math inline">\(r&gt;0\)</span>，使<span class="math inline">\(y_i(w^T x_i + b) =r\)</span>，可以设置<span class="math inline">\(r=1\)</span>)</li>
<li><span class="math inline">\(→ max \ \frac{1}{||w||} ,s.t. min \ y_i(w^T x_i + b) = 1 → min \ \frac{1}{2} w^T w , s.t. y_i(w^T x_i + b)\geqslant 1\)</span>(convex optimization 二次凸优化问题)(primal problem原问题)</li>
<li>拉格朗日：<span class="math inline">\(L(w, b, λ) = \frac{1}{2} w^T w + \sum λ_i(1-y_i(w^T x_i + b)) (λ_i \geqslant 0, (1-y_i(w^T x_i + b)) \leqslant 0\)</span>；若<span class="math inline">\((1-y_i(w^T x_i + b))&gt;0\)</span>，L为正无穷，无解；仅在 <span class="math inline">\(λ_i=0， (1-y_i(w^T x_i + b)) =0\)</span>，达到最大L)</li>
<li><strong>primal problem 原问题</strong>&lt;=&gt; <span class="math inline">\(min_{w,b} \ max_λ \ L(w, b, λ),s.t. λ_i \geqslant 0\)</span> （对w，b没有限制）<span class="math inline">\(→ min \ \frac{1}{2} w^T w\)</span></li>
<li><p><strong>dual problem 对偶问题</strong>：<span class="math inline">\(max_λ \ min_{w,b} \ L(w, b, λ),s.t. λ_i \geqslant 0\)</span></p>
<ul>
<li>min max L &gt;= max min L 弱对偶关系（鸡头凤尾），若直接相等，则为强对偶关系</li>
<li>（若L问题是二次凸优化问题，则min max L = max min L为强对偶关系）</li>
<li><span class="math inline">\(\frac{dL}{db} = \frac{d[\sum λ_i(1-y_i(w^T x_i + b))]}{db} = \frac{d[- Σ λ_i y_i b]}{db} = -\sum λ_i y_i =0\)</span>，带入原式<span class="math inline">\(L = \frac{1}{2} w^T w + \sum λ_i - \sum λ_i y_i w^T x_i\)</span></li>
<li><span class="math inline">\(\frac{dL}{dw} = w - \sum λ_i y_i x_i = 0 → w = \sum λ_i y_i x_i\)</span>，带入原式<span class="math inline">\(L=\frac{1}{2} w^T w + \sum λ_i - w^T w = \sum λ_i - \frac{1}{2}w^T w\)</span></li>
<li><font color="red"><span class="math inline">\(→ min \ \frac{1}{2} w^T w - \sum λ_i,s.t. λ_i\geqslant0, \sum λ_i y_i =0\)</span></font> （此处不能求λ偏导）</li>
<li><span class="math inline">\(→ min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i,s.t. λ_i\geqslant 0, \sum λ_i y_i =0\)</span></li>
<li><strong>KKT条件</strong>：参数偏导为0（<span class="math inline">\(\frac{dL}{db}=0，\frac{dL}{dw}=0，\frac{dL}{dλ}=0\)</span>），<span class="math inline">\(λ_i(1-y_i(w^T x_i + b))=0， λ_i\geqslant0，(1-y_i(w^T x_i + b)\leqslant0\)</span></li>
<li>原问题和对偶问题具有强对偶关系&lt;=&gt;满足KKT条件</li>
<li><span class="math inline">\(\hat{w} = \sum λ_i y_i x_i\)</span>, （存在<span class="math inline">\(x_k,y_k,1-y_k(w^T x_k + b)=0\)</span>）<span class="math inline">\(\hat{b} = y_k - w^T x_k = y_k - (\sum λ_i y_i x_i^T) x_k\)</span></li>
</ul></li>
<li><p><span class="math inline">\(f(x) = sign(\hat{w}^T x + \hat{b})\)</span></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM2.png" alt="SVM2" style="zoom:50%;"></p>
<ul>
<li>落在虚线的样本点就是<span class="math inline">\(x_k（y_k(w^T x_k + b)=1\)</span>），就称为support vector支持向量，只有支持向量对求解有意义，其他的样本点对应的λ均为0</li>
</ul></li>
</ul></li>
<li><strong>soft-margin SVM</strong>（软间隔）
<ul>
<li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li>
<li>soft-margin SVM在hard-margin SVM基础上允许一点点错误，$min   w^T w + loss $
<ul>
<li>分错点的个数：<span class="math inline">\(loss = \sum I\{y_i(w^T x_i + b)&lt;1\}\)</span> （关于w是不连续的，无法求导，因此不采取）</li>
<li><p>hinge 距离：<span class="math inline">\(hinge \ loss = max \{0, 1-y_i(w^T x_i + b)\}\)</span></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM3.png" alt="SVM3" style="zoom: 67%;"></p></li>
<li><span class="math inline">\(min \ \frac{1}{2} w^T w + C \sum max \{0, 1-y_i(w^T x_i + b)\},s.t. y_i(w^T x_i + b)\geqslant 1\)</span> （超参数C）</li>
<li><p>→ 设定<span class="math inline">\(ξ_i = y_i(w^T x_i + b)，min \ \frac{1}{2} w^T w + C \sum ξ_i,s.t. y_i(w^T x_i + b)\geqslant (1-ξ_i), ξ_i\geqslant 0\)</span>（同样用对偶问题方式来求解）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM4.png" alt="SVM4" style="zoom: 50%;"></p></li>
</ul></li>
</ul></li>
<li>约束优化问题
<ul>
<li>primal problem 原问题：<span class="math inline">\(min \ f(x),s.t. m_i(x) \leqslant 0, n_j(x)=0 (i=1,...,M, j=1,...,N)\)</span></li>
<li>原问题的无约束形式（关于x的函数）：拉格朗日：<font color="red"><span class="math inline">\(L(x, λ, η) = f(x) + \sum λ_i m_i(x) + \sum η_i n_i(x) → min_x \ max_{λ,η} \ L(x, λ, η),s.t. λ_i\geqslant 0\)</span></font></li>
<li>证明两者等价：如果违法约束<span class="math inline">\(m_i(x)&gt;0，max_λ \ L → ∞\)</span>；反之，<span class="math inline">\(max_λ \ L\)</span> 必有最大值（<span class="math inline">\(λ_i=0\)</span>时）（即排除了<span class="math inline">\(m_i(x)&gt;0\)</span>情况，过滤掉违反约束的情况）</li>
<li>dual problem 对偶问题（关于λ,η的函数）：<font color="red"><span class="math inline">\(max_{λ,η} \ min_x \ L(x, λ, η),s.t. λ_i\geqslant 0\)</span></font>
<ul>
<li><strong>弱对偶性：对偶问题&lt;=原问题</strong> （<span class="math inline">\(max_{λ,η} \ min_x \ L(x, λ, η) \leqslant min_x \ max_{λ,η} \ L(x, λ, η)\)</span>）</li>
<li>证明：<span class="math inline">\(min_x \ L \leqslant L \leqslant max_{λ,η} \ L → A(λ,η) \leqslant L \leqslant B(x) → A(λ,η) \leqslant B(x) → max \ A(λ,η) \leqslant min \ B(x)\)</span></li>
<li><span class="math inline">\(→ max_{λ,η} \ min_x \ L(x, λ, η) \leqslant min_x \ max_{λ,η} \ L(x, λ, η)\)</span></li>
<li>（在<span class="math inline">\(min_x \ L\)</span>已经确定x，则只剩下关于 λ,η的函数A，函数B同理）</li>
<li>强对偶性：对偶问题=原问题</li>
</ul></li>
<li>几何解释
<ul>
<li>primal problem: <span class="math inline">\(min \ f(x),s.t. m_1(x)\leqslant 0\)</span> (D定义域，D=dom<sub>f</sub> ∩ dom<sub>m1</sub>)， <font color="red">原问题最优解：<span class="math inline">\(p^* = min f(x)\)</span></font></li>
<li><span class="math inline">\(L(x, λ) = f(x) + λ m_1(x),s.t. λ\geqslant 0\)</span> <font color="red">对偶最优解：<span class="math inline">\(d^* = max_λ \ min_x \ L(x, λ)\)</span></font></li>
<li>将问题投影入二维空间：引入集合(区域) <span class="math inline">\(G = \{(m_1(x), f(x))|x∈D\}\)</span>
<ul>
<li>不知道G是凸还是非凸，非凸具有一般性，则画个非凸的图像</li>
<li>凸集是指集合内任意两点的连线都在集合内</li>
<li>凸优化问题是指x是闭合的凸集且f是x上的凸函数的最优化问题，这两个条件任一不满足则该问题即为非凸的最优化问题</li>
<li>目标函数f如果不是凸函数，则不是凸优化问题</li>
<li>决策变量x中包含离散变量（0-1变量或整数变量），则不是凸优化问题</li>
<li>如果其二阶导数在区间上非负，就称为凸函数；如果其二阶导数在区间上恒大于0，就称为严格凸函数</li>
<li>结论：凸函数的局部最优解就是全局最优解</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM5.png" alt="SVM5" style="zoom: 67%;"></p>
<ul>
<li><p><span class="math inline">\(p^* = inf \{f(x)|(m_1(x), f(x))∈G, m_1(x) \leqslant 0\}\)</span>（集合中没有最小值概念，对应的是下确界）</p>
<ul>
<li><span class="math inline">\(P*\)</span> 对应图中蓝色部分（左半边区域对纵轴的映射），下确界则为左半边区域最低点在纵轴的映射</li>
</ul></li>
<li><span class="math inline">\(d^* = max_λ \ g(λ) , g(λ) = min_x \  f(x) + λ m_1(x) , g(λ) = inf \{f(x) + λ_i m_1(x)|(m_1(x), f(x))∈G\}\)</span>
<ul>
<li>一条过原点直线 <span class="math inline">\(f(x)+λm_1(x)= 0\)</span>(斜率λ可变)，g(λ)范围可以从直线开始与G相切到离开G相切的地方（红线范围）<span class="math inline">\(g(λ)\leqslant p^*\)</span></li>
<li>当调整斜率<span class="math inline">\(λ^*\)</span>，得到一个 <span class="math inline">\(g(λ^*) = f(x) + λ^* m_1(x)\)</span> 同时与G的俩角相切，此时，直线与纵轴的交点为<span class="math inline">\(d^*\)</span>（绿线）</li>
<li><p><span class="math inline">\(d^* \leqslant p^*\)</span> （凸优化+slater条件 → <span class="math inline">\(d^* = p^*\)</span>）（SVM是二次规划问题，符合slater条件）</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM6.png" alt="SVM6" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li>slater条件
<ul>
<li>Convex + Slater → Strong Duality （充分不必要条件）</li>
<li>定义：存在<span class="math inline">\(\hat{x}\)</span>在relint，使<span class="math inline">\(m_i(x)&lt;0 (i=1,...,M)\)</span>
<ul>
<li>relative interior（relint）：在一个有边界的区域，relint对应其无边界的内部区域</li>
<li>仿射函数即由由1阶多项式构成的函数，一般形式为 <span class="math inline">\(f (x) = Ax + b\)</span>（A 是一个 m×k 矩阵，反映了一种从 k 维到 m 维的空间映射关系，称f是仿射函数；A、x、b都是标量且b=0，f才是线性函数）</li>
</ul></li>
<li>对于大多数凸优化，slater是成立的（存在一些凸优化问题是不符合slater条件，没有强对偶关系的）</li>
<li>放松的slater条件：在<span class="math inline">\(m_i(x)\)</span>中，若M中有k个仿射函数，则仅需校验剩余M-k个是否满足<span class="math inline">\(m_i(x)&lt;0\)</span> （凸二次规划问题：目标函数f是凸的，不等式约束<span class="math inline">\(m_i\)</span>是仿射函数，等式约束<span class="math inline">\(n_j\)</span>也是仿射函数；所以凸二次规划问题符合放松的slater条件，SVM属于凸二次规划问题，则可以直接使用KKT条件求解）</li>
</ul></li>
<li>KKT条件
<ul>
<li>KKT &lt;=&gt; Strong Duality (<span class="math inline">\(d^* = p^*\)</span>)（充要条件）</li>
<li>从<span class="math inline">\(p^*\)</span>得到最优<span class="math inline">\(x^*\)</span>，从<span class="math inline">\(d^*\)</span>得到<span class="math inline">\(λ^*\)</span>、<span class="math inline">\(η^*\)</span></li>
<li>可行域（可行条件）：<span class="math inline">\(m_i(x^*) \leqslant 0, n_j(x^*)=0, λ^*\geqslant 0\)</span></li>
<li>互补松弛
<ul>
<li><span class="math inline">\(d^* = max_{λ,η} \ g(λ,η) = g(λ^*, η^*) = min_x \ L(x, λ^*, η^*) \leqslant L(x^*, λ^*, η^*) \\ = f(x^*) + \sum λ_im_i(x^*) + \sum η_in_i(x^*) = f(x^*) + \sum λ_im_i(x^*) \leqslant f(x^*) = p^*\)</span></li>
<li>（<span class="math inline">\(λ_i\geqslant 0，m_i \leqslant 0，则 λ_i m_i \leqslant 0\)</span>）</li>
<li><font color="red">互补松弛条件 ：$λ_i m_i(x^<em>) = 0 → λ_i<sup>* m_i(x</sup></em>) $</font></li>
</ul></li>
<li>梯度为0
<ul>
<li><span class="math inline">\(min_x \ L(x, λ^*, η^*) &lt;= L(x^*, λ^*, η^*)\)</span></li>
<li><span class="math inline">\(x^*\)</span>是对应x最小值，则 <span class="math inline">\(\frac{dL}{dx} = 0\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>kernel SVM</strong>
<ul>
<li>Kernel Method（思想角度）</li>
<li>Kernel Trick（计算角度）</li>
<li><p>Kernel function</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM9.png" alt="SVM9"></p>
<ul>
<li><strong>非线性带来高维转换（从模型角度）</strong>
<ul>
<li>PLA (Perceptron Learning Algorithm)通过初始化不同w、b，求得不同超平面；Hard-Margin SVM找到最好的超平面</li>
<li>但对数据而言是往往是包含噪声，因此需要对严格线性可分的条件放松，允许放一点点错误，获得更好的范化性能（如左图）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM7.png" alt="SVM7" style="zoom: 50%;"></p>
<ul>
<li>但面对右图的情况，非线性可分问题，即使允许放一点点错误，也是无法分类的。</li>
<li>对于PLA，则有多层感知机（神经网络）→深度学习 （多一层感知机，就可以更逼近一个连续函数，则可以解决非线性问题）<br>
</li>
<li><font color="red">非线性可分问题 → Φ(x) 非线性转换到高维空间 → 线性可分问题</font></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/SVM/SVM8.png" alt="SVM8" style="zoom: 67%;"></p>
<ul>
<li>面对典型异或问题，PLA是无法解决该问题（深度学习可以），将二维空间转换为三维空间，即可用红色超平面划分（Cover Theorem：高维空间比低维更易线性可分）</li>
<li>三种方法转高维：1、类似MLP直接转高维；2、Kernel方法转高维；3、深度学习运用与或非构建有向无环图（神经网络）（与或非（三种基础运算均可用PLA表示）解决异或问题（复合运算）），神经网络：复合表达式、复合函数、MLP（FeedForward Neural Network）
<ul>
<li><p>XOR：x<sub>1</sub>⊕x<sub>2</sub> = (¬x<sub>1</sub>∧x<sub>2</sub>)∨(x<sub>1</sub>∧¬x<sub>2</sub>)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/FNN/FNN1.png" alt="FNN1" style="zoom: 67%;"></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>对偶表示带来内积（从优化角度）</strong>
<ul>
<li>从频率视角归化到优化问题</li>
<li>Hard-Margin SVM 将最大间隔分类思想，转换为凸优化问题，通过拉格朗日的对偶性简化原问题为对偶问题
<ul>
<li>Doul Problem: <span class="math inline">\(min \ \frac{1}{2} \sum_i \sum_j λ_i λ_j y_i y_j x_i^T x_j - \sum_i λ_i s.t. λ_i \geqslant 0, \sum_i λ_i y_i =0\)</span></li>
<li>内积：<span class="math inline">\(x_i^T x_j\)</span></li>
<li>非线性转换：<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span> （高维空间的内积形式）（现实数据很复杂，并且Φ(x)可以是无限维，因此很<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span>难i求解和计算量很大）</li>
<li>Kernel Trick: <strong>Kernel function的引入，就是为了解决计算问题，直接得到<span class="math inline">\(Φ(x_i)^T Φ(x_j)\)</span> 结果</strong>（不需要先求Φ(x)再求内积）</li>
</ul></li>
<li><strong>Kernel function : <span class="math inline">\(K(x, x&#39;) = Φ(x)^T Φ(x&#39;) = \ &lt;Φ(x), Φ(x&#39;)&gt;\)</span></strong>
<ul>
<li>存在<span class="math inline">\(x, x&#39;∈X\)</span>，使<span class="math inline">\(K(x, x&#39;) = Φ(x)^T Φ(x&#39;)\)</span>，则K就是一个核函数（如<span class="math inline">\(K(x, x&#39;)=e^{\frac{-(x-x&#39;)^2}{2σ^2}}\)</span>）</li>
<li>蕴含了：非线性转换+内积</li>
</ul></li>
</ul></li>
<li>一般核函数指<strong>正定核函数</strong> <a href="https://www.bilibili.com/video/BV1aE411o7qd?p=37" target="_blank" rel="noopener">详解</a>
<ul>
<li>更精确定义：K可以将任意输入空间X映射到高维空间，则K(x, x')为核函数</li>
<li>正定核函数：K可以将任意输入空间X映射到高维空间，有K(x, x')，存在Φ（Φ∈Hilbert Space）可以输入空间X映射到高维空间，且使<span class="math inline">\(K(x, x&#39;) = \ &lt;Φ(x), Φ(x&#39;)&gt;\)</span>，则K(x, x')为正定核函数</li>
<li>正定核函数（另一个定义）：K可以将任意输入空间X映射到高维空间，有K(x, x')，若满足两个条件（对称性、正定性）则为正定和函数
<ul>
<li>对称性：<span class="math inline">\(K(x, x&#39;) = K(x&#39;, x)\)</span></li>
<li>正定性：任取N个元素，x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>∈X，对应的Gram矩阵是半正定的（K=[K(x<sub>i</sub>, x<sub>j</sub>)]）（两个定义等价，即证明：<strong>K(x, x') = &lt;Φ(x), Φ(x')&gt; &lt;=&gt; Gram matrix 半正定且对称</strong>）</li>
<li>Hilbert Space: 完备的、可能是无限维的、被赋予内积的，线性空间（向量空间，满足加法和数乘等条件）（完备是对极限是封闭的，即无论如何操作，仍然属于该空间内）（内积：对称性（&lt;f, g&gt; = &lt;g, f&gt;）、正定性（内积大于等于0，&lt;f, f&gt; &gt;= 0）、线性性（&lt;r<sub>1</sub> f<sub>1</sub> + r<sub>2</sub> f<sub>2</sub> , g&gt; = r<sub>1</sub> &lt;f<sub>1</sub>, g&gt; + r<sub>2</sub> &lt;f<sub>2</sub>, g&gt;））<br>
</li>
</ul></li>
<li>必要性证明
<ul>
<li>在Hilbert Space中的Φ(x)具有对称性性质，<span class="math inline">\(K(x, x&#39;) = &lt;Φ(x), Φ(x&#39;)&gt; = &lt;Φ(x&#39;), Φ(x)&gt; = K(x&#39;, x)\)</span></li>
<li>K=[K(x<sub>i</sub>, x<sub>j</sub>)]（维度N×N）（半正定：任意a列向量，<span class="math inline">\(a^T K a \geqslant 0\)</span>）</li>
<li><span class="math inline">\(a^T K a = \sum_i \sum_j a_i a_j K_{ij} = \sum_i \sum_j a_i a_j K(x_i, x_j) = \sum_i \sum_j a_i a_j &lt;Φ(x_i), Φ(x_j)&gt; → 线性性\\ → \sum_i \sum_j a_i a_j Φ(x_i)^T Φ(x_j) = \sum_i a_i Φ(x_i)^T \sum_j a_j Φ(x_j) = [\sum_i a_i Φ(x_i)]^T \sum_j a_j Φ(x_j) \\ = \ &lt;\sum_i a_i Φ(x_i), \sum_j a_j Φ(x_j)&gt; \ = ||\sum_i a_i Φ(x_i)||^2 &gt;= 0，半正定性\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/SVM/" data-id="ckhlr6dig000s0jeg8tpjrqrl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Decision-Tree" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/02/Decision-Tree/" class="article-date">
  <time datetime="2020-10-02T08:49:49.000Z" itemprop="datePublished">2020-10-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/02/Decision-Tree/">Decision_Tree</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>基于数据特征构造决策树
<ul>
<li>有向边</li>
<li>结点
<ul>
<li>内部结点(internal node)-&gt;表示特征</li>
<li><p>叶子结点(leaf node)-&gt;表示类别</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT1.png" alt="DT1" style="zoom:67%;"></p></li>
<li>从根结点开始，对实例的某一特征进行取得阈值，从而划分，再递归根据后续的特征，再取值划分，直至到叶子结点，完成分类</li>
</ul></li>
<li>决策树表示给定特征条件下类的条件概率分布。
<ul>
<li>一个条概率分布定义特征空间的一个划分上</li>
<li>将特征空间划分为互不相交的单元cell，每个单元定义一个类的概率分布就构成了一个条件概率分布，则一条路径对应一个单元，构成<strong>叶子结点基于其父结点的条件概率</strong></li>
</ul></li>
<li>决策树能对训练数据有很好的分类，但是会造成过拟合现象，则需要剪枝，增加其泛化性，才能在测试数据达到更好效果 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">详解</a></li>
</ul></li>
<li>决策树学习过程：特征选择、决策树生成、剪枝
<ul>
<li><strong>ID3算法</strong>（分类、多叉树）
<ul>
<li>特征选择（在某个特征下，根据信息增益来判断数据是否更好的分类）
<ul>
<li>Information Gain 信息增益。信息增益越大对应的特征越重要</li>
<li>Entropy 熵，表示随机变量不确定的度量</li>
<li><p>D表示数据集，A表示特征，Ck为第k个类别（共K类），pi为概率，H(D)表示熵，H(D|A)表示条件熵（A特征将D划分为n个子集Di），gain(D,A)表示当前特征A的信息增益（细节推导见统计学方法，第二版，75页）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/statistics/DT2.png" alt="DT2"><figcaption>DT2</figcaption>
</figure></li>
</ul></li>
<li>生成：选择对应最大信息增益的特征，再根据该特征将数据划分成两个子集，再其中未分好的子集中再次递归选择最大信息增益的特征</li>
<li>缺点：由于信息增益会导致偏向于选择取值较多的特征、没有考虑连续特征、没考虑缺失值</li>
</ul></li>
<li><strong>C4.5算法</strong>（分类、多叉树）
<ul>
<li>特征选择
<ul>
<li><p>Information Gain Ratio 信息增益比=信息增益 / 特征熵</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT3.png" alt="DT3" style="zoom: 67%;"></p></li>
</ul></li>
<li>生成：与ID3算法类似</li>
<li>缺点：基于信息论的熵模型的，这里面会涉及大量的对数运算</li>
<li>二叉树模型会比多叉树运算效率高</li>
<li>无剪枝</li>
</ul></li>
<li><strong>CART</strong> classification and regression tree（分类、回归、二叉树）
<ul>
<li>分类
<ul>
<li>特征选择
<ul>
<li>Gini基尼指数</li>
<li>基尼指数Gini(D)表示集合D的不确定性，Gini(D, A)表示基于特征A 划分后D的不确定性</li>
<li>基尼指数越大，样本集合不确定性也越大（基尼指数和熵都可以近似表示分类误差率）</li>
</ul>
<figure>
<img src="https://github.com/soloistben/images/raw/master/statistics/DT4.png" alt="DT4"><figcaption>DT4</figcaption>
</figure></li>
<li>生成
<ul>
<li>根据计算现有特征对样本集合D的基尼指数，每次迭代均选择最小基尼指数对应的特征作为最优切分点</li>
<li>生成决策树之后，根据底端开始不短剪枝，直至根结点，形成子树</li>
</ul></li>
<li><p>损失函数</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT5.png" alt="DT5" style="zoom:75%;"></p>
<ul>
<li>T为任意子树，C(T)为对训练数据的预测误差（基尼指数），|T|为子树叶子结点个数，a为大于0的参数，Ca(T)表示了整体的损失</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/DT6.png" alt="DT6" style="zoom:75%;"></p></li>
</ul></li>
<li>回归</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/10/02/Decision-Tree/" data-id="ckhlr6dh900050jegakm2kc5t" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Statistics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/11/Statistics/" class="article-date">
  <time datetime="2020-09-11T08:34:56.000Z" itemprop="datePublished">2020-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/11/Statistics/">Statistics</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="statistics-for-machine-learning">Statistics for Machine Learning</h4>
<h5 id="one.-两大派系">One. 两大派系</h5>
<ul>
<li><strong>频率派：统计机器学习</strong>（model (<span class="math inline">\(f(x)=w^T x+b\)</span>), strategy (loss function), algorithm (GD, SGD, 牛顿法、拟牛顿法)，本质为<strong>优化问题</strong>）
<ul>
<li>正则化（L1,L2）</li>
<li>核化（Kernel SVM）</li>
<li>集成化（AdaBoost，RandForest）</li>
<li>层次化（Neural Network：MLP(Multi-Layer Perceptron)，Autoencoder，CNN，RNN）(统称 Deep Neural Network)</li>
</ul></li>
<li><strong>贝叶斯派：概率图模型</strong>（本质为：通过Inference求（后验概率）<strong>积分问题</strong>(Monte Carlo method, MCMC)；直接求解过于复杂，则衍生出概率图模型））
<ul>
<li>有向图：Bayesian Network (Deep Directed Network)
<ul>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder (VAE)</li>
<li>GAN</li>
</ul></li>
<li>无向图：Markov Network (Deep Boltzmann Network )</li>
<li>混合模型（有向+无向）：Mixed Network (Deep Belief Network)</li>
<li>统称 Deep Generative Model（但深层很难计算）</li>
<li><strong>Deep Learning = Deep Generative Model + Deep Neural Network</strong></li>
</ul></li>
<li>Data
<ul>
<li>X: data, X={x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>}<sup>T</sup> Dimension(N, P)</li>
<li>θ: parameter, X~p(X|θ)</li>
</ul></li>
<li><strong>频率派</strong>
<ul>
<li>θ为未知常数；X为随机变量</li>
<li><span class="math inline">\(loss = P(X|θ) = \prod_{i=1}^n P(x_i|θ)\)</span>
<ul>
<li>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>之间独立同分布</li>
</ul></li>
<li>Maximum Likelihood Estimation 极大似然估计
<ul>
<li><span class="math inline">\(θ_{MLE}= argmax_θ \ log^{P(X|θ)}\)</span></li>
</ul></li>
</ul></li>
<li><strong>贝叶斯派</strong>
<ul>
<li>θ为随机变量，服从概率分布θ~P(θ)，即prior probability 先验概率；X为随机变量</li>
<li>posterior probability 后验概率
<ul>
<li><span class="math inline">\(P(θ|X) = \frac{P(X, θ)}{P(X)} = \frac{P(X|θ)P(θ)}{P(X)} = \frac{likelihood*prior}{\int_θ P(X|θ) d_θ}\)</span></li>
</ul></li>
<li>Maximum A Posterior Probability 最大后验概率
<ul>
<li>找到θ在分布中最大值点</li>
<li><span class="math inline">\(θ_{MLE} = argmax_θ \ P(X|θ) = argmax_θ \ P(X|θ)P(θ)\)</span></li>
<li>P(θ|X)其分母是不变的，则θ<sub>MLE</sub>与分子成正比，只需要算分子</li>
</ul></li>
<li>贝叶斯估计
<ul>
<li><span class="math inline">\(P(θ|X) = \frac{P(X|θ)P(θ)}{\int_θ P(X|θ) d_θ}\)</span></li>
<li>必须完整计算整个分子式</li>
</ul></li>
<li>贝叶斯预测
<ul>
<li><span class="math inline">\(X (train), \hat{X} (test), X → θ → \hat{X}\)</span></li>
<li>训练数据通过学习参数θ，与测试数据关联</li>
<li><span class="math inline">\(P(\hat{X}|X) = \int_θ P(\hat{X}, θ|X) d_θ = \int_θ P(\hat{X}|X)P(θ|X) d_θ\)</span></li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-linear-regression">Two. Linear Regression</h5>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR1.png" alt="LR1" style="zoom: 50%;"></p>
<p>数据定义：N个p维样本，即X维度(N, p) （N &gt; p，样本之间独立同分布）；真实值Y维度(N,1)；直线<span class="math inline">\(f(w) = w^T x + b\)</span>（偏置b可先忽略）</p>
<ul>
<li>特点（<strong>现有模型都是基于下面特点，打破一个或者多个</strong>）
<ul>
<li>线性（属性线性、全局线性、系数线性）
<ul>
<li>属性非线性：特征转换（多项式回归）</li>
<li>全局非线性：线性分类（激活函数是非线性，激活函数带来了分类效果）</li>
<li>系数非线性：神经网络（感知机）</li>
</ul></li>
<li>全局性
<ul>
<li>局部性：线性样条回归（每段都拆分为单独回归模型），决策树</li>
</ul></li>
<li>数据未加工
<ul>
<li>预处理：PCA，流行</li>
</ul></li>
</ul></li>
<li><strong>矩阵表达</strong>
<ul>
<li>Least Squares 最小二乘估计法（最小平方法）
<ul>
<li><span class="math inline">\(\mathbf{L(w) = \sum ||w^T x_i - y_i||^2} = \sum (w^T x_i - y_i)^2 \\ = (w^T X^T - Y^T) (Xw - Y) = w^T X^T X w - 2 w^T X^T Y + Y^T Y\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathbf{\hat{w} = argmin \ L(w)}\)</span></li>
<li>求导$  = 2 X^T X w - 2 X^T Y=0 → X^T X w = X^T Y$
<ul>
<li><span class="math inline">\(\hat{w} = (X^T X)^{-1} X^T Y\)</span> (伪逆：<span class="math inline">\((X^T X)^{-1} X^T\)</span>)</li>
</ul></li>
<li><span class="math inline">\(x_3\)</span>的误差为<span class="math inline">\(w^T x_3 - y_3\)</span>，即所有误差分成一小段一小段</li>
</ul></li>
<li><p><strong>几何意义</strong></p>
<ul>
<li><span class="math inline">\(f(w) = w^T x \Leftrightarrow f(β) = x^T β\)</span></li>
<li>可以将数据X看成p维的空间，Y是不在该p维空间内</li>
<li>目标：在p维空间中找到一条直线<span class="math inline">\(f(β)\)</span>离Y最近，即Y在p维空间的投影
<ul>
<li><p>若向量a与向量b垂直，则 <span class="math inline">\(a^T b = 0\)</span></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Regression/LR2.png" alt="LR2" style="zoom: 50%;"></p></li>
<li>虚线为<span class="math inline">\(Y - Xβ\)</span> 与<span class="math inline">\(X\)</span>的p维空间垂直，<span class="math inline">\(X^T (Y-Xβ) = 0 → β = (X^T X)^{-1} X^T Y\)</span></li>
<li>误差分散在p个维度上</li>
</ul></li>
</ul></li>
<li><p><strong>概率角度</strong></p>
<ul>
<li>最小二乘法 &lt;=&gt; 噪声为高斯分布的极大似然估计法（MLE with Gaussian noise）</li>
<li>数据本身会带有噪声 ε~N(0, σ<sup>2</sup>)</li>
<li><span class="math inline">\(y = f(w) + ε = w^T x + ε\)</span>
<ul>
<li><span class="math inline">\(y|x,w\)</span> ~<span class="math inline">\(N(w^T x, σ^2) \Leftrightarrow P(y|x,w) =\frac{1}{\sqrt{2\pi}σ} e^{-\frac{(y - w^T x)^2}{2*σ^2}}\)</span></li>
</ul></li>
<li>定义log-likelihood：
<ul>
<li><span class="math inline">\(L_{MLE} (w) = log^{P(y|x,w)} = log^{\prod P(y_i|x_i,w)} = \sum log^{P(y_i|x_i,w)} \\ = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} + log^{e^{-\frac{(y - w^T x)^2}{2σ^2}}}] = \sum [log^{\frac{1}{\sqrt{2\pi}σ}} -\frac{(y - w^T x)^2}{2σ^2}]\)</span></li>
<li>样本之间独立同分布</li>
<li><span class="math inline">\(\hat{w} = argmax \ L_{MLE} (w) = argmax \ \frac{(y - w^T x)^2}{2σ^2} = argmin \ (y_i - w^T x_i)^2\)</span></li>
<li>则与最小二乘法定义一样 (<strong>LSE &lt;=&gt; MLE with Gaussian noise</strong>)</li>
</ul></li>
</ul></li>
<li><p><strong>Regularization 正则化</strong></p>
<ul>
<li>若样本没有那么多，X维度(N, p)的N没有远大于p，则求w~中的(X^T X)往往不可逆，（p过大，有无数种结果）会引起<strong>过拟合</strong>
<ul>
<li>最直接是加样本数据</li>
<li>降维or特征选择or特征提取 (PCA)</li>
<li>正则化（损失函数加个约束）：<span class="math inline">\(argmin [L(w)+λP(w)]\)</span></li>
</ul></li>
<li>L1 -&gt; Lasso
<ul>
<li><span class="math inline">\(P(w) = ||w||_1\)</span></li>
</ul></li>
<li>L2 -&gt; Ridge 岭回归
<ul>
<li><span class="math inline">\(P(w) = ||w||_2 = w^T w\)</span></li>
<li>权值衰减</li>
<li><span class="math inline">\(J(w) = \sum||w^T x_i - y_i||^2 + λ w^T w = w^T X^T X w - 2 w^T X^T Y + Y^T Y + λ w^T w \\ = w^T(X^T X + λ I) w - 2 w^T X^T Y + Y^T Y\)</span>
<ul>
<li><span class="math inline">\(\hat{w} = argmin \ J(w)\)</span></li>
<li><span class="math inline">\(\frac{dJ}{dw} = 2 (X^T X + λ I)w - 2 X^T Y = 0, \hat{w} = (X^T X + λ I)^{-1} X^T Y\)</span></li>
<li><span class="math inline">\(X^T X\)</span> 是半正定矩阵+对角矩阵=<span class="math inline">\((X^T X + λI)\)</span>正定矩阵，必然<strong>可逆</strong></li>
</ul></li>
</ul></li>
<li>贝叶斯的角度
<ul>
<li>参数w服从分布，<span class="math inline">\(w\)</span>~<span class="math inline">\(N(0,σ\_0^2) → P(w) = \frac{1}{\sqrt{2\pi}σ_0} e^{-\frac{||w||^2}{2σ_0^2}}\)</span></li>
<li><span class="math inline">\(P(w|y) = \frac{P(y|w)P(w)}{P(y)}\)</span></li>
<li>MAP: <span class="math inline">\(\hat{w} = argmax \ P(w|y) = argmax \ P(y|w)P(w) = argmax \ log^{P(y|w)P(w)} \\ = argmax \ log^{\frac{1}{2\piσ_0σ} e^{(-\frac{(y_i - w^T x_i)^2}{2σ^2} -\frac{||w||^2}{2σ_0^2})}} = argmin [(y_i - w^T x_i)^2 + \frac{σ^2}{σ_0^2}||w||^2] = argmin [L(w)+λP(w)]\)</span></li>
<li><span class="math inline">\(λ = \frac{σ^2}{σ_0^2}\)</span></li>
<li><strong>Regularized LSE &lt;=&gt; MAP with Gaussian noise and Gaussian prior</strong></li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-linear-classification">Three. Linear Classification</h5>
<ul>
<li><font color="red">线性回归------&gt;激活函数，降维-------&gt;线性分类</font></li>
<li>硬分类：0/1
<ul>
<li>线性判别分析 (Fisher)</li>
<li>感知机</li>
</ul></li>
<li>软分类：[0,1]区间内概率
<ul>
<li>生成式模型：Gaussian Discriminant Analysis, Naive Bayes, Markov（转换用贝叶斯求解）</li>
<li>判别式模型：Logisitic Regression, KNN, Perceptron, Decision Tree, SVM, CRF, （直接学习P(Y|X)，用MLE学习参数）</li>
</ul></li>
<li><p><strong>Perceptron 感知机</strong> (1958年)</p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC1.png" alt="LC1" style="zoom: 67%;"></p>
<ul>
<li>判别模型</li>
<li>样本：{(x<sub>i</sub>, y<sub>i</sub>)}, N个</li>
<li>思想：错误驱动（先初始化w，检查分错的样本，前提是线性可分）（感知错误，纠正错误）</li>
<li>模型：<span class="math inline">\(f(x) = sign(w^T x + b)\)</span> （<span class="math inline">\(w^T x\)</span>大于等于0表示为1（分类正确），反之为-1（分类错误））</li>
<li>策略：loss function（被错误分类的样本个数）
<ul>
<li><span class="math inline">\(L(w) = \sum I\{y_i * (w^T x_i) &lt; 0\}\)</span> （非连续函数，不可导）</li>
<li><span class="math inline">\(L(w) = \sum-y_i w^T x_i,\frac{dL}{dw} = -y_i x_i\)</span></li>
</ul></li>
<li>若是非线性可分，可是使用pocket algorithm</li>
<li>从感知机到深度学习（发展历程）
<ul>
<li>1958年提出PLA</li>
<li>1969年马文·明斯基（AI之父）提出PLA局限性（无法解决非线性问题）（第1次陷入低谷）</li>
<li>1981年提出MLP（多层感知机），FeedForward Neural Network</li>
<li>1986年BP+MLP，RNN</li>
<li>1989年Universal Approximation Theorem（通用近似定理）：当隐含层大于等于1层时，可以逼近任意连续函数（1 layer is good -&gt; why deep）（当年算力不行）（第2次陷入低谷）</li>
<li>1993～1995年 SVM+kernel+theory = SVM流派，集成化派：AdaBoost，RandForest</li>
<li>2006年Hinton提出Deep Belief Network (基于无向图RBM) 和 Deep AutoEncoder</li>
<li>2009年GPU发展，2011年speech，2012年ImageNet</li>
<li>2013年Variational Autoencoder (VAE)</li>
<li>2014年GAN</li>
<li>2016年Alpha Go</li>
<li>2018年GNN（连接主义+符号主义-&gt;推理功能）</li>
<li>深度学习火的主要原因：效果比传统SVM好（<font color="red">将来会引入SVM和概率图模型进入深度学习形成大融合，实现可解释性</font>）</li>
</ul></li>
</ul></li>
<li><p><strong>线性判别分析</strong></p>
<p><img src="https://github.com/soloistben/images/raw/master/statistics/Linear_Classification/LC2.png" alt="LC2" style="zoom:50%;"></p>
<ul>
<li>样本：N个p维样本，二分类(+1,-1)，正样本个数<span class="math inline">\(N1\)</span>，均值<span class="math inline">\(X_{c1}\)</span>，方差<span class="math inline">\(S_{c1}\)</span>，负样本个数<span class="math inline">\(N_{2}\)</span>，均值<span class="math inline">\(X_{c2}\)</span>，方差<span class="math inline">\(S_{c2}\)</span>，（<span class="math inline">\(X_{c1} = \frac{1}{N_1} \sum x_i,S_{c1} = \frac{1}{N_1} \sum (x_i - X_{c1})(x_i - X_{c1})^T\)</span>）</li>
<li>思想：类内小，类间大
<ul>
<li>将所有样本映射到一个Z平面（模型学习找最优平面），设定阈值，根据类的方差将样本分类</li>
<li>类内样本距离应该更紧凑（高内聚），类间更松散（松耦合）</li>
<li>Z平面的法向量为最后找到的分类函数 <span class="math inline">\(w^T x\)</span>（因为垂直，则Z平面即w向量）
<ul>
<li>（前提设置<span class="math inline">\(||w||=1\)</span>）</li>
<li>则样本点投影到Z平面为：<span class="math inline">\(|x_i|cos(x_i,w) = |x_i||w|cos(x_i,w) =x_i w = w^T x_i\)</span></li>
</ul></li>
</ul></li>
<li>模型：分别求出两类投影在Z平面上的<strong>均值Z_1,Z_2</strong>和<strong>方差S_1,S_2</strong>
<ul>
<li><span class="math inline">\(N_1 = \frac{1}{N_1} \sum w^T x_i\)</span></li>
<li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - Z_1) (w^T x_i - Z_1)^T\)</span></li>
<li>类间：<span class="math inline">\((Z_1-Z_2)^2\)</span>，类内：<span class="math inline">\(S_1+S_2\)</span></li>
</ul></li>
<li>策略：<span class="math inline">\(L(w) = \frac{(Z_1-Z_2)^2}{(S_1+S_2)}= \frac{w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w}{w^T(S_{c1}+ S_{c2}) w }\)</span>
<ul>
<li>分子 =$ [w^T ( x_i -  x_i)]^2= [w^T (X_{c1} - X_{c2})]^2 = w^T (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w$</li>
<li>分母 = <span class="math inline">\(w^T S_{c1} w + w^T S_{c2} w = w^T (S_{c1}+ S_{c2}) w\)</span>
<ul>
<li><span class="math inline">\(S_1 = \frac{1}{N_1} \sum (w^T x_i - \frac{1}{N_1} \sum w^T x_j)(w^T x_i - \frac{1}{N_1} \sum w^T x_j)^T \\ = w^T [\frac{1}{N_1}\sum(x_i - X_{c1})(x_i - X_{c1})^T] w = w^T S_{c1} w\)</span></li>
</ul></li>
<li>定义S_b类内方差（between-class），S_w类间方差（with-class）</li>
<li><span class="math inline">\(L(w) = \frac{w^T S_b w}{w^T S_w w}, \hat{w} = argmax \ L(w)\)</span>
<ul>
<li><span class="math inline">\(\frac{dL}{dw} = 2S_b w (w^T S_w w)^{-1} + (w^T S_b w) * -(w^T S_w w)^{-2} * 2 * S\_w w = 0\)</span></li>
<li><span class="math inline">\(S_b w (w^T S_w w) = (w^T S_b w) S_w w\)</span> （<span class="math inline">\((w^T S_w w)\)</span> 最终计算得一个实数，一维，没有方向）（求解<span class="math inline">\(\hat{w}\)</span>关心的是方向，因为平面的大小可以缩放，所以意义不大）</li>
<li><span class="math inline">\(w = (w^T S_w w)/(w^T S_b w)S_w^{-1}S_b w\)</span>，正比于<span class="math inline">\((S_w^{-1}S_b w)\)</span> ，正比于<span class="math inline">\((S_w^{-1}(X_{c1} - X_{c2}))\)</span></li>
<li>（<span class="math inline">\(S_b w = (X_{c1} - X_{c2})(X_{c1} - X_{c2})^T w，(X_{c1} - X_{c2})^T w\)</span>为实数）</li>
<li>（若<span class="math inline">\(S_w\)</span>是对角矩阵，各向同性，<span class="math inline">\(S_w\)</span>正比于单位矩阵，则<span class="math inline">\(w\)</span>正比于<span class="math inline">\((X_{c1} - X_{c2})\)</span></li>
</ul></li>
</ul></li>
<li><strong>线性判别分析为早期分类方法，有很大局限性，目前不用</strong></li>
</ul></li>
<li><p><strong>Logistic Regression</strong></p>
<ul>
<li><font color="red">线性回归------&gt;sigmoid-------&gt;线性分类</font></li>
<li>判别模型</li>
<li>model
<ul>
<li><span class="math inline">\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</span>，将<span class="math inline">\(w^T x\)</span>映射到处于[0,1]区间的概率值p</li>
<li><span class="math inline">\(p_1 = P(y=1|x) = sigmoid(w^T x) = \frac{1}{1+e^{w^T x}}\)</span></li>
<li><span class="math inline">\(p_0 = P(y=0|x) = sigmoid(w^T x) = \frac{e^{w^T x}}{1+e^{w^T x}}\)</span></li>
<li>综合表达：<span class="math inline">\(P(y|x) = p_1^y * p_0^{1-y}\)</span></li>
</ul></li>
<li><span class="math inline">\(\hat{w} = argmax \ P(Y|X) = argmax \ log^{\prod P(y_i|x_i)} = argmax \ \sum log^{P(y_i|x_i)} \\= argmax \ \sum[y_i*log^{p_1} + (1-y_i)*log^{p_0}] \Leftrightarrow (-cross entropy)\)</span>
<ul>
<li>MLE &lt;=&gt; loss function (min cross entropy)</li>
</ul></li>
</ul></li>
<li><strong>Gaussian Discriminant Analysis</strong>
<ul>
<li>生成模型、连续
<ul>
<li><span class="math inline">\(\hat{y} = argmax \ P(y|x) = argmax \ P(x|y)P(y)\)</span></li>
<li>分类：最终比较<span class="math inline">\(P(y=0|x)，P(y=1|x)\)</span>大小</li>
<li><span class="math inline">\(P(y|x)\)</span>正比于<span class="math inline">\(P(x|y)P(y)\)</span>，即联合概率<span class="math inline">\(P(x, y)\)</span></li>
</ul></li>
<li>Data：N个d维样本，二分类(0,1)，正样本个数<span class="math inline">\(N_1\)</span>，方差<span class="math inline">\(S_1\)</span>，负样本个数<span class="math inline">\(N_2\)</span>，方差<span class="math inline">\(S_2\)</span></li>
<li><strong>prior probability</strong>
<ul>
<li>先验概率服从伯努利分布</li>
<li>y ~ Bernoulli，<span class="math inline">\(P(y=1) = p，P(y=0) = 1-p\)</span></li>
<li><span class="math inline">\(P(y) = p^y(1-p)^{1-y}\)</span></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li>条件概率服从高斯分布（样本足够大时服从高斯分布）</li>
<li>x|y=1 ~ N(u<sub>1</sub>, σ<sup>2</sup>)</li>
<li>x|y=0 ~ N(u<sub>2</sub>, σ<sup>2</sup>)</li>
<li>方差一样（权值共享），均值不一样</li>
<li><span class="math inline">\(P(x|y) = N(u_1, σ^2)^y * N(u_2, σ^2)^{1-y}\)</span></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li><span class="math inline">\(log^{MLE} → L(θ) = log^{\prod P(x_i, y_i)} = \sum log^{P(x_i|y_i)P(y_i)} = \sum [log^{P(x_i|y_i)} + log^{P(y_i)}] \\ = \sum [log^{N(u_1, σ^2)^{y_i} * N(u_2, σ^2)^{1-y_i}} + log^{p^{y_i}*(1-p)^{1-y_i}}] \\ = \sum[y_ilog^{N(u_1, σ^2)} + (1-y_i)log^{N(u_2, σ^2)} + y_ilog^p + (1-y_i)log^{1-p}]\)</span></li>
<li><span class="math inline">\(θ = (u_1, u_2, σ, p)\)</span></li>
<li><span class="math inline">\(\hat{θ} = argmax \ L(θ)\)</span></li>
<li>求解4个参数
<ul>
<li><span class="math inline">\(p\)</span>
<ul>
<li>相关部分 <span class="math inline">\(L = \sum [y_ilog^p + (1-y_i)log^{1-p}]\)</span></li>
<li><span class="math inline">\(\frac{dL}{dp} = \sum [\frac{y_i}{p} - \frac{1-y_i}{1-p}] = 0 → \sum [y_i(1-p)- (1-y_i)p] = \sum(y_i - p) = 0\)</span></li>
<li><span class="math inline">\(\hat{p} = \frac{1}{N} \sum y_i = \frac{N_1}{N}\)</span>（二分类（0,1），<span class="math inline">\(\sum y_i = N_1\)</span>）</li>
</ul></li>
<li><span class="math inline">\(u_1\)</span> （同理 <span class="math inline">\(u_2\)</span>）
<ul>
<li>相关部分 <span class="math inline">\(L = \sum y_ilog^{N(u_1, σ^2)} = \sum y_ilog^{\frac{1}{(2\pi)^{\frac{d}{2}}σ^{\frac{1}{2}}} e^{\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ}}}\)</span></li>
<li><span class="math inline">\(u_1 = argmax L ∝ argmax \sum y_i\frac{(x_i-u_1)^T(x_i-u_1)}{-2σ} = argmax \frac{-1}{2} \sum y_i[(x_i-u_1)^T(x_i-u_1)σ^{-1}] \\ = argmax \frac{-1}{2} \sum y_i[x_i^Tσ^{-1}x_i-2u_1^Tσ^{-1}x_i+u_1^Tσ^{-1}u_1]\)</span></li>
<li><span class="math inline">\(\frac{dL}{du_1} = \frac{-1}{2}\sum y_i[-2σ^{-1} x_i + 2σ^{-1} u_1] = 0 → \sum y_i(u_1 - x_i) = 0\)</span></li>
<li><span class="math inline">\(u_1 = \frac{\sum y_i x_i}{\sum y_i} = \frac{\sum y_i x_i}{N_1}\)</span></li>
</ul></li>
<li>σ
<ul>
<li>相关部分<span class="math inline">\(L = \sum [y_ilog^{N(u_1, σ^2)}+(1-y_i)log^{N(u_2, σ^2)}] = \sum [log^{N(u_1, σ^2)}+log^{N(u_2, σ^2)}]\)</span>
<ul>
<li>（二分类，非0即1，可以拆分算，可以省去<span class="math inline">\(y_i\)</span>）</li>
</ul></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=20" target="_blank" rel="noopener">详解</a></li>
<li><span class="math inline">\(σ = \frac{1}{N} (N_1S_1 + N_2S_2)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Naive Bayes</strong></p>
<ul>
<li><strong>朴素贝叶斯 = 贝叶斯定理 + 特征条件独立</strong>
<ul>
<li>贝叶斯定理计算复杂，设定特征条件独立简化计算</li>
<li>但特征条件独立，特性太强了，不符合现实情况（见Bayes_MRF对图概率模型的缺点描述）</li>
<li>最简单概率图模型</li>
</ul></li>
<li>生成模型、离散</li>
<li>Data
<ul>
<li>X: data, (n, d), n个数据样本，每个d维向量</li>
<li>Y: class, Y={c<sub>1</sub>, c<sub>2</sub>, ...,c<sub>k</sub>}, k个类别</li>
<li>y: label, (1, n), n个标签</li>
</ul></li>
<li><strong>prior probability</strong>
<ul>
<li>P(Y=c<sub>k</sub>)</li>
<li>属于贝叶斯派，认为参数也属于未知变量，符合概率分布</li>
<li>若样本特征的分布大部分是<font color="red">连续值</font>，则先验为<font color="red">高斯分布</font>的朴素贝叶斯</li>
<li>若样本特征的分大部分是<font color="red">多元离散值</font>，则先验为<font color="red">多项式分布</font>的朴素贝叶斯</li>
<li>若样本特征是二元离散值或者很稀疏的<font color="red">二元离散值</font>，先验为<font color="red">伯努利分布</font>的朴素贝叶斯</li>
<li><a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">sk-learn</a></li>
</ul></li>
<li><strong>conditional probability</strong>
<ul>
<li><span class="math inline">\(P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(d)}=x^{(d)}|Y=c_k) = \prod_{j}^{d} P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li>
<li>特征条件独立（上标表示第j-th维度）</li>
</ul></li>
<li><strong>joint probability distributions</strong>
<ul>
<li>联合概率分布</li>
<li><span class="math inline">\(P(X, Y) = P(X|Y)P(Y)\)</span></li>
</ul></li>
<li><strong>posterior probability</strong>
<ul>
<li><span class="math inline">\(P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum P(X=x|Y=c_k)P(Y=c_k)} = \frac{P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)}\)</span></li>
<li>则分类器为
<ul>
<li><span class="math inline">\(y=f(x) = argmax \ P(Y=c_k|X=x) ∝ argmax \ P(Y=c_k)\prod_{j}^{d}P(X^{(j)}=x^{(j)}|Y=c_k)\)</span></li>
<li>分母不变，则仅与分子成正比</li>
<li>意义：<strong>样本x属于c_k类别的最大概率为多少</strong></li>
<li>代码实践中，训练时学习均值和方差，测试时直接计算对数极大似然</li>
</ul></li>
</ul></li>
<li><strong>loss function</strong>
<ul>
<li>最大后验概率转-&gt;期望风险最小化</li>
<li><span class="math inline">\(L(Y, f(x)) = \left\{\begin{matrix}  1 \ if \ Y!= f(x)\\  0 \ if \ Y=f(x) \end{matrix}\right.\)</span>
<ul>
<li>Y: train label, y=f(x) : predict label</li>
</ul></li>
</ul></li>
<li>期望风险函数：<span class="math inline">\(R_{exp(f)} = E[L(Y, f(x))]\)</span>
<ul>
<li>根据联合概率分布：<span class="math inline">\(R_{exp(f)} = E_x \sum [L(c_k|f(x))]P(c_k|X)\)</span></li>
</ul></li>
<li><span class="math inline">\(f(x) = argmin \ Σ[L(c_k|f(x))]P(c_k|X)\)</span>
<ul>
<li>根据L(Y, f(x))函数展开，消去Y=f(x)项</li>
<li><span class="math inline">\(f(x) = argmin \ \sum P(y \neq c_k|X=x) = argmin \ (1-P(y=c_k|X=x)) = argmax \ P(y=c_k|X=x)\)</span></li>
<li>意义：<strong>样本x属于其他类别的最小概率为多少</strong>（等价于 样本x属于c<sub>k</sub>类别的最大概率为多少）</li>
</ul></li>
<li>详情案例见统计学习方法(第二版)63页</li>
<li>Naive Bayes Pyhon实现（sklearn）<a href="https://github.com/soloistben/images/blob/master/statistics/Linear_Classification/naive_bayes_demo.py" target="_blank" rel="noopener">code</a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">prior: P(y) = class_count[y]/n_samples  (non-negative, sum = 1.)</span></span><br><span class="line"><span class="string">condition: P(x|y) = ΠP(X^(i)=x^(i)|y)   (i for i-th feature, P(x|y)~N(μ,σ^2))</span></span><br><span class="line"><span class="string">posterior: P(y|x) = P(x,y)/P(x) = P(y)P(x|y)/Σ[P(y)P(x|y)]</span></span><br><span class="line"><span class="string">             P(y|x) = argmax P(y)P(x|y)</span></span><br><span class="line"><span class="string">MLE: y = argmax log[P(y)ΠP(x|y)] = argmax [log P(y) - 1/2Σ[log(2*pi*σ^2)+(x-μ)^2/σ^2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x:[n_samples, n_feature]</span></span><br><span class="line"><span class="string">y:[n_samples,]</span></span><br><span class="line"><span class="string">μ:[n_class, n_feature]</span></span><br><span class="line"><span class="string">σ^2:[n_class, n_feature]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class_count: [n_class,] (sum(class_count) = n_samples) </span></span><br><span class="line"><span class="string">(n_new: class_cout in this times, n_past: class_cout in last times)</span></span><br><span class="line"><span class="string">update μ, σ^2</span></span><br><span class="line"><span class="string">    μ_new = np.mean(X_i)</span></span><br><span class="line"><span class="string">    σ^2_new = np.var(X_i)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train time: learning μ, σ^2 in train data</span></span><br><span class="line"><span class="string">test time: log MLE in test data</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes_Gaussian</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, y, var_smoothing=<span class="number">1e-9</span>)</span>:</span></span><br><span class="line">        self.X = X</span><br><span class="line">        self.y = y</span><br><span class="line">        self.epsilon_ = var_smoothing * np.var(X, axis=<span class="number">0</span>).max()</span><br><span class="line">        self.classes_ = np.unique(y)</span><br><span class="line"></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        n_classes = len(self.classes_)</span><br><span class="line"></span><br><span class="line">        self.theta_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.sigma_ = np.zeros((n_classes, n_features))</span><br><span class="line">        self.class_count_ = np.zeros(n_classes, dtype=np.float64)</span><br><span class="line">        self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64) <span class="comment"># init P(y)</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._partial_fit(self.X, self.y)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">            jll = self._joint_log_likelihood(test_X)</span><br><span class="line">            <span class="keyword">return</span> self.classes_[np.argmax(jll, axis=<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_partial_fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">            <span class="comment"># Put epsilon back in each time</span></span><br><span class="line">            self.sigma_[:, :] -= self.epsilon_</span><br><span class="line">          </span><br><span class="line">          classes = self.classes_</span><br><span class="line">          unique_y = np.unique(y)</span><br><span class="line">  </span><br><span class="line">          <span class="comment"># loop on n_class, learning mu and var</span></span><br><span class="line">          <span class="keyword">for</span> y_i <span class="keyword">in</span> unique_y:</span><br><span class="line">              i = classes.searchsorted(y_i)</span><br><span class="line">              X_i = X[y == y_i, :]    <span class="comment"># X_i [n_class, n_feature]</span></span><br><span class="line">              N_i = X_i.shape[<span class="number">0</span>]</span><br><span class="line">              new_theta, new_sigma = self._update_mean_variance(</span><br><span class="line">                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i)</span><br><span class="line">  </span><br><span class="line">              self.theta_[i, :] = new_theta</span><br><span class="line">              self.sigma_[i, :] = new_sigma</span><br><span class="line">              self.class_count_[i] += N_i</span><br><span class="line">  </span><br><span class="line">          self.sigma_[:, :] += self.epsilon_</span><br><span class="line">          self.class_prior_ = self.class_count_ / self.class_count_.sum()</span><br><span class="line">          <span class="keyword">return</span> self</span><br><span class="line">  </span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_joint_log_likelihood</span><span class="params">(self, test_X)</span>:</span></span><br><span class="line">          joint_log_likelihood = []</span><br><span class="line">          <span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(self.classes_)):</span><br><span class="line">              jointi = np.log(self.class_prior_[i])</span><br><span class="line">              n_ij = - <span class="number">0.5</span> * np.sum(np.log(<span class="number">2.</span>*np.pi*self.sigma_[i, :]))</span><br><span class="line">              n_ij -= <span class="number">0.5</span> * np.sum(((test_X - self.theta_[i, :])**<span class="number">2</span>)/(self.sigma_[i, :]), <span class="number">1</span>)</span><br><span class="line">              joint_log_likelihood.append(jointi + n_ij)</span><br><span class="line">  </span><br><span class="line">          joint_log_likelihood = np.array(joint_log_likelihood).T</span><br><span class="line">          <span class="keyword">return</span> joint_log_likelihood</span><br><span class="line">  </span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">_update_mean_variance</span><span class="params">(n_past, mu, var, X)</span>:</span></span><br><span class="line">          </span><br><span class="line">          <span class="keyword">if</span> X.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">              <span class="keyword">return</span> mu, var</span><br><span class="line">  </span><br><span class="line">          n_new = X.shape[<span class="number">0</span>]</span><br><span class="line">          new_var = np.var(X, axis=<span class="number">0</span>)</span><br><span class="line">          new_mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> new_mu, new_var</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/11/Statistics/" data-id="ckhlr6dim000z0jega6t37xoy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Bayes-MRF" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/12/Bayes-MRF/" class="article-date">
  <time datetime="2020-08-12T12:10:46.000Z" itemprop="datePublished">2020-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/12/Bayes-MRF/">Bayes_MRF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="bayes-network贝叶斯网络-markov-random-fields-马尔可夫随机场">Bayes Network贝叶斯网络 &amp; Markov Random Fields 马尔可夫随机场</h4>
<h5 id="one.-前提">One. 前提</h5>
<ul>
<li><strong>Probabilistic Graphical Model (PGM 概率图模型)</strong> （将概率引入图模型，没有图，只能计算，引入图，比较直观，容易观察）
<ul>
<li>Representation 表示
<ul>
<li>有向图 Bayesian Network (有向无环，则起始结点决定这终止节点的概率)</li>
<li>无向图 Markov Network (Markov Random Fields) (无向，则结点的概率仅取决于1阶邻居)</li>
<li>高斯图 （连续）
<ul>
<li>Gassian Bayes Network</li>
<li>Gassian Markov Network</li>
</ul></li>
</ul></li>
<li>Inference 推断
<ul>
<li>精确推断
<ul>
<li>Variable Elimination, Belief Propagation, Junction Tree Algorithm</li>
</ul></li>
<li>近似推断
<ul>
<li>确定性近似（变分推断）</li>
<li>随机性近似（蒙特卡洛，MCMC）</li>
</ul></li>
</ul></li>
<li>Learning 学习
<ul>
<li>参数学习
<ul>
<li>完备数据（非隐变量）（有向，无向）</li>
<li>隐变量（EM）</li>
</ul></li>
<li>结构学习 (学习更好的图结构，参数)</li>
</ul></li>
</ul></li>
<li><strong>高维随机变量</strong> P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大
<ul>
<li>边缘概率 P(x<sub>i</sub>)</li>
<li>条件概率 P(x<sub>j</sub>|x<sub>i</sub>)</li>
</ul></li>
<li><strong>运算原则</strong>
<ul>
<li>sum rule: <span class="math inline">\(P(x_{1}) = \int_{x_{2}} P(x_{1}, x_{2}) d_{x_{2}}\)</span> (边缘概率)</li>
<li>poduct rule: <span class="math inline">\(P(x_{1}, x_{2}) = P(x_{1})*P(x_{2}|x_{1}) = P(x_{2})*P(x_{1}|x_{2})\)</span></li>
<li>chain rule: <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li>
<li>bayesian rule: <span class="math inline">\(P(x_{2}|x_{1}) = \frac{P(x_{1}, x_{2})}{P(x_{1}) }= \frac{P(x_{2})*P(x_{1}|x_{2}) }{\int P(x_{1}, x_{2}) d{x_{2}}}\)</span></li>
</ul></li>
<li><strong>缺点</strong>
<ul>
<li><font color="red">高维复杂 P(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>p</sub>) 计算量太大</font></li>
<li>简化
<ul>
<li>每个<strong>维度之间相互独立</strong> (特性太强了)
<ul>
<li><span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i})\)</span></li>
<li>Naive Bayes 朴素贝叶斯 <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y)\)</span></li>
</ul></li>
<li>Markov Property 马尔可夫特性
<ul>
<li><strong>将来独立于过去</strong>（相关性太单调了，不是很符合现实，现实往往跟几个相关）</li>
<li>x<sub>i+1</sub> 只与 x<sub>i</sub>相关，与其他 x<sub>i-1</sub>,...,x<sub>1</sub>无关</li>
</ul></li>
<li><strong>条件独立性</strong>（可降低计算复杂度）
<ul>
<li>给定x<sub>B</sub>情况下，集合x<sub>A</sub>与集合x<sub>C</sub>无关 （x<sub>A</sub>，x<sub>B</sub>，x<sub>C</sub>无交集）</li>
<li>x<sub>B</sub> 只与x<sub>C</sub>相关 <a href="https://www.bilibili.com/video/BV1BW41117xo?p=1" target="_blank" rel="noopener">video</a> <a href="https://www.bilibili.com/s/video/BV1Dk4y1q78a" target="_blank" rel="noopener">MRF video</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="two.-bayes">Two. Bayes</h5>
<ul>
<li>链式法则 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{1}, x_{2}, ..., x_{i-1})\)</span></li>
<li><p>因子分解 <span class="math inline">\(P(x_{1}, x_{2}, ..., x_{p}) = \prod_{i=1} P(x_{i}|x_{p(i)})\)</span>（x<sub>p(i)</sub>为x<sub>i</sub>父亲集合，即指向x<sub>i</sub>的结点）（条件独立性） <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_1.png" alt="bayes_1" style="zoom: 80%;"></p></li>
<li><strong>tail to tail</strong>
<ul>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A)\)</span></li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span>
<ul>
<li>则 <span class="math inline">\(P(C|A) = P(C|A,B)\)</span>（A，B同时发生时，不影响C），则在发生A时，B,C相互独立（<font color="red">若A被观测，则路径被阻塞，B,C相互独立，“倒V路径”</font>）</li>
</ul></li>
<li>条件独立性：<span class="math inline">\(P(B|A)P(C|A) = P(B|A)P(C|A,B) = P(B,C|A)\)</span></li>
</ul></li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_2.png" alt="bayes_2" style="zoom:75%;"></p>
<ul>
<li><strong>head to tail</strong>
<ul>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|B)\)</span></li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span></li>
<li>发生B时，A,C相互独立（<font color="red">若B被观测，则路径被阻塞，A,C相互独立</font>）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_3.png" alt="bayes_3" style="zoom:80%;"></p></li>
<li><strong>head to head</strong>
<ul>
<li>默认情况下（C还没被观察）A,B相互独立，路径被阻塞（<font color="red">若C被观测，路径是连通，A、B有关系，不独立则难以分解</font>）</li>
<li>因子分解 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B)P(C|A,B)\)</span> （父亲结点先于子结点）</li>
<li>链式法则 -&gt; <span class="math inline">\(P(A,B,C) = P(A)P(B|A)P(C|A,B)\)</span>
<ul>
<li><span class="math inline">\(P(B) = P(B|A)\)</span>，则默认情况下，C还没被观察，A,B相互独立</li>
</ul></li>
<li>这个模式是想判断 <span class="math inline">\(P(A|C) == P(A|C,B)\)</span>，在没有B条件时，直接基于C判断A，概率会更大（最初A,B相互独立）</li>
</ul>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_4.png" alt="bayes_4" style="zoom:75%;"></p>
<ul>
<li>（<font color="red">若D被观测，路径也是是连通，A、B有关系</font>）</li>
</ul></li>
<li>有向图是的条件独立性（证明发生x<sub>B</sub>, x<sub>A</sub>, x<sub>C</sub>相互独立）
<ul>
<li>D-separation
<ul>
<li><p>x<sub>A</sub>, x<sub>B</sub>, x<sub>C</sub> 三个集合两两无交集 <img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_5.png" alt="bayes_5"></p></li>
<li>若A与C，存在B<sub>1</sub>，B<sub>2</sub>关系，且属于x<sub>B</sub>集合；存在B<sub>3</sub>，B<sub>4</sub>关系，且不属于x<sub>B</sub>集合</li>
<li>则符合：发生x<sub>B</sub>时，存在上述情况， x<sub>A</sub>, x<sub>C</sub>相互独立 （<strong>全局马尔可夫性</strong>）</li>
<li><span class="math inline">\(P(x_{i}|x_{-i}) = \frac{P(x_{i}, x_{-i})}{P(x_{-i})} = \frac{p(x)}{\int P(x_{i}) d{x_{i}}} = \frac{\prod_{j} P(x_{j}|x_{p(j)})}{\int \prod_{j} P(x_{j}|x_{p(j)})d_{x_{i}}}\)</span>
<ul>
<li>x<sub>-i</sub>表示集合{x<sub>1</sub>,...x<sub>p</sub>}中去除x<sub>i</sub>, <strong>x/x<sub>i</sub></strong></li>
<li><span class="math inline">\(\prod_{j} P(x_{j}|x_{p(j)})\)</span> 分为与x<sub>i</sub>有关和无关两部分</li>
<li><p>则 <span class="math inline">\(\frac{\prod_{j} P(x_{j}|x_{p(j)})}{\int \prod_{j} P(x_{j}|x_{p(j)})d_{x_{i}}}\)</span>无关部分则可以约去</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/bayes_6.png" alt="bayes_6"><figcaption>bayes_6</figcaption>
</figure></li>
<li><span class="math inline">\(P(x_{i}, x_{-i}) = \prod_{j} P(x_{j}|x_{p(j)})\)</span>，即 x_i只与x_i相关的有联系(红点)，与无关的相互独立，也称为Markov Blanket</li>
<li>一个人与全世界的关系=一个人与身边人的关系</li>
</ul></li>
</ul></li>
</ul></li>
<li>Bayes Network 模型 （<font color="red">从单一到混合，有限到无限，空间到时间，离散到连续</font>）
<ul>
<li>离散
<ul>
<li>单一
<ul>
<li><p><strong>Naive Bayes</strong> 朴素贝叶斯 -&gt; 做分类 -&gt; <span class="math inline">\(P(x|y) = \prod_{i} P(x_{i}|y=1)\)</span> (x 是p维， 当y被观测时，x各维度相互独立)</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/Naive_Bayes.png" alt="Naive_Bayes" style="zoom:67%;"></p></li>
</ul></li>
<li>混合
<ul>
<li><p><strong>GMM</strong> 高斯混合模型（多个高斯分布） -&gt; 做聚类</p>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/GMM.png" alt="GMM" style="zoom:75%;"></li>
</ul></li>
<li>时间
<ul>
<li><strong>Markov Chain</strong> 马尔可夫链</li>
<li><strong>Gaussian Process</strong> 无限维高斯分布</li>
</ul></li>
<li>动态模型 = 混合 + 时间
<ul>
<li><strong>HMM</strong> 隐马尔可夫 (隐状态离散)</li>
<li><strong>LDS</strong> 线性动态系统 <strong>Kalmm Filter</strong> 卡尔曼滤波器（连续，高斯，线性）</li>
<li><strong>Partide Filter</strong> （非连续，非高斯）</li>
</ul></li>
</ul></li>
<li><p>连续</p>
<ul>
<li><strong>Gaussian Bayes Network</strong> 高斯图</li>
</ul></li>
</ul></li>
</ul>
<h5 id="three.-mrf">Three. MRF</h5>
<ul>
<li>无向图</li>
<li><p>条件独立性，发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关 （<strong>global markov</strong> 全局马尔可夫）</p>
<ul>
<li>存在集合x<sub>A</sub>, x<sub>C</sub>被x<sub>B</sub>分割（对应 bayes D-separation）， 那么发生x<sub>B</sub>时，x<sub>A</sub>与x<sub>C</sub>无关</li>
</ul></li>
<li><strong>local markov</strong> 局部马尔可夫
<ul>
<li><p>结点(蓝点)与邻居以外结点(白点)相互独立，仅与邻居(红点) 相关</p>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/markov_1.png" alt="markov_1" style="zoom:75%;"></p></li>
</ul></li>
<li><p><strong>pair markov</strong>（应用图像领域，图像-&gt;成对马尔可夫随机场-&gt;网格状马尔可夫随机场）</p>
<ul>
<li>在集合x<sub>-i,-j</sub>(集合没有i，j结点), 对于任意两个点x<sub>i</sub>, x<sub>j</sub>没有直接连接，则相互独立, (i ≠ j)</li>
</ul></li>
<li>条件独立性体现的三个方面，并且相互等价，可以互推 global markov &lt;=&gt; local markov &lt;=&gt; pair markov &lt;=&gt; 基于最大团的因子分解
<ul>
<li>clique团，最大团
<ul>
<li>集合间的结点相互联通</li>
<li>在一个团无法添加结点，则是最大团</li>
<li>(c<sub>1</sub>, c<sub>2</sub>,...表示团)</li>
</ul></li>
</ul></li>
<li>因子分解 <span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}),Z = Σ_{x_{1}}...Σ_{x_{p}}\prod_{i}^{p}Φ(x_{c_{i}})\)</span> （用因子分解证明条件独立性）
<ul>
<li>Φ 势函数， 必须为正（大于0）
<ul>
<li><span class="math inline">\(Φ (x_{c_{i}}) = e^{-E(x_{c_{i}})}\)</span> (E为能量函数，也叫势函数)</li>
<li>Φ (x<sub>ci</sub>) &lt;-&gt; P(x) 称为 Gibbs Distribution (Boltzmann Distribution 玻尔兹曼分布)</li>
<li><span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i}^{p} Φ(x_{c_{i}}) = \frac{1}{Z} \prod_{i}^{p} e^{-E(x_{c_{i}})} = \frac{1}{Z} e^{-\sum E(x_{c_{i}})}\)</span></li>
<li>Gibbs Distribution是统计物理的名称，与指数族分布形式一样（俩个等价）</li>
<li>最大熵原理：在满足已知事实，最终推出分布属于指数族分布</li>
<li>结论：Gibbs Distribution &lt;=&gt; Markov Random Field</li>
</ul></li>
<li>c<sub>i</sub>最大团，x<sub>ci</sub>最大团随机变量集合</li>
<li>Z 为联合概率分布的归一化因子</li>
<li>基于最大团的因子分解，则可以证明为马尔可夫随机场 (Hammesley-clifford定理)</li>
<li>局部势函数：只考虑局部变量；边缘概率：考虑全局变量</li>
<li>Pair-MRF 因子分解：<span class="math inline">\(P(x) = \frac{1}{Z} \prod_{i} Φ(x_{i}) \prod_{j} Φ(x_{i}, x_{j})\)</span> (考虑边)</li>
<li>最大后验概率推理（图像分割问题）：max<sub>x</sub> P(x)
<ul>
<li><p>找到一个x分布，使P(x)最大（则找到图像分割在结果）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov.png" alt="pair-markov"><figcaption>pair-markov</figcaption>
</figure></li>
<li>假设<span class="math inline">\(θ(x_{i}) = -logΦ(x_{i}),θ(x_{i}, x_{j}) = -logΦ(x_{i},x_{j})\)</span></li>
<li><p><span class="math inline">\(max_{x} P(x)\)</span> -&gt; 能量最小化 -&gt; <span class="math inline">\(min_{x} E(x) = \sum θ(x_{i}) + \sum θ(x_{i}, x_{j})\)</span></p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_2.png" alt="pair-markov_2"><figcaption>pair-markov_2</figcaption>
</figure>
<p><img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_3.png" alt="pair-markov_3" style="zoom:75%;"></p></li>
<li><p>假设图像具有连续性，相邻结点没有突变（则对角线为0，要没都属于前景要么都是背景），边的势函数可以设置为（斜对角线，为大于0的值，若当前俩点，一个前景一个背景，则惩罚它）</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_4.png" alt="pair-markov_4"><figcaption>pair-markov_4</figcaption>
</figure></li>
<li><p>结点的设置势函数，一个前景一个背景</p>
<figure>
<img src="https://github.com/soloistben/images/raw/master/bayes_mrf/pair-markov_5.png" alt="pair-markov_5"><figcaption>pair-markov_5</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/12/Bayes-MRF/" data-id="ckhlr6dh300010jegcrxcv6lf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页 &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/11/17/C-plus-note/">C_plus_note</a>
          </li>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
          <li>
            <a href="/2020/10/06/PGM-Inference/">PGM_Inference</a>
          </li>
        
          <li>
            <a href="/2020/10/05/Exponential-Family-Distribution/">Exponential_Family_Distribution</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>