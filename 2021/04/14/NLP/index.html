<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>NLP | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Natural Language Processing  word embedding 发展历程  基于字典组成one-hot模型  一方面词典的数目一般都是上万数量级的，造成对于单个词而言，向量表示过于稀疏 另一方面由于仅仅存储0、1数据，没有办法保存对应的语序信息，也就无法通过one-hot编码挖掘词与词之间的语义关系  tf-idf  tf是词频，也就是当前词在文档中出现的次数 idf的计算">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP">
<meta property="og:url" content="http://yoursite.com/2021/04/14/NLP/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="Natural Language Processing  word embedding 发展历程  基于字典组成one-hot模型  一方面词典的数目一般都是上万数量级的，造成对于单个词而言，向量表示过于稀疏 另一方面由于仅仅存储0、1数据，没有办法保存对应的语序信息，也就无法通过one-hot编码挖掘词与词之间的语义关系  tf-idf  tf是词频，也就是当前词在文档中出现的次数 idf的计算">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-04-16T09:28:04.938Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP">
<meta name="twitter:description" content="Natural Language Processing  word embedding 发展历程  基于字典组成one-hot模型  一方面词典的数目一般都是上万数量级的，造成对于单个词而言，向量表示过于稀疏 另一方面由于仅仅存储0、1数据，没有办法保存对应的语序信息，也就无法通过one-hot编码挖掘词与词之间的语义关系  tf-idf  tf是词频，也就是当前词在文档中出现的次数 idf的计算">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-NLP" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/04/14/NLP/" class="article-date">
  <time datetime="2021-04-14T09:26:06.000Z" itemprop="datePublished">2021-04-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      NLP
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h5 id="natural-language-processing">Natural Language Processing</h5>
<ul>
<li>word embedding 发展历程
<ul>
<li>基于字典组成one-hot模型
<ul>
<li>一方面词典的数目一般都是上万数量级的，造成对于单个词而言，向量表示过于稀疏</li>
<li>另一方面由于仅仅存储0、1数据，没有办法保存对应的<strong>语序信息</strong>，也就无法通过one-hot编码挖掘词与词之间的语义关系</li>
</ul></li>
<li>tf-idf
<ul>
<li>tf是词频，也就是当前词在文档中出现的次数</li>
<li>idf的计算方法是log(语料库中总文档数 / 包含当前词的文档数)</li>
<li>tf-idf就是one-hot的一种优化</li>
<li>文档比较有<strong>代表性的词</strong>发挥更大的作用，规避常见词（the, I, is, ...）</li>
<li>无法考虑词的位置信息，上下文信息以及一些分布特征</li>
</ul></li>
<li>Encoder-Decoder（少用）
<ul>
<li>encoder输入onehot，deocder输出</li>
</ul></li>
<li>NNLM（Nerual Network Language Model）
<ul>
<li>基于上下文，上文预测下文</li>
<li>词向量只是副产物</li>
</ul></li>
<li><font color="red">word2vec</font>
<ul>
<li>Continuous Bag-of-Word CBOW 上下文来预测当前词（用得更多，bert也类似）
<ul>
<li>计算量大（利用huffman结构，词频高的在树的浅层、词频低的在深层，减少softmax计算量）</li>
<li>负采样：找些错误样本，在正确样本学习时排斥错误样本</li>
</ul></li>
<li>Continuous Skip-gram 当前词来预测上下文（太难了）</li>
<li>缺点：没有解决一词多义的问题</li>
</ul></li>
<li>Glove (Global Vectors for Word Representation)
<ul>
<li>类似word2vec，但是上下文的范围更广，是全局的句子单词</li>
<li>因为要全部文本，所以无法online training，因此用的比较少，效果并没有比word2vec太好</li>
<li>缺点：没有解决一词多义的问题</li>
</ul></li>
<li>Fasttext
<ul>
<li>将单词拆成字符作为输入，并且利用n-gram（apple -&gt; app, ppl, ple），学习到一个embedding（embedding最终做个分类任务）</li>
<li>对于低频词生成的词向量效果会更好</li>
<li>缺点：没有解决一词多义的问题</li>
</ul></li>
<li>为了解决<font color="red">一词多义</font>的问题才有后面预训练模型的学习
<ul>
<li>ELMO、BERT</li>
</ul></li>
</ul></li>
<li>Model
<ul>
<li>seq-to-class
<ul>
<li>输入所有token，输出一个class</li>
<li>输入所有token，对应token各输出一个class（识别句子中所有词汇的词性，word segmentation）（bert里面可以学习该两项特性，未必需要模型预先学习这些处理）</li>
</ul></li>
<li>seq-to-seq
<ul>
<li>autoencoder+attention</li>
</ul></li>
<li>multiple sequences
<ul>
<li>两个句子分别输入同一模型，再拼接结果</li>
<li>类似bert，用特定符号连接句子，作为一个输入</li>
</ul></li>
</ul></li>
<li>Task
<ul>
<li>word segmentation 断词
<ul>
<li>对于英文，word之间用空格分开</li>
<li>对于中文，台湾大学简陈台大 -&gt; (台湾大学)(简陈)(台大) （具体如何划分，看任务）</li>
</ul></li>
<li>coreference resolution 指代消解
<ul>
<li>哪些词汇指向相同的东西（主语&lt;-&gt;代词）</li>
</ul></li>
<li>summarization 总结文本
<ul>
<li>extractive summarization 提取摘要，每个句子都需要个label，模型识别哪些句子应当放入摘要（原句），作为总结整个文本</li>
<li>abstractive summarization 模型输出语句总结文本，seq-to-seq模型，并非输出原句，但会有共有的词汇（需要模型对输入词汇有拷贝作用，拷贝重要词汇作用于输入）</li>
</ul></li>
<li>machine translation 机器翻译
<ul>
<li>seq-to-seq模型（输入语言，输出另外语言）（输入语音，输出文字）（输入一种语言的语音，输出另外语言的语音）</li>
</ul></li>
<li>grammar error correction 语法错误纠正
<ul>
<li>seq-to-seq模型（修改语法）</li>
</ul></li>
<li>sentiment classification 情感分析
<ul>
<li>seq-to-class模型（输入评论，输出正负情感）</li>
</ul></li>
<li>veracity prediction 立场预测
<ul>
<li>seq-to-class模型（输入post文、评论回应等，输入正反立场）</li>
</ul></li>
<li>natural language inference (NLI) 自然语言推理
<ul>
<li>seq-to-class模型（输入premise、hypothesis句子，输出三个类别:contradiction、entailment、neutral）</li>
<li>两个句子的关系</li>
</ul></li>
<li>search engine 搜索引擎
<ul>
<li>seq-to-class模型（匹配问题答案的相关性，relevant）</li>
</ul></li>
<li><strong>QA问答</strong>
<ul>
<li>（Watson模型）问题处理 - 生成候选答案 - 候选答案评分 - 排序</li>
<li>阅读理解：输入question和knowledge source（unstructured documents，可以来源于搜索引擎），过滤掉不相关的documents， 输出答案</li>
</ul></li>
<li><strong>dialogue 对话模型</strong>
<ul>
<li>chatting 尬聊，需要输入历史对话，得到下一次的回复（模型应当加入更多特性：personality个性化、empathy同理心、knowledge知识丰富的）</li>
<li>task-oriented 任务导向：引导用于完成一个任务（拆分模型：natural language generation (NLG) 提前设定好可以问的问题，需要输入历史对话 -&gt; NLG -&gt; 得到下一次的回复）</li>
<li>natural language understanding (NLU) -&gt; state tracker -&gt; state -&gt; policy -&gt; NLG -&gt; answer</li>
<li>NLU: intent classification + slot filling（内容识别，提取内容）</li>
</ul></li>
<li><strong>knowledge graph 知识图谱</strong>
<ul>
<li>node 为 entity 实体， edge 为 node之间的关系</li>
<li><font color="red">从文字抽取实体、实体关系</font></li>
<li>name entity recognition (NER)
<ul>
<li>根据任务确定实体范围，一般为人名、组织、地名、时间</li>
<li>seq-to-class</li>
<li>多个名字指向同一个东西，多个东西指向同一个名字，name entity linking</li>
</ul></li>
<li>relation extraction
<ul>
<li>若relation是有限的，那么可以看成分类任务</li>
</ul></li>
</ul></li>
</ul></li>
<li>pretrain + fine-tune (用大量预料做预训练，再针对特定任务的数据微调模型)
<ul>
<li>What is pretrian model?
<ul>
<li>each token -&gt; embedding
<ul>
<li><font color="red">(old method (<strong>word2vec and Glove</strong>): same token -&gt; same embedding, exclude context)</font></li>
<li>english word -&gt; token : too much words (FastText: 将单词拆成字符作为输入，学习到一个embedding)</li>
<li>chinese character -&gt; token (EMNLP: 将汉字看为图片，用CNN学习汉字各个部位，得到embedding)</li>
</ul></li>
<li><font color="red">Contextualized Word Emnedding </font>(基于上下文来学习单词的embedding) (学习token到一词多义)
<ul>
<li>LSTM, self-attention, tree-base (少用，没有LSTM厉害)</li>
<li>self-supervised learning 自监督学习（无监督学习）：常见的语言模型，根据上文学习到embedding，该embedding还能预测下文（设计model，下文不能被偷看，此处self-attention需要加限制，mask掉下文需要预测部分）</li>
<li>token-level
<ul>
<li><strong>ELMO</strong> (Embedding from Language Models) (bi-lstm)、
<ul>
<li>语言中预测一个单词，需要看它常常跟哪些词汇在一起（上下文）</li>
<li>双向：基于上文预测下文，基于下文预测上文，再结合两部分embedding（按什么比例结合，看任务）</li>
<li>缺点：每个LSTM<strong>只看到了部分的文本</strong>，各自工作</li>
</ul></li>
<li><strong>GPT</strong> (Generative Pre-Training)（从上文预测下文，Left-to-Right）不希望fine-tune，直接pretrain解决问题
<ul>
<li>Transformer decoder部分</li>
<li>GPT3</li>
<li>in-context learning 纯文本输入，让模型理解文本内容
<ul>
<li>few-shot learning： 输入：给出问题描述和少许例子，让模型读题目写答案</li>
<li>one-shot learning： 输入：给出问题描述和1个例子，让模型读题目写答案</li>
<li>zero-shot learning： 输入：给出问题描述，让模型读题目写答案</li>
</ul></li>
<li>closed book QA
<ul>
<li>直接问问题，模型给出答案（问加法问题，直接给出答案）</li>
</ul></li>
</ul></li>
<li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformer) <font color="red">(限定文本长度 512)</font>
<ul>
<li>Transformer encoder部分</li>
<li>bert可以<strong>随机mask部分词汇</strong>，根据上下文的self-attention去预测mask的词汇，规避了ELMo的缺点（类似CBOW，求和固定的全部上下文，预测mask东西，bert的attention可以自己需要多少看多少）</li>
<li>mask 输入的<font color="red">whole word(整个单词)(bert模型)</font>、phrase-level(短语) &amp; entity-level(实体)(ERNIE模型)、mask 多个token (span bert模型)</li>
<li><strong>Transformer-XL</strong> 可以跨文本读取更长的文本
<ul>
<li>Transformer 问题1：<strong>context fragmentation（内容碎片化）</strong>，一次输入x最长只能是512长度，若输入超过512只能且分成两段喂入模型，这样形成两个片段之间内容信息没有被使用到（也就是处于500的token没有使用到处于600位置上token的信息，反之第二片段，也没有使用到第一段的信息，这样语义上是不完整的）</li>
<li>XL maens extra-long</li>
<li>解决方法：segment-level recurrence（跨越片段获取信息）将前面训练过片段固定缓存下来，为下个片段提供信息，反向传播不会影响前面固定片段信息（长度只能达到80%RNN能学的长度，但效果好）（尽可能保存多个片段，效果会更好）</li>
<li>vanilla transfromer 解决方案：固定窗口大小，在x中滑动，事后面的xt能学到前面的消息，但同时也受限于固定的窗口滑动大小（长度只能达到450% vanilla transfromer能学的长度，效果好，训练速度快）</li>
<li>Transformer 问题1: <strong>absolute Positional Encoding （绝对路径）</strong>，每个片段内部都使用绝对路径，两个片段没有距离可言，则跨片段获取信息是不起作用的</li>
<li>解决方法：Relative Positional Encoding（相对路径），直接考虑input x之间的相对位置，而不是绝对位置
<ul>
<li>在计算attention score的时候，只考虑query向量与key向量的相对位置关系，根据正弦函数生成</li>
<li>在当前key中<strong>拼接</strong>前片段的hidden state长度，query不变</li>
</ul></li>
</ul></li>
<li><strong>XLNet</strong> (内部使用Transformer-XL)
<ul>
<li>属于自回归模型</li>
<li>预测当前token i的时候，xi不参与预测计算（已知xi再去预测xi，有点无意义）
<ul>
<li>每个输入都有content stream和query stream，前者带有输入信息，后者不带有输入信息（训练得到）（两者类似encode和decode）</li>
<li>每层都更新content stream和query stream，query stream用于预测当前词的时候作为输入</li>
<li>微调的时候不需要query stream，content stream已经学到信息了</li>
</ul></li>
<li>输入序列是正常顺序，拆分词的时候，进行打乱顺序（解决双向问题）</li>
<li>“语言模型中，上下文乱序也能学出mask的token”</li>
</ul></li>
<li>Reformer, Longformer 解决self-attention计算量的问题</li>
<li>bert缺点：假如必须上文预测下文，则bert效果不太好，但若是不需要按顺序预测，估计bert也ok</li>
</ul></li>
<li>seq2seq 的pretrain
<ul>
<li>autoencoder+attention，破坏（mask、多个mask、删除、乱序）的输入部分，去预测正确的输入（BART/MASS模型）</li>
<li><strong>BART/MASS</strong>：综合bert和gpt，前面部分使用bert双向attention，后面部分使用单向的attention</li>
<li>UniLM：综合bert、gpt、BART/MASS三个模型</li>
</ul></li>
<li>ERNIE (Enhanced Representation through Knowledge Integration)
<ul>
<li>为中文设计，mask盖住phrase-level(短语) &amp; entity-level(实体)</li>
<li>加入图谱</li>
</ul></li>
<li>Grover (Generating aRticles by Only Viewing mEtadata Records)</li>
<li>BERT &amp; PALs (Projected Attention Layers)</li>
<li>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
<ul>
<li>置换token，语法没错，之改变语义</li>
<li>不做embedding，对每个token做yes/no二分类，检查句子所有token是否有问题</li>
<li>先用个small bert的预测一个token，填入原句，再利用model进行判断是否有问题</li>
</ul></li>
</ul></li>
<li>sentence-level，为整个句子进行embedding
<ul>
<li>skip thought (encode上句，decode下句) : 根据上一句生产下一句的embedding，相邻句子，embdding类似</li>
<li>qiuck thought (分别encode上句、下句) : 避开做生成seq（计算量很大），相近句子，embdding越相似</li>
<li>bert中，两个句子学习NSP (next stence prediction)，[CLS]做分类（效果不好，检测是否相关还ok，顺序就不太行）</li>
<li>使用SOP (sentence order prediction) 两句话颠倒（顺序很重要），则要输出错误</li>
<li>结合NSP和SOP会更好</li>
</ul></li>
</ul></li>
</ul></li>
<li>How to fine-tune?
<ul>
<li>句子分割符号：seq1 [SEP] seq2</li>
<li>句首符号：[CLS] seq
<ul>
<li>one class
<ul>
<li>[CLS]可以代表整个句子的embedding，可以用[CLS]的embedding做分类</li>
<li>用所有embedding求平均或者RNN综合输出一个embedding做分类</li>
</ul></li>
<li>class for each token
<ul>
<li>将所有token输入一个分类器做多分类</li>
</ul></li>
<li>copy from input
<ul>
<li>Extracetion-base QA: 输入Document、Query，输出文本两个下标，代表答案范围</li>
<li>输入[CLS] query [SEP] Document，得到document所有token的输出，分别用两个向量，在token中做dot-product or LSTM再接softmax，选择最高成绩，分别作为起始下标和终止下标</li>
</ul></li>
<li>seq2seq
<ul>
<li>传统做法为，encoder-decoder + attention，存在decoder中的token没有在encoder见识过（没有pretrain过）</li>
<li>输入seq1 [SEP] seq2，输出根据[SEP]开始，预测seq2第一个的token，最后个token预测结束字符</li>
</ul></li>
</ul></li>
<li>fine-tune方法
<ul>
<li><strong>pretrain作为特征提取，固定不变，再接入下游任务</strong> （更优，pretrain模型很大）</li>
<li>pretrain 和 下游任务一起训练 （pretrain有预训练参数不是随机参数，所以也不会直接过拟合）</li>
<li>折中做法：Adaptor模型，fine-tune时候，pretrain中加入adapter层（self-attention后，前馈神经网络后面），adaptor层与下游任务一起训练，pretrian部分不变</li>
</ul></li>
<li>有实验证明 基于pretrain的模型，loss会下降很快，也就是可以快速训练，范化能力更强</li>
</ul></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/04/14/NLP/" data-id="cknipj2pl000pj0eg02vg2ocv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/04/09/Interview/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Interview</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/14/NLP/">NLP</a>
          </li>
        
          <li>
            <a href="/2021/04/09/Interview/">Interview</a>
          </li>
        
          <li>
            <a href="/2020/11/17/C-plus-note/">C_plus_note</a>
          </li>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>