<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Interview | MR.C</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简历项目  自适应图聚类  GCN细节  2016年诞生 针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片） 基于拓扑结构与节点属性特征，卷积过程：传播节点信息，汇聚邻居节点信息来更新自身节点。（计算公式上，基于邻接矩阵求得拉普拉斯矩阵，再与特征矩阵做内积，意义在于从谱域傅里叶转换到频域做乘积计算，再逆傅里叶转换回谱域，实现卷积的过程）（拉普拉斯算是邻接矩阵的一个变体，主">
<meta property="og:type" content="article">
<meta property="og:title" content="Interview">
<meta property="og:url" content="http://yoursite.com/2021/04/09/Interview/index.html">
<meta property="og:site_name" content="MR.C">
<meta property="og:description" content="简历项目  自适应图聚类  GCN细节  2016年诞生 针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片） 基于拓扑结构与节点属性特征，卷积过程：传播节点信息，汇聚邻居节点信息来更新自身节点。（计算公式上，基于邻接矩阵求得拉普拉斯矩阵，再与特征矩阵做内积，意义在于从谱域傅里叶转换到频域做乘积计算，再逆傅里叶转换回谱域，实现卷积的过程）（拉普拉斯算是邻接矩阵的一个变体，主">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/interview/deepwalk.jpg">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/interview/dropout.png">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/interview/skip_gram.jpg">
<meta property="og:image" content="https://github.com/soloistben/images/raw/master/interview/deepcopy.png">
<meta property="og:updated_time" content="2021-05-08T07:30:50.123Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Interview">
<meta name="twitter:description" content="简历项目  自适应图聚类  GCN细节  2016年诞生 针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片） 基于拓扑结构与节点属性特征，卷积过程：传播节点信息，汇聚邻居节点信息来更新自身节点。（计算公式上，基于邻接矩阵求得拉普拉斯矩阵，再与特征矩阵做内积，意义在于从谱域傅里叶转换到频域做乘积计算，再逆傅里叶转换回谱域，实现卷积的过程）（拉普拉斯算是邻接矩阵的一个变体，主">
<meta name="twitter:image" content="https://github.com/soloistben/images/raw/master/interview/deepwalk.jpg">
  
    <link rel="alternate" href="/atom.xml" title="MR.C" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MR.C</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Interview" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/04/09/Interview/" class="article-date">
  <time datetime="2021-04-09T09:34:54.000Z" itemprop="datePublished">2021-04-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Interview
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol type="1">
<li><p>简历项目</p>
<ul>
<li>自适应图聚类
<ul>
<li>GCN细节
<ul>
<li>2016年诞生</li>
<li>针对非结构化数据（（欧氏空间）结构化数据：一维的文本or信号、二维的图片）</li>
<li><font color="red">基于拓扑结构与节点属性特征，卷积过程：传播节点信息，汇聚邻居节点信息来更新自身节点。</font>（计算公式上，基于邻接矩阵求得拉普拉斯矩阵，再与特征矩阵做内积，意义在于从谱域傅里叶转换到频域做乘积计算，再逆傅里叶转换回谱域，实现卷积的过程）（拉普拉斯算是邻接矩阵的一个变体，主要是特征分解后的半正定特性更优）</li>
<li>CNN的卷积操作属于GNN的一种特殊情况（3x3九宫格中，中心像素点汇聚周围8个像素点的信息）</li>
<li>本质实在数据中做<strong>低通滤波</strong>的作用，过滤高频噪声信息，提取出低频信息（类似于CNN人脸识别的时候获得人脸五官的基础特征信息，叠加深层之后，获得整个人脸特征）</li>
</ul></li>
<li>用什么方法解决了什么问题？结果如何？
<ul>
<li>问题：图神经网的浅层、深层会出现过平滑问题导致性能下降、所有节点卷积层是固定的</li>
<li>动机：每个节点需求是不一样的，处于稠密子图中的节点需要信息可能只需要浅层，处于稀疏子图的节点需要更深层的信息</li>
<li>自适应机制：将RNN中自适应机制转移到GNN中，基于上层embedding评估饱和度是多少，累计饱和度是否还需要继续进行卷积操作，实现所有节点可以自适应控制自己的卷积层数，有效缓解过平滑问题，学习到更好的embedding，效果在5个数据有3%～6%的提升。（固定卷积变成动态卷积）</li>
<li>自监督机制：基于聚类两大特性设计损失函数（簇内的距离应当越小，簇间的距离应当越大），也属于自监督学习（无监督学习）</li>
</ul></li>
<li>应用场景
<ul>
<li>基于拓扑结构和特征信息的聚类操作、推荐系统、知识图谱、蛋白质相互作用网络、社交网络、风控网络</li>
</ul></li>
</ul></li>
<li>病毒宿主预测任务
<ul>
<li>问题：分析病毒基因数据，预测宿主</li>
<li>针对基因学习序列信息，kmer提取基因信息 or bert预训练基因序列（解决一次多义的问题），基于宿主生物分类结构来构造拓扑结构，实现匹配网络</li>
<li>损失函数：针对基因匹配的正确样本与错误样本，学习与正确样本更相似，更排斥错误样本</li>
</ul></li>
<li>卷积循环神经网络
<ul>
<li><p>问题：提取过去时序图片信息、预测未来某一时刻图片信息</p></li>
<li><p>CNN+LSTM（在LSTM内部的点乘换成conv即可）</p></li>
<li><p>GAN：生成器生成分布，判别器针对生成分布与真实分布进行<strong>判别是真实图片还是虚假图片</strong>，则算是回归任务，生成分布尽可能拟合真实分布</p>
<ul>
<li>更注重细节，常用于<strong>图片超分辨率任务</strong></li>
<li>判别器的目的是将真实和虚假的图像准确分辨。所以生成器产生的图像会越来越真，判别器的辨别能力会越来越强</li>
<li>判别器的损失则由判别真实图片的损失与虚假图片的损失</li>
<li>GAN大量用于<strong>扩充数据集</strong>或者给<strong>未标记的数据集打标签</strong>等任务上</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>机器学习、深度学习</p>
<ul>
<li><p><font color="red">SVM原理</font></p>
<ul>
<li>提出SVM是为了解决二分类问题；成功分类的直线（平面）有无数个，SVM就要找到最优的结果（即<strong>所有样本距离平面都足够大</strong>）max margin(w, b) s.t. yi (w^T xi + b) &gt; 0</li>
<li>最优平面：w x+b，参数需要基于样本学习</li>
<li>hard-margin SVM是基于样本属于可分的，但是实际数据是存在噪声，可能导致分不好，甚至不可分</li>
<li>二次凸优化问题
<ul>
<li>符合参数的偏导为0，才能转换<strong>对偶问题</strong>（max min交换，<a href="https://www.zhihu.com/question/36694952" target="_blank" rel="noopener">相对原本的问题更容易求解</a>）</li>
<li>针对线性可分数据</li>
</ul></li>
<li>soft-margin SVM在hard-margin SVM基础上允许一点点错误</li>
<li><font color="red">kernel函数</font>，是针对完全非线性的数据，<strong>非线性转换到高维空间，转成线性可分问题</strong>，再使用SVM。（高维空间比低维更易线性可分）（深度学习使用多层感知机解决异或问题，可以不转高维）
<ul>
<li>kernel 函数还可以解决对偶问题（计算高维空间内积运算），直接得到内积结果，不需要先得到单个函数结果，再计算内积。</li>
</ul></li>
</ul></li>
<li><p>牛顿法</p>
<ul>
<li>用于求极值问题，找到导数为0的点</li>
<li>从x1点求导求出切线，切线与x轴的交点为x2，依次继续操作，直至得到导数为0的点</li>
<li>与梯度下降一样会面临局部最优和<strong>鞍点问题</strong>（导数为0，在一个维度上看是极小值，另一个维度则是极大值；对应的hessian矩阵的特征值有正有负）</li>
<li><strong>高维参数空间存在大量鞍点</strong>（梯度下降是一次一次找到最优梯度，但牛顿法是直接找到梯度为0的点，更容易陷入鞍点问题）</li>
<li>随机梯度下降：给正确梯度加点噪声、随机初始化出发点、随机绕动，就在一定程度避免鞍点</li>
</ul></li>
<li><p>LSTM &amp; GRU</p>
<ul>
<li>Long short-term memory 长短期记忆（4个需要学习的激活函数，4倍参数量，<a href="https://zhuanlan.zhihu.com/p/113901983" target="_blank" rel="noopener">最后一个tanh是共用的，只做计算</a>，其他都是独立的），可以解决<strong>梯度消失</strong>（state是通过累加方式，所以避免梯度消失（res net异曲同工）；RNN使用gradient clipping（如果梯度的范数大于某个给定值，将梯度同比收缩）防止梯度爆炸）
<ul>
<li>对于RNN，多出cell state来记忆之前的信息，变化较慢</li>
<li>forget gate 忘记阶段。这个阶段主要是对<strong>上一个节点传进来的输入进行选择性忘记</strong>（<a href="https://zhuanlan.zhihu.com/p/100948638" target="_blank" rel="noopener">忘记不重要的，记住重要的</a>）
<ul>
<li>sigmoid层，选择部分忘记（0全部遗忘，1全部保留）（当前时刻需要忘记过去不重要的事情）</li>
</ul></li>
<li>input gate 记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”
<ul>
<li>sigmoid层，决定什么值需要更新</li>
<li>tanh层，创建一个新的候选值向量，生成候选记忆（生成当前新记忆融合进cell state传递倒下一个时刻）</li>
</ul></li>
<li>output gate 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出
<ul>
<li>sigmoid层，确定cell state 记忆的哪部分需要输出（生成hidden state）</li>
</ul></li>
<li>参数 4*(input_size*hidden_size + hidden_size*hidden_size)</li>
</ul></li>
<li>Gated Recurrent Unit 循环门单元（3个激活函数，3倍参数量），<a href="https://zhuanlan.zhihu.com/p/34203833" target="_blank" rel="noopener">LSTM的变种</a>
<ul>
<li>合并了forget gate和input gate到一个单独的update gate中，合并cell state到hidden state</li>
<li>reset gate（经过sigmoid）衡量需要保留多少之前的记忆（类似forget gate）</li>
<li>update gate （经过sigmoid）衡量当前加入多少信息进入记忆中</li>
<li>参数 3*(input_size*hidden_size + hidden_size*hidden_size)</li>
</ul></li>
<li>一方面GRU的参数更少，因而训练稍快或需要更少的数据来泛化。另一方面，如果你有<strong>足够的数据</strong>，LSTM的强大表达能力可能会产生更好的结果</li>
</ul></li>
<li><p>损失函数</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/97698386" target="_blank" rel="noopener">损失函数是用来估量<strong>模型的输出</strong>与<strong>真实值</strong>之间的差距，反向传播给模型的优化指引方向。</a></li>
<li>交叉熵（Cross Entropy，逻辑回归，对数损失函数）：假设样本服从伯努利分布（01分布），得到分布的似然函数再求极值（使用对数log，是因为直接求导太复杂，对数后的单调性不变）</li>
<li>MSE（Mean Squared Error 均方差损失，最小二乘法）是线性回归的一种，可以就将问题转化成了一个凸优化问题：假设样本和噪声都服从高斯分布，通过极大似然估计（MLE）可以推导出最小二乘式子（计算简单、欧式距离）</li>
<li>MAE（Mean Absolute Error Loss 平均绝对误差）：假设样本服从拉普拉斯分布。（MAE与MSE区别可参考L2、L1）</li>
<li>交叉熵和均方差区别
<ul>
<li>MSE假设了样本之间服从高斯分布，<font color="red">在分类任务下这个假设没办法被满足</font>，因此效果会很差</li>
<li>MSE当预测结果接近0 or 1，loss偏差很小，则会导致梯度很小，更新则很慢，此时是非凸函数，容易得到局部最优。</li>
<li>交叉熵当预测结果接近0 or 1，容忍度极低，直接会趋向于无穷，惩罚力度很大，换而言之MSE惩罚力度不够。（均可在函数求导中体现）</li>
<li>交叉熵可解释性差，只能选0或1，MSE的高斯分布可解释性更好</li>
</ul></li>
</ul></li>
<li><p>DeepWalk</p>
<ul>
<li><p>思想类似word2vec，使用图中节点与节点的共现关系来学习节点的向量表示，<a href="https://leovan.me/cn/2020/04/graph-embedding-and-gnn/" target="_blank" rel="noopener">学习node embedding（在没有feature）</a></p></li>
<li><p>网络节点的表示中<strong>节点构成的序列</strong>就是随机游走，网络上不断重复地随机选择游走<strong>路径</strong>（深度优先搜索）</p>
<ul>
<li><p>从某个特定的端点开始，游走的每一步都从与当前节点相连的边中随机选择一条（固定长度），沿着选定的边移动到下一个顶点，不断重复这个过程（类似NLP，当前token预测下个token）（每一个节点映射成d维向量）s</p>
<p><img src="https://github.com/soloistben/images/raw/master/interview/deepwalk.jpg" alt="deepwalk 绿色部分即为一条随机游走" style="zoom: 80%;"></p></li>
<li><p>借用词向量中使用的skip-gram模型</p></li>
<li><p>每个节点重复多次，即能学到更泛化性的embedding（但节点的遍历情况会很多，重复几次的随机游走能很好表示节点的邻居信息嘛？）</p></li>
<li><p>优点：并行化（并行游走采样）、适应性（适应网络局部的变化）</p></li>
</ul></li>
</ul></li>
<li><p>Spectral-GNN和Spatial-GNN的区别</p>
<ul>
<li>本质一样，GNN是一个热传导模型/信息扩散模型，从<strong>不同方式去汇聚信息</strong>，<strong>更新信息</strong></li>
<li>Spatial-GNN
<ul>
<li>直接推广 CNN 的<strong>加权求和思想</strong>，使用不同的领域节点采样方法和不同加权求和方法来更新节点特征</li>
<li>可以多种不同的采样方式，多种不同的加权求和方式</li>
<li>GraphSAGE 和 GAT 等都可以 Inductive learning，扩展到新的节点和新的图，因为这一类方法直接学习的采样过程和加权求和方式（GraphSAGE 还支持 mini-batch 的训练方式）</li>
<li>GraphSAGE：每一层的node的表示都是由上一层生成的，跟本层的其他节点无关
<ul>
<li>聚合邻居节点方法：mean、GCN(归一化的拉普拉斯再sum)、LSTM(忽略顺序)、max-pooling</li>
<li>GIN理论证明sum方法较好，比较合理</li>
<li>与GCN比，相对灵活，迁移性强，理论上没差</li>
</ul></li>
</ul></li>
<li>Spectral-GNN
<ul>
<li>从 CNN 的<strong>卷积定理</strong>，f 和 g 的卷积是 f 和 g 傅里叶变换之后乘积的傅里叶逆变换。然后通过拉普拉斯矩阵来实现傅里叶变换和逆傅里叶变换</li>
<li>算是基于图谱理论的Spatial-GNN的特例</li>
<li>优点：捕捉graph的全局信息，从而很好地表示node的特征；理论性强</li>
<li>缺点：
<ul>
<li>Transductive，都是单独的图结构，不能迁移到其他图结构（GAT，可以不受图结构影响）</li>
<li>同阶邻居权重一样（GAT是可以不一样，attention）</li>
<li>所有节点一起训练，无法快速学习新的node embedding（GraphSAGE 支持 mini-batch ）</li>
</ul></li>
</ul></li>
<li>新增加点意味着环境变了，那之前的节点的表示自然也应该有所调整
<ul>
<li>Inductive：<strong>对于老节点，可能新增一个节点对其影响微乎其微</strong>，所以可以暂且使用原来的embedding</li>
<li>Transductive：面对新节点，必须更新全部节点</li>
</ul></li>
</ul></li>
<li><p>有向图GNN</p>
<ul>
<li>GCN不可以使用在有向图，<strong>拉普拉斯需要对称才能正交分解</strong>，GAT可以用在有向图</li>
</ul></li>
<li><p>异构网</p>
<ul>
<li>HetGNN（2019）
<ul>
<li>随机游走得到路径，计算出现异构节点的频率，频率高的则视为重要邻居</li>
<li>Bi-LSTM聚合单个节点内异构content（成一维embedding）</li>
<li>Bi-LSTM聚合各个同类邻居信息（成一维embedding）</li>
<li>attention 线性组合所有异构邻居（更新成最终node embedding）</li>
</ul></li>
<li>HAN（2021北邮）
<ul>
<li>异构图拆成多个同构图</li>
<li>各个同构图中使用GAT（学习node embedding）</li>
<li>attention 线性组合各个同构图的node embedding（更新成最终node embedding）</li>
</ul></li>
</ul></li>
<li><p>CNN卷积核计算、池化计算</p>
<ul>
<li>图片大小n*n*c，卷积核大小f*f*c、卷积核个数为m个，padding大小p（为了保留边缘信息），步长stride为s</li>
<li>padding后图片为(n+2p)*(n+2p)*c</li>
<li>卷积核后图片三维：<strong>((n+2p-f)/s + 1)*((n+2p-f)/s + 1)*m</strong></li>
<li>使用尺寸大的卷积核，计算量大，多个小尺寸的，达到一样效果，但是计算量更小</li>
<li>池化（缩小模型大小，提高计算速度，提高特征的robust ）计算与卷积一样（一般不加padding）</li>
<li>卷积（padding保持图片大小）+池化（无padding，缩小图片）=卷积层</li>
<li>与全连接层相比，卷积层有两大优势：
<ul>
<li>parameter sharing<strong>（卷积核）参数共享</strong>（节省参数，在卷积操作时，卷积计算可以在不同图片区域使用相同的参数，一个过滤器适合一个图片某一块，则也会适合另一块）</li>
<li>sparsity connections <strong>稀疏连接</strong>（输入少，相比全连接而言，不需要输入全部参数，只要满足卷积核大的输入个数），有着两个优势也可以预防过拟合</li>
<li>参数计算：(f*f+1)*c，1为bias，全连接参数=输入输出所有参数的乘积</li>
</ul></li>
<li>一维卷积类似：n-f+1(若padding为0，stride为1)</li>
</ul></li>
<li><p>解决<strong>过拟合</strong>方法</p>
<ul>
<li>dropout原理、设置原理
<ul>
<li><p>在训练时，设置dropout使部分神经元暂时隐藏（不起作用），<font color="red"> 减少特征（隐层节点）间的相互作用，缓解过拟合（类似正则化）</font>，梯度下降仅更新未隐藏的神经元。（再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 ）</p>
<p><img src="https://github.com/soloistben/images/raw/master/interview/dropout.png" alt="dropout" style="zoom:80%;"></p></li>
<li><p>在过拟合严重处可降低dropout概率，不担心过拟合处可提高dropout概率</p></li>
<li><p>不在输入层使用dropout</p></li>
<li><p>一般认为设置为0.5或者0.3（Dropout是一个超参，需要根据具体的网络、具体的应用领域进行尝试）</p></li>
<li><p>在测试时不需要</p></li>
<li><p>除非出现过拟合状态，否则不用dropout，在计算机视觉常用dropout，在别的领域少用</p></li>
</ul></li>
<li>损失函数加入权重正则化一起训练（正则化是损失函数的惩罚项，对某些参数做一些限制）
<ul>
<li><font color="red"> L1, L2的正则化</font>
<ul>
<li>L1（平均绝对误差，MAE，mean average error）：目标值与预测值的绝对误差值的总和
<ul>
<li>鲁棒性、不稳定性、可能有多个解（在非稀疏向量上的计算效率就很低）</li>
<li>符合拉普拉斯分布，是不完全可微的，在图像上会有很多角出现（造成最优值出现在坐标轴上，<strong>因此就会导致某一维的权重为0</strong> ，产生<strong>稀疏权重矩阵</strong>，进而防止过拟合）</li>
<li>L1正则化是指权值向量中各个元素绝对值之和</li>
<li><strong>优点</strong>：输出具有稀疏性，<strong>即产生一个稀疏模型，进而可以用于特征选择</strong>（会趋向于产生少量的特征，而其他的特征都是0）；一定程度上，L1也可以防止过拟合</li>
<li><strong>缺点</strong>：但在非稀疏情况下计算效率低，存在多个解</li>
<li><font color="red">如果特征符合稀疏性，说明特征矩阵很多元素为0，只有少数元素是非零的矩阵，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响）</font>，此时就可以只关注系数是非零值的特征。</li>
</ul></li>
<li>L2（均方误差，MSE，mean squared error）：目标值与预测值的差值均方和
<ul>
<li>不是很鲁棒性、稳定、一个解（计算方便）</li>
<li>高斯分布，是完全可微的，图像上的棱角比较圆滑（参数<strong>不断趋向于0</strong>）</li>
<li><font color="red">如果误差大于1，则误差会放大更多（比L1更大），因此模型会对样本更加敏感</font>（若有一个异常样本，模型需要调整or牺牲很多正常样本，来适应单个异常值，正常的样本的误差比这单个的异常值的误差小）</li>
<li>L2正则化是指权值向量中各个元素的平方和然后再求平方根</li>
<li><strong>优点</strong>：计算效率高（因为存在解析解）；可以防止模型过拟合；选择更多的特征，这些特征都会接近于0</li>
<li><strong>缺点</strong>：非稀疏输出；无特征选择</li>
</ul></li>
</ul></li>
<li>L1与L2的区别只在于，L2是权重的平方和，而L1就是权重的和</li>
<li>权重过大会引起梯度爆炸</li>
</ul></li>
<li>降低模型复杂度、减轻过拟合</li>
<li>数据增强</li>
<li>早停机制提早结束训练，在出现过拟合之前</li>
</ul></li>
<li><p>bert预训练与word2vector比较</p>
<ul>
<li><p>基于上下文的word embedding（动态：根据上下文决定word的语义）</p>
<ul>
<li>BERT使用Transformer中encoder部分的self-attention、Mask Language Model、Next Sentence Prediction
<ul>
<li>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示（自监督学习属于无监督学习）（<font color="red"><strong>解决一词多义</strong></font>）</li>
<li><font color="red">为什么可以解决一词多义？</font>
<ul>
<li>以根据<a href="https://www.cnblogs.com/Lee-yl/p/11245754.html" target="_blank" rel="noopener">上下文单词的语义</a>去调整单词的Word Embedding表示</li>
<li>根据上下文+attention机制，学习到不同语义环境的token embedding（CBOW将上下文直接<strong>加和</strong>，无视顺序）</li>
<li>相比单向训练模式，双向训练模型捕获上下文信息会更加全面（相比LSTM模型没有长度限制问题）</li>
</ul></li>
<li><font color="red">Mask Language Model</font>：在训练时在输入中随机mask 15%单词，然后根据上下文预测单词（完型填空）（80%直接mask、10%更换任意单词、10%保留原单词）（若100%mask掉，有可能在微调模型时候，没有见过某些单词）</li>
<li>Next Sentence Prediction：判断第二句话是否第一句话的下文，50%的IsNext和50%的NotNext（选择题（多分类）、判断题（二分类）、简答题（回归））</li>
</ul></li>
</ul></li>
<li><p>word2vector（静态：预训练完，就不会改变embedding）</p>
<ul>
<li><p>Skip-gram：如果是用一个词语作为输入，来预测它周围的上下文（其实每次预测一个word，<strong>输入是the的one-hot编码，输出是quick的one-hot编码</strong>）（调整|V|次输入词，分别预测K次，反向传播调整embedding，计算复杂度则O(K|V|)，（|V|为word总数，K为窗口大小））</p>
<p><img src="https://github.com/soloistben/images/raw/master/interview/skip_gram.jpg" alt="skip_gram" style="zoom: 67%;"></p></li>
<li><p>CBOW：如果是拿一个词语的上下文作为输入，来预测这个词语本身（在训练阶段中心词的词向量是把窗口大小内<strong>上下文的词向量相加作为输入</strong>，忽略了<strong>顺序信息</strong>）（需要学习|V|个word embedding，计算复杂度为O(|V|)）</p></li>
<li><p><font color="red">为什么不可以解决一词多义？</font></p>
<ul>
<li>GloVe与word2vec，两个模型都可以根据词汇的“共现co-occurrence”信息，将词汇编码成一个向量（所谓共现，即语料中词汇一块出现的<font color="red"><strong>频率</strong></font>）（GloVe是全局范围，word2vec是固定局部范围）</li>
<li>GloVe，本质上是对共现矩阵进行降维。首先，构建一个词汇的共现矩阵<strong>，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率</strong>。由于context是多种词汇的组合，其维度非常大，希望像network embedding一样，在context的维度上降维（PCA的原理），学习word的低维表示</li>
<li>GloVe与word2vec，embedding效果差不多，GloVe更容易并行化，所以对于较大的训练数据，GloVe更快</li>
<li><font color="red">即便使用CBOW框架，基于上下文预测word，但是最终结果仍是简单的向量加和平均，将多个语义信息糅合在单一向量中，用一个向量表示多以语义信息，是不足够的</font></li>
</ul></li>
<li><p>one-hot encode输入单词，输入一个神经网络（隐含层未使用激活函数，线性的），得到的embedding则是整个word2vector模型，同时加入整个模型的训练（质上是一种<font color="red">降维操作</font>，词袋数量很大时候，onehot维度很大，则用神经网络降维到固定的维度）</p></li>
<li><p>训练trick: hierarchical softmax (把 N 分类问题变成 log(N)次二分类，利用了huffman编码结构) 和 <a href="https://bitjoy.net/2019/06/22/cs224n%ef%bc%881-8%ef%bc%89introduction-and-word-vectors/" target="_blank" rel="noopener">negative sampling(预测总体类别的一个子集，原本是更新所有词汇的概率，使用之后只更新部分)</a> 减少计算时间复杂度（速度最快的组合是CBOW+负采样）</p></li>
<li><p>word2vector 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练</p></li>
</ul></li>
</ul></li>
<li><p>多分类问题</p>
<ul>
<li>拆成多个二分类
<ul>
<li>例如三分类拆成一个为一类，两个看成另一类，但会出现样本不均衡问题</li>
<li>四分类拆成一对一对，分别将两个看成一类</li>
</ul></li>
</ul></li>
<li><p>激活函数</p>
<ul>
<li>实际上大多数现象呈现关系都是<font color="red"><strong>正相关</strong></font>关系，并非线性关系，因此用非线性函数作为激活函数是更适合表达正相关关（如果不使用激活函数，这种情况下每一层输出都是上一层输入的线性函数。非线性函数的反复叠加，才使得神经网络有足够的能力来抓取复杂的特征）</li>
<li>sigmoid、tanh优点：能压缩数据范围到(0,1)区间，保证数据幅度不会有问题（该大大，该小小）</li>
<li>sigmoid=1/(1-e<sup>-x</sup>), tanh=(e<sup>x</sup>-e<sup>-x</sup>)/(e<sup>x</sup>+e<sup>-x</sup>), relu = max(0, x)</li>
<li>sigmoid、tanh会引起梯度消失、幂运算相对耗时，relu<font color="red">收敛更快、不会梯度消失、计算复杂低</font>（若初始化不好，学习率过大，导致参数更新过大导致神经元坏死，处于负数区域，梯度为0，某些神经元就不会被激活，采用Xavier初始化方法避免学习率过大or采用算法调节学习率）（leaky relu=max(0.01x, x)解决上诉问题，但效果未必好；ELU=x if x&gt;0 else a(e<sup>x</sup>-1)也能解决问题，但计算量大，效果未必好）</li>
<li>relu在x=0处不可导，但实际情况很少会出现靠近0的数，则可以忽略</li>
<li>线性函数作为激活函数一般是在输出时（全连接层）</li>
</ul></li>
<li><p>模型调参（如何让模型更加鲁棒性）</p>
<ul>
<li>数据层面上
<ul>
<li>training时候保证数据是打乱顺序，防止学习到输入的顺序信息，会使模型更加鲁棒性</li>
<li>数据增强（增加数据多样性）</li>
<li>数据采用，尽可能数据平衡，训练集验证集测试集分布要一致</li>
<li>数据不平衡的时候，在训练期间应用类别加权操作。<a href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html" target="_blank" rel="noopener">给稀少的类更多的权重，给主要类更少的权重；sklearn计算类权重</a>，<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis" target="_blank" rel="noopener">或者尝试使用过采样和欠采样技术重新采样你的训练集</a></li>
</ul></li>
<li>模型过拟合层面
<ul>
<li>针对模型做一些正则化操作（L1、L2、Dropout）</li>
<li>早停机制</li>
</ul></li>
<li>训练层面
<ul>
<li>选择正确的<strong>优化器</strong>（关心快速收敛，使用自适应优化器，如Adam，但它可能会陷入局部最小；SGD+momentum可以实现找到全局最小值，但它依赖于鲁棒初始化，而且可能比其他自适应优化器需要更长的时间来收敛）
<ul>
<li>Adam和SGD，优化方法
<ul>
<li>优化算法的功能，是通过改善训练方式，来最小化(或最大化)损失函数E(x)</li>
</ul></li>
<li><strong>SGD</strong>梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数
<ul>
<li>momentum通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练，使网络能更优和更稳定的收敛；减少振荡过程（通常设定为0.9）</li>
<li>当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，也减少了振荡过程。</li>
<li><strong>Adam</strong>（Adaptive Moment Estimation）
<ul>
<li>计算每个参数的自适应学习率</li>
<li>在稀疏数据集使用，效果好</li>
</ul></li>
<li>在数据统计特性明显，分布好计算（估计）的时候，容易调整SGD的参数，使得SGD收敛好（设置好参数，<font color="red">可以达到全局最优</font>）</li>
<li>在数据统计特性不好，变化大，误差曲面复杂的时候，优先使用傻瓜算法Adam（简化了调参，但默认参数，<font color="red">可能会导致不收敛、局部最优</font>）</li>
</ul></li>
</ul></li>
<li>调整学习率（0.1，0.001，0.000001，以10为阶数进行尝试），微调：0.001；完整训练：&gt;=0.001。使用衰减学习率。</li>
</ul></li>
</ul></li>
<li><p>自回归模型</p>
<ul>
<li>是统计上处理时间序列的方法，假设{x1, x2, ..., xt-1, xt} 存在一维线性关系，有依赖关系，需要用xt-1去预测xt，而不是x预测y 。</li>
<li>预训练模型往往针对自回归语言生成模型设计，自回归每次会使用已生成的序列作为已知信息预测未来的一个单词，最终再把每个时间步生成的单词拼成一个完整的序列输出，（GPT典型自回归模型）</li>
<li>优点：
<ul>
<li>所需数据不多，可用x自身变数数列来进行预测；</li>
<li>非自回归和半非自回归的<strong>依赖关系学习和生成难度较大</strong>，它们的生成质量往往弱于自回归模型</li>
</ul></li>
<li>缺点：
<ul>
<li>随着数据量增大，自回归模型推断耗时也随之变大</li>
<li>非自回归模型学习时间更快，每个单词之间没有依赖关系，整个输出序列的每个单词被并行地同步预测（质量则更差）</li>
<li>只能上文预测下文，无法双向</li>
</ul></li>
</ul></li>
<li><p>聚类算法：</p>
<ul>
<li>K-means：每次<strong>随机选择k个中心点</strong>，将每个数据点归类到离它最近的那个中心点所代表的簇中（簇内距离），迭代多次，一直到迭代了最大的步数或者前后 <strong>J</strong> 的值相差小于一个阈值为止。</li>
<li>Spectral cluster：只需要结点之间的相似度矩阵即可（可以由feature or 邻接矩阵构造相似读矩阵），并不需要结点的完整信息，因此不必像k-means那样要求N维的欧氏空间的向量，特征分解选择簇，抓住了结点之间的主要信息（k小个的特征值，低频信息），排除了冗余信息（k小之外的属于高频信息）。（<font color="red">聚类在符合高斯分布的样本有着差不多的效果，谱聚类还可以适用非高斯分布</font>）</li>
<li>KNN：计算样本X中每个样本点的距离，选择K小个的样本，划分为<strong>k个样本中归属于类别最多的一类</strong>（属于监督学习）</li>
<li>层次聚类(Hierarchical Clustering)：过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层（叶子）</li>
<li>密度聚类（DBSCAN）：簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类</li>
</ul></li>
<li><p>大数定理</p>
<ul>
<li>大数法则（Law of Large Numbers），关于随机变量序列的算术平均值向常数收敛的一系列极限定理的统称。</li>
<li>其表达方式主要有：切比雪夫（Chebyshev）大数法则、贝努利（Bernoulli）大数法则和泊松（Poisson）大数法法则</li>
<li>弱大数定理：样本均值以概率收敛于期望值</li>
<li>强大数定律：样本均值以概率1收敛于期望值=1</li>
<li>数学期望：每次可能的结果乘以其结果概率的总和</li>
</ul></li>
</ul></li>
<li><p>python</p>
<ul>
<li><p>深拷贝原理</p>
<ul>
<li><p>直接赋值：其实就是对象的引用（别名）</p></li>
<li><p>浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象</p></li>
<li><p>深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象</p>
<p><img src="https://github.com/soloistben/images/raw/master/interview/deepcopy.png" alt="deepcopy" style="zoom: 80%;"></p></li>
</ul></li>
<li><p>垃圾回收机制</p>
<ul>
<li>主要通过<strong>引用计数（Reference Counting）</strong>进行垃圾回收
<ul>
<li>每一个对象的核心就是一个结构体（内部有一个引用计数器）</li>
<li>引用计数+1：创建、引用、作为参数传入到函数、作为元素存储在容器（list、set等）</li>
<li>引用计数-1： del销毁对象、对象被赋予新对象、对象离开作用域、容器被销毁or容器删除该元素</li>
<li>引用计数法有其明显的优点，如高效、实现逻辑简单、具备实时性，<font color="red">一旦一个对象的引用计数归零，内存就直接释放了 </font> （缺点是需要<strong>单独分配空间</strong>来维护引用计数、当释放一个较大的对象时需要较长时间、循环引用是该机制必然存在的，需要算法对其补充）</li>
<li>只有容器对象才会产生<strong>循环引用</strong>的情况，比如<strong>列表、字典、用户自定义类的对象、元组</strong>等
<ul>
<li>“标记-清除”(Mark and Sweep)算法（双端链表，一个链表存放着需要被扫描的容器对象，另一个链表存放着临时不可达对象）：
<ul>
<li>标记阶段，遍历所有的对象，是否还有<strong>对象引用它</strong>，那么就标记该对象为可达</li>
<li>清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。</li>
<li>垃圾回收的阶段，会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行</li>
</ul></li>
<li>分代回收(Generational Collection)
<ul>
<li>上面算法需要暂停应用，分代回收以空间换时间，减少暂停时间</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>全局解释器</p>
<ul>
<li><strong>并行</strong>：多个CPU同时执行多个任务，就好像有两个程序，这两个程序是真的在两个不同的CPU内同时被执行。</li>
<li><strong>并发</strong>：CPU交替处理多个任务，还是有两个程序，但是只有一个CPU，会交替处理这两个程序，而不是同时执行，只不过因为CPU执行的速度过快，而会使得人们感到是在“同时”执行，执行的先后取决于各个程序对于时间片资源的争夺。</li>
<li>Global Interpreter Lock (<strong>GIL</strong>)：每个线程在执行时候都需要先获取GIL，保证同一时刻只有一个线程可以执行代码，不受其他线程干扰（多线程并不是真正意义上的同时执行）</li>
<li><font color="red">GIL的存在导致多线程无法很好的发挥<strong>多核</strong>CPU的<strong>并发</strong>处理能力</font>，<a href="https://liuchuang0059.github.io/2019/07/18/%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81(GIL)/" target="_blank" rel="noopener">线程只能等待其他线程执行完毕释放GIL</a></li>
<li>创建python时就只考虑到单核cpu，解决多线程之间数据完整性和状态同步的最简单方法自然就是加锁（于是有了GIL这把超级大锁），只会隔一段时间去释放GIL，或者遇到IO时自动释放</li>
<li>Python解释器进程内的多线程是以协作多任务方式执行</li>
<li>使用Multi-processing，避开GIL</li>
</ul></li>
<li><p>多线程</p>
<ul>
<li><font color="red">多线程类似于同时执行多个不同程序</font></li>
<li>优点
<ul>
<li>占据长时间的程序中的任务放到后台去处理</li>
<li>程序的运行速度可能加快</li>
<li>在一些等待的任务使用多线程，释放一些珍贵的资源占用</li>
</ul></li>
<li>线程在执行过程中与进程还是有区别的。每个独立的进程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。</li>
<li>每个线程都有他自己的一组CPU寄存器，称为线程的上下文，该上下文反映了线程上次运行该线程的CPU寄存器的状态</li>
<li>指令指针和堆栈指针寄存器是线程上下文中两个最重要的寄存器，线程总是在进程得到上下文中运行的，这些地址都用于标志拥有线程的进程地址空间中的内存。
<ul>
<li>线程可以被抢占（中断）</li>
<li>在其他线程正在运行时，线程可以暂时搁置（也称为睡眠） -- 这就是线程的退让</li>
</ul></li>
<li>线程可以分为:
<ul>
<li><strong>内核线程：</strong>由操作系统内核创建和撤销</li>
<li><strong>用户线程：</strong>不需要内核支持而在用户程序中实现的线程</li>
</ul></li>
<li><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017627212385376" target="_blank" rel="noopener">进程与线程</a>
<ul>
<li>每个进程至少要干一件事，一个进程至少有一个线程（线程是最小的执行单元），复杂程序有多个线程（比如Word，它可以多个线程同时进行打字、拼写检查、打印等事情）</li>
<li>操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样</li>
<li>多任务三种模式：多进程、多线程、多进程+多线程（复杂、少用）</li>
<li>涉及到同步、数据共享的问题</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>基础知识点（数据结构）</p>
<ul>
<li>数组、链表
<ul>
<li>数组元素存储地址连续（若频繁访问某个下标的元素，选择数组方式，时间复杂度O(1)）</li>
<li>链表元素存储地址不连续（若频繁访问某个下标的元素，需要从头访问，时间复杂度O(n)）</li>
</ul></li>
<li><font color="red">快速排序原理</font>
<ul>
<li>选择第一个元素的值做准基数</li>
<li>在右边找到较小的值放到左边、在左边找到较大值放到右边，达到<strong>左侧元素小于准基数，右侧大于准基数</strong></li>
<li>每次根据选择后点的位置，划分成两部分，再分别进行上诉操作</li>
<li>达到小到大的排序</li>
</ul></li>
<li>堆排序
<ul>
<li>大根堆：根结点大于左右子结点的二叉树（i结点的左子树为2i，右子树为2i+1）（存储形式为数组，逻辑为二叉树）</li>
<li>从第n/2个数开始，重复将数组调整成大根堆（大的子树与根结点做互换），最终整个子树的根结点为数值最大(把根结点和最后一个结点交换，把交换后的最后一个结点移出堆)；然后继续在n/2 -1上述操作。</li>
<li>遍历n/2次（也算n），每次调整时间复杂度为O(log n)，则最后为时间复杂度O(nlogn)，空间复杂度为O(1)（数值交换的使用临时变量）（不稳定、不存在最好最坏情况）</li>
<li>最大的数每次都放在最后，最终顺序为小到大</li>
</ul></li>
<li>哈希冲突：
<ul>
<li>在哈希映射中（key-value存储数据，java中的map，python中的dict），通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。<a href="https://hit-alibaba.github.io/interview/basic/algo/Hash-Table.html" target="_blank" rel="noopener">理想的哈希函数对于不同的输入应该产生不同的结构，同时散列结果应当具有同一性（输出值尽量均匀）和雪崩效应（微小的输入值变化使得输出值发生巨大的变化）</a></li>
<li>当出现多个输入映射到同一个结果，则为<a href="https://www.cnblogs.com/wuchaodzxx/p/7396599.html" target="_blank" rel="noopener">哈希冲突</a>
<ul>
<li><strong>开放定址法（再散列法）</strong>：发生冲突时，以冲突的地址再次哈希地址，若还冲突，继续哈希，直到找出一个不冲突的哈希地址（如果记录总数可以预知，可以创建完美哈希函数；缺点：存储记录的数目不能超过数组的长度，超过就需要扩容，而扩容会导致某次操作的时间成本飙升、再散列的操作使删除元素也会变得很麻烦）</li>
<li><strong>链地址法</strong>：为每个值建立一个单链表，当发生冲突时，将记录插入到链表中（避免了动态调整的开销，插入删除操作方便，但是查询链表比较耗时）</li>
<li>建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>大数据（spark，hadoop）</p>
<ul>
<li>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎</li>
<li>spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载</li>
<li>Spark 很快，支持交互式计算和复杂算法（Spark 是一个通用引擎，可用它来完成各种各样的运算，包括 SQL 查询、文本处理、机器学习）</li>
</ul></li>
</ol>
<p>code + quick_sort <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(arr, low, high)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> low&lt;high:</span><br><span class="line">        mid = partition(arr, low, high)	<span class="comment"># 找到一个准基数下标</span></span><br><span class="line">        quick_sort(arr, low, mid<span class="number">-1</span>)		<span class="comment"># 左边是小于准基数的部分，进行快排</span></span><br><span class="line">        quick_sort(arr, mid+<span class="number">1</span>, high)	<span class="comment"># 右边是大于准基数的部分，进行快排</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(arr, low, high)</span>:</span></span><br><span class="line">	p = arr[low]	<span class="comment"># 设定当前low位置为基准</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        <span class="keyword">while</span> low&lt;high <span class="keyword">and</span> arr[high]&gt;=p:	<span class="comment"># 在右边找比基准小的数</span></span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        arr[low] = arr[high]	<span class="comment"># 放置左边</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">while</span> low&lt;high <span class="keyword">and</span> arr[low]&lt;=p:		<span class="comment"># 在左边找到比基准大的数 </span></span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        arr[high] = arr[low]	<span class="comment"># 放置右边</span></span><br><span class="line">    arr[low] = p	<span class="comment"># 放置基准</span></span><br><span class="line">    <span class="keyword">return</span> low 		<span class="comment"># 返回基准下标</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>DFS (Depth First Search) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">neighbor_dict = &#123;node1:[neighbor1, ...], ...&#125;	<span class="comment"># 邻居集合，数据处理成邻接表形式</span></span><br><span class="line">node_list = [node1, node2, ...]	<span class="comment"># 所有节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归</span></span><br><span class="line">visited = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS_Traverse</span><span class="params">(node_list)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> node_list:	<span class="comment"># 这层遍历防止非连通子图</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            DFS(G, node)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="string">'''针对node做操作'''</span></span><br><span class="line">    visited.append(node)</span><br><span class="line">    <span class="comment"># 遍历邻居</span></span><br><span class="line">    <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[node]:	</span><br><span class="line">        <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            DFS(neighbor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非递归(使用栈)(直接指定某个起始节点，并非完整图的一个DFS)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFS_Traverse</span><span class="params">(node, neighbor_dict)</span>:</span></span><br><span class="line">    stack = []		<span class="comment"># list模仿栈</span></span><br><span class="line">	visited = []</span><br><span class="line">    </span><br><span class="line">    stack.append(node)	<span class="comment"># 入栈起始节点</span></span><br><span class="line">  	visited.append(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(stack)&gt;<span class="number">0</span>:</span><br><span class="line">        node = stack.pop()	<span class="comment"># 弹出栈顶的元素</span></span><br><span class="line">        <span class="string">'''针对node做操作'''</span></span><br><span class="line">        <span class="comment"># 遍历邻居</span></span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[node]:	</span><br><span class="line">            <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                stack.append(neighbor)</span><br><span class="line">                visited.append(neighbor)</span><br></pre></td></tr></table></figure></p></li>
<li><p>BFS (Breadth First Search) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">neighbor_dict = &#123;node1:[neighbor1, ...], ...&#125;	<span class="comment"># 邻居集合，数据处理成邻接表形式</span></span><br><span class="line">node_list = [node1, node2, ...]	<span class="comment"># 所有节点</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''非递归'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BFS_Traverse</span><span class="params">(node_list)</span>:</span></span><br><span class="line">    queue = []</span><br><span class="line">	visited = []</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> node_list:	<span class="comment"># 这层遍历防止非连通子图</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            <span class="string">'''针对node做操作'''</span></span><br><span class="line">            queue.append(node)	<span class="comment"># 入队起始节点</span></span><br><span class="line">            visited.append(node)</span><br><span class="line">            <span class="keyword">while</span> len(queue):</span><br><span class="line">                start_node = queue.pop(<span class="number">0</span>)	<span class="comment"># 弹出队头的起始节点，找邻居</span></span><br><span class="line">                <span class="comment"># 遍历邻居</span></span><br><span class="line">                <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[start_node]:	</span><br><span class="line">                    <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                        <span class="string">'''针对node做操作'''</span></span><br><span class="line">                        queue.append(neighbor)	<span class="comment"># 入队邻居节点</span></span><br><span class="line">            			visited.append(neighbor)</span><br><span class="line">                        </span><br><span class="line"><span class="string">'''BFS 求无权图某节点的最短路径'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BFS_Min_Distance</span><span class="params">(node, node_list)</span>:</span></span><br><span class="line">    queue = []</span><br><span class="line">    visited = []</span><br><span class="line">    d = &#123;node_:<span class="number">0</span> <span class="keyword">for</span> node_ <span class="keyword">in</span> node_list&#125;	<span class="comment"># 初始化为0，n个的距离数组</span></span><br><span class="line">    </span><br><span class="line">    queue.append(node)	<span class="comment"># 入队起始节点</span></span><br><span class="line">    visited.append(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(queue):</span><br><span class="line">        start_node = queue.pop(<span class="number">0</span>)	<span class="comment"># 弹出队头的起始节点，找邻居</span></span><br><span class="line">        <span class="comment"># 遍历邻居</span></span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> neighbor_dict[start_node]:	</span><br><span class="line">            <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                <span class="string">'''针对node做操作'''</span></span><br><span class="line">                d[neighbor] = d[start_node] + <span class="number">1</span> </span><br><span class="line">                queue.append(neighbor)	<span class="comment"># 入队邻居节点</span></span><br><span class="line">                visited.append(neighbor)</span><br></pre></td></tr></table></figure></p></li>
<li><p>LCS (Longest Common Subsequence)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">'''最长公共子序列，（递归）暴力破解版，自顶向下'''</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LCS_rc</span><span class="params">(text1, text2)</span>:</span></span><br><span class="line">    <span class="comment"># 最基本，若其中一个为空序列，则没有公共部分</span></span><br><span class="line">    <span class="keyword">if</span> len(text1)==<span class="number">0</span> <span class="keyword">or</span> len(text2)==<span class="number">0</span>:  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> text1[<span class="number">-1</span>] == text2[<span class="number">-1</span>]:</span><br><span class="line">        <span class="keyword">return</span> LCS_rc(text1[:<span class="number">-1</span>], text2[:<span class="number">-1</span>]) + <span class="number">1</span>   <span class="comment"># 找到一个就加1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 找不到就各缩短一边来再做比较</span></span><br><span class="line">        <span class="keyword">return</span> max(LCS_rc(text1[:<span class="number">-1</span>], text2), LCS_rc(text1, text2[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">'''自底向上'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LCS</span><span class="params">(text1, text2)</span>:</span></span><br><span class="line">    <span class="comment"># 创建二维表格，多创建一行，0行0列作为全零，不变，用于后续计算</span></span><br><span class="line">    <span class="comment"># 利用循环生成list，防止出现引用问题</span></span><br><span class="line">    <span class="comment"># text1 作为纵向，text2作为横向</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*(len(text2)+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(text1)+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(text1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(text2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> text1[i<span class="number">-1</span>] == text2[j<span class="number">-1</span>]:</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span> <span class="comment"># 找到一个公共元素</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>])  <span class="comment"># 取相邻近的较大元素</span></span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]   <span class="comment"># 最后一个元素即公共子序列长度</span></span><br></pre></td></tr></table></figure></li>
<li><p>0/1背包（动态规划）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>   <span class="comment"># 每件物品只能装一次</span></span><br><span class="line">W = <span class="number">4</span></span><br><span class="line">wt = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  <span class="comment"># 每个物品的重量</span></span><br><span class="line">val = [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>] <span class="comment"># 每个物品的价值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dp[i][w]: 对前i个物品，当前背包容量为w，最大价值则为dp[i][w]</span></span><br><span class="line"><span class="comment"># dp[3][5]=6: 对前3个物品，当前背包容量为5时，可以装下最大价值为6</span></span><br><span class="line">dp = [[<span class="number">0</span>]*(W+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> range(<span class="number">1</span>, W+<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 当前容量w</span></span><br><span class="line">        <span class="comment"># 装第i件物品：dp[i-1][w-wt[i-1]]+wt[i-1] (w-wt = 剩余背包容量)</span></span><br><span class="line">        <span class="comment"># 不装第i件物品：dp[i-1][w]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> w-wt[i<span class="number">-1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 装不下了</span></span><br><span class="line">            dp[i][w] = dp[i<span class="number">-1</span>][w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 能装下情况，选价值大的</span></span><br><span class="line">            dp[i][w] = max(dp[i<span class="number">-1</span>][w-wt[i<span class="number">-1</span>]]+val[i<span class="number">-1</span>], dp[i<span class="number">-1</span>][w])</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/04/09/Interview/" data-id="cknipj2q50016j0egqbc6anm6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/04/14/NLP/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          NLP
        
      </div>
    </a>
  
  
    <a href="/2020/11/17/C-plus-note/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">C_plus_note</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-protein/">basic protein</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cluster/">cluster</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/basic-protein/" style="font-size: 10px;">basic protein</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">七月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/14/NLP/">NLP</a>
          </li>
        
          <li>
            <a href="/2021/04/09/Interview/">Interview</a>
          </li>
        
          <li>
            <a href="/2020/11/17/C-plus-note/">C_plus_note</a>
          </li>
        
          <li>
            <a href="/2020/10/06/EM/">EM</a>
          </li>
        
          <li>
            <a href="/2020/10/06/Hidden-Markov-Model/">Hidden_Markov_Model</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 (soloistben)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>